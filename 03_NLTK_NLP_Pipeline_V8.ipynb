{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. NLTK NLP Text Analytics Pipeline V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.00 Installing Libraries and Dependencies\n",
    "\n",
    "#needs 3.11 for spaCy \n",
    "\n",
    "1. conda update -n base -c conda-forge conda\n",
    "2. conda create -n nltk-env python=3.11                 \n",
    "3. conda activate nltk-env\n",
    "4. conda install ipykernel jupyter nltk pandas numpy plotly matplotlib ipywidgets openpyxl seaborn -c conda-forge\n",
    "5. pip install spacy\n",
    "6. pip install sentence-transformers scikit-learn textblob nbformat ipython tqdm\n",
    "6. python -m spacy download en_core_web_sm\n",
    "7. python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.01 NLTK Preprocessing and tokenization for Peeking under the Hood Text Analytics\n",
    "\n",
    "Running the code cell below will result in us pulling the following token types for each of our seven corpora.\n",
    "1. word tokens - formed from decomposing sentences into their constituent pieces.\n",
    "2. NLTK text objects - The NLTK library has a unique tokenizer that adds additional metadata to the word token which allows for unique analysis as compared to normal word tokens.\n",
    "3. original sentence tokens - based on numerous features, but normally bounded by punctuation marks. This is why we normally tokenize this first before other text processing.\n",
    "4. normalized sentence tokens - original sentence tokens that have been lowercased, and had stop words, punctuations, and special chracters removed.\n",
    "\n",
    "You can always figure out what type of data corpus you are dealing with by running these print checks. It is also extremely important to also note the importance of keeping your documents categorized, lest they get out of control. The more processing and feature extractions you do, the more you may end up with more different buckets of data to keep up with.\n",
    "* We will look at the utility of each token type in these modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.01 Configuration\n",
    "\n",
    "*Set up file paths and select the SpaCy model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup path to files\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Text input for the path to CSV files\n",
    "path_input = widgets.Text(\n",
    "    value='data/data/rawTranscriptFiles/interview_transcripts',\n",
    "    placeholder='Enter the path to your CSV files',\n",
    "    description='CSV Path:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Text input for the output path\n",
    "output_path_input = widgets.Text(\n",
    "    value='data/data/rawTranscriptFiles/interview_transcripts_output',\n",
    "    placeholder='Enter the output path for processed files',\n",
    "    description='Output Path:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Dropdown for selecting the SpaCy model\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=['en_core_web_sm', 'en_core_web_md', 'en_core_web_lg'],\n",
    "    value='en_core_web_sm',\n",
    "    description='SpaCy Model:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Display the widgets\n",
    "display(path_input, output_path_input, model_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.02 This button just follows up with the last cell to define the path to the folders. Just push once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path to the folder containg csv files\n",
    "# Use the widget values in your code\n",
    "path_to_csv_files = path_input.value\n",
    "path_to_csv_files_output = output_path_input.value\n",
    "model = model_dropdown.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.00 Data Loading and Preprocessing\n",
    "\n",
    "*Read CSV files and preprocess text data.*\n",
    "push this button to set up and preprocess the text from the csv files. Look in the output folder and you should see a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob  # For sentiment analysis (optional)\n",
    "from tqdm.notebook import tqdm  # For the progress bar in Jupyter notebooks\n",
    "\n",
    "# Define the SpaCy model to use (ensure you have downloaded it, e.g., en_core_web_sm)\n",
    "model = model  # Replace with your desired model if different\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "# Download NLTK stopwords and tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # For sentence and word tokenization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Function to read CSV files from a folder and its subfolders\n",
    "def read_transcripts_from_folder(base_dir):\n",
    "    csv_files = glob.glob(os.path.join(base_dir, '**/*.csv'), recursive=True)\n",
    "    transcripts = []\n",
    "    for file in tqdm(csv_files, desc=\"Reading CSV Files\", unit=\"file\"):  # Add progress bar here\n",
    "        df = pd.read_csv(file)\n",
    "        filename = os.path.splitext(os.path.basename(file))[0]  # Extract filename without extension\n",
    "        transcripts.append((df, filename))\n",
    "    return transcripts\n",
    "\n",
    "# 2. Text preprocessing with SpaCy (no stopword removal yet)\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text using SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize and remove punctuation, no stopword removal yet\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 3. SpaCy POS, NER, and Dependency parsing analysis\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]  # Part-of-speech tagging\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]  # Named entity recognition\n",
    "    dependencies = [(token.text, token.dep_, token.head.text) for token in doc]  # Dependency parsing\n",
    "    return pos_tags, entities, dependencies\n",
    "\n",
    "# 4. Sentiment analysis using TextBlob\n",
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity  # Sentiment polarity and subjectivity\n",
    "\n",
    "# 5. NLTK tokenization and text object creation\n",
    "def nltk_tokenization_and_text_object(df):\n",
    "    # Sentence tokens of unprocessed and processed text\n",
    "    df['nltk_unprocessed_sentence_tokens'] = df['text'].apply(nltk.sent_tokenize)  # Unprocessed sentence tokens\n",
    "    df['nltk_processed_sentence_tokens'] = df['cleaned_text'].apply(nltk.sent_tokenize)  # Processed sentence tokens\n",
    "    \n",
    "    # Word tokens of unprocessed and processed text\n",
    "    df['nltk_unprocessed_word_tokens'] = df['text'].apply(nltk.word_tokenize)  # Unprocessed word tokens\n",
    "    df['nltk_processed_word_tokens'] = df['cleaned_text'].apply(nltk.word_tokenize)  # Processed word tokens\n",
    "\n",
    "    # Create NLTK Text object for the entire transcript\n",
    "    all_tokens = df['nltk_unprocessed_word_tokens'].explode().dropna().tolist()  # Flatten and remove NaN\n",
    "    nltk_text_obj = nltk.Text(all_tokens)  # Create NLTK Text object from all tokens\n",
    "    \n",
    "    # Store the NLTK Text object in the DataFrame's attributes\n",
    "    df.attrs['nltk_text_object'] = nltk_text_obj\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 6. Apply preprocessing and analysis to the 'text' column of each CSV\n",
    "def preprocess_and_analyze_transcripts(transcripts):\n",
    "    results = []\n",
    "    \n",
    "    for df, filename in tqdm(transcripts, desc='Processing Transcripts', unit=\"transcript\"):  # Add progress bar here\n",
    "        df['cleaned_text'] = df['text'].apply(preprocess_text)  # Preprocess the text\n",
    "        \n",
    "        # Apply SpaCy analysis (POS tags, NER, Dependency parsing)\n",
    "        df['pos_tags'], df['entities'], df['dependencies'] = zip(*df['text'].apply(spacy_analysis))\n",
    "        \n",
    "        # Word count and sentence length\n",
    "        df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "        df['sentence_length'] = df['text'].apply(lambda x: len(x.split()))  # Original text sentence length\n",
    "        \n",
    "        # Sentiment analysis (polarity and subjectivity)\n",
    "        df['sentiment_polarity'], df['sentiment_subjectivity'] = zip(*df['text'].apply(sentiment_analysis))\n",
    "        \n",
    "        # Apply NLTK tokenization and text object creation\n",
    "        df = nltk_tokenization_and_text_object(df)\n",
    "        \n",
    "        results.append((df, filename))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 7. Main pipeline function to process CSV files\n",
    "def process_csv_files(base_dir):\n",
    "    # Step 1: Read all transcripts from the CSV files\n",
    "    transcripts = read_transcripts_from_folder(base_dir)\n",
    "    \n",
    "    # Step 2: Preprocess and analyze the transcripts\n",
    "    processed_transcripts = preprocess_and_analyze_transcripts(transcripts)\n",
    "    \n",
    "    return processed_transcripts\n",
    "\n",
    "# Example usage of the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    base_dir = path_to_csv_files  # Replace with your actual path containing CSV transcript files\n",
    "    output_dir = path_to_csv_files_output  # Replace with your desired output directory for processed CSV files\n",
    "    \n",
    "    # Ensure the output directory exists (create it if it doesn't)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process the CSV files\n",
    "    processed_transcripts = process_csv_files(base_dir)\n",
    "    \n",
    "    # Save the processed CSV files with original filenames\n",
    "    for df, filename in tqdm(processed_transcripts, desc=\"Saving CSV Files\", unit=\"file\"):  # Add progress bar here\n",
    "        # Define the desired column order\n",
    "        column_order = [\n",
    "            'sentence_number', 'speaker', 'cleaned_text', 'entities', 'word_count', 'sentence_length', \n",
    "            'sentiment_polarity', 'sentiment_subjectivity', 'nltk_unprocessed_sentence_tokens', \n",
    "            'nltk_processed_sentence_tokens', 'nltk_unprocessed_word_tokens', 'nltk_processed_word_tokens', \n",
    "            'pos_tags', 'dependencies'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all columns exist in the DataFrame to prevent KeyError\n",
    "        existing_columns = [col for col in column_order if col in df.columns]\n",
    "        \n",
    "        # Save the DataFrame with reordered columns\n",
    "        output_filename = f'processed_transcript_{filename}.csv'\n",
    "        df[existing_columns].to_csv(os.path.join(output_dir, output_filename), index=False)\n",
    "        \n",
    "        print(f\"Processed file saved as: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.01\n",
    "Merge some variables together. Just push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, filename in processed_transcripts:\n",
    "    nltk_text_obj = df.attrs['nltk_text_object']\n",
    "    # Now you can use nltk_text_obj for concordance\n",
    "\n",
    "print(nltk_text_obj[0:10])  # Display the first 10 tokens\n",
    "type(nltk_text_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.02 Checking the dataframes\n",
    "Printing the first set of rows from a transcript to check that things are working. Just push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 rows of the first processed transcript\n",
    "print(processed_transcripts[0][0].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.03 Filter stop words\n",
    "\n",
    "Cleans out repetitive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tokens from 'nltk_unprocessed_word_tokens' column\n",
    "all_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()\n",
    "\n",
    "# Filter out stopwords and non-alphabetic tokens\n",
    "filtered_tokens = [word for word in all_tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "# Create NLTK Text object\n",
    "nltk_text_obj_clean = nltk.Text(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.00 Peeking Under The Hood Text Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.01 Average Sentence Length\n",
    "Average Sentence Length uses the total number of words and total number of sentences\n",
    "in a corpus to calculate exactly what it says: the average sentence length.\n",
    "While the equation is very basic and straightforward it provides information that can\n",
    "be used to infer, for example, how complex sentences are on average throughout a\n",
    "given text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # For Jupyter notebooks; use 'tqdm' if running as a script\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Function to calculate and print metrics (sentence and word count, average sentence length)\n",
    "def calculate_metrics(processed_transcripts):\n",
    "    metrics_list = []  # List to store all metrics\n",
    "\n",
    "    # Initialize tqdm progress bar over the processed_transcripts\n",
    "    for i, (df, filename) in enumerate(tqdm(processed_transcripts, desc='Calculating Metrics', unit='transcript')):\n",
    "        # Retrieve sentence and word tokens from the DataFrame\n",
    "        sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().dropna().tolist()  # Flatten and remove NaN\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().dropna().tolist()  # Flatten and remove NaN\n",
    "\n",
    "        # Ensure there are sentences to avoid division by zero\n",
    "        if len(sentence_tokens) > 0:\n",
    "            average_sentence_length = len(word_tokens) / len(sentence_tokens)\n",
    "        else:\n",
    "            average_sentence_length = 0  # Default to zero if no sentences\n",
    "\n",
    "        # Store metrics in a dictionary\n",
    "        metrics = {\n",
    "            'Transcript': filename,\n",
    "            'Number of sentences': len(sentence_tokens),\n",
    "            'Number of word tokens': len(word_tokens),\n",
    "            'Average sentence length': average_sentence_length\n",
    "        }\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        # Prepare the result string\n",
    "        result = f\"\\nTranscript {i + 1} ({filename}):\\n\" \\\n",
    "                 f\"Number of sentences: {len(sentence_tokens)}\\n\" \\\n",
    "                 f\"Number of word tokens: {len(word_tokens)}\\n\" \\\n",
    "                 f\"Average sentence length: {average_sentence_length:.2f}\"\n",
    "\n",
    "        # Use tqdm.write() to print without disrupting the progress bar\n",
    "        tqdm.write(result)\n",
    "\n",
    "    # Convert the list of metrics to a DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "# 2. Run the function and get the DataFrame\n",
    "metrics_df = calculate_metrics(processed_transcripts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.02 Average Word Length Distribution\n",
    "Another fairly straightforward measure that can provide insight into how long, on average, words are in a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # For Jupyter notebooks; use 'tqdm' if running as a script\n",
    "\n",
    "# 1. Function to calculate and print metrics (sentence and word count, average sentence length)\n",
    "def calculate_metrics(processed_transcripts):\n",
    "    # Initialize tqdm progress bar over the processed_transcripts\n",
    "    for i, (df, filename) in enumerate(tqdm(processed_transcripts, desc='Calculating Metrics', unit='transcript')):\n",
    "        # Retrieve sentence and word tokens from the DataFrame\n",
    "        sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().dropna().tolist()  # Flatten and remove NaN\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().dropna().tolist()  # Flatten and remove NaN\n",
    "\n",
    "        # Ensure there are sentences to avoid division by zero\n",
    "        if sentence_tokens:\n",
    "            average_sentence_length = len(word_tokens) / len(sentence_tokens)\n",
    "        else:\n",
    "            average_sentence_length = 0  # Default to zero if no sentences\n",
    "\n",
    "        # Use tqdm.write() to print without disrupting the progress bar\n",
    "        result = f\"\\nTranscript {i + 1} ({filename}):\\n\" \\\n",
    "                 f\"Number of sentences: {len(sentence_tokens)}\\n\" \\\n",
    "                 f\"Number of word tokens: {len(word_tokens)}\\n\" \\\n",
    "                 f\"Average sentence length: {average_sentence_length:.2f}\"\n",
    "        tqdm.write(result)\n",
    "\n",
    "# 2. Run the metric calculation on the processed transcripts\n",
    "calculate_metrics(processed_transcripts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.03 Lexical Diversity\n",
    "Lexical diversity quantifies the variety of unique words found in a document. It produces a numerical measure that indicates how diverse the vocabulary is that is used in a text. Broadly speaking, scores of (0.8 - 1) are considered extremely high and difficult to maintain in typical communicative texts. Scores of 0.4-0.79 are considered moderate to high; most high-quality texts fall in this range. Scores of (0 - 0.39) are considered low lexical diversity and tend to suggest highly specialized or technical language usage (e.g., instruction manuals) or language aimed at young readers. This measure is sensitive to corpus length (longer corpora have more opportunities to repeat words), but comparing lexical diversity scores can allow for quantitative comparison that might suggest potential changes in how the usage of language may differ between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def calculate_lexical_diversity(processed_transcripts):\n",
    "    for df, filename in processed_transcripts:\n",
    "        nltk_text_obj = df.attrs.get('nltk_text_object', None)\n",
    "        \n",
    "        if nltk_text_obj is not None:\n",
    "            # Filter tokens: remove stopwords and punctuation, and convert to lowercase\n",
    "            filtered_tokens = [\n",
    "                word.lower() for word in nltk_text_obj\n",
    "                if word.lower() not in stop_words and word.isalpha()\n",
    "            ]\n",
    "            \n",
    "            total_words = len(filtered_tokens)\n",
    "            unique_words = len(set(filtered_tokens))\n",
    "            lexical_diversity = unique_words / total_words if total_words > 0 else 0\n",
    "            print(f\"Lexical diversity for {filename}: {lexical_diversity:.3f}\")\n",
    "        else:\n",
    "            print(f\"No NLTK Text object available for {filename}\")\n",
    "\n",
    "calculate_lexical_diversity(processed_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.04 Unique Words Over Time\n",
    "Unique words can be used to identify the frequency of words that appear only once in a given corpus. We can also print a list of these word tokens. Looking at unique words between or across text corpora can allow us to look for the appearances and disappearances of specialized educational terminology over time. To find the frequency (number) of unique words, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to calculate and print the number of unique words for each transcript\n",
    "def calculate_unique_words(processed_transcripts):\n",
    "    for i, (df, filename) in enumerate(processed_transcripts):\n",
    "        # Retrieve word tokens from the DataFrame (unprocessed word tokens)\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "\n",
    "        # Calculate the number of unique words\n",
    "        unique_words = set(word_tokens)\n",
    "        unique_word_count = len(unique_words)\n",
    "\n",
    "        # Print results for each transcript\n",
    "        print(f\"\\nTranscript {i + 1} ({filename}):\")\n",
    "        print(\"Number of unique words:\", unique_word_count)\n",
    "\n",
    "# 2. Run the unique word count calculation on the processed transcripts\n",
    "calculate_unique_words(processed_transcripts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.05 Most common used words by corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Calculate and print the top 10 most common words for each transcript\n",
    "def calculate_most_common_words(processed_transcripts, top_n=10):\n",
    "    for df, filename in processed_transcripts:\n",
    "        # Retrieve the NLTK Text object from the DataFrame's attributes\n",
    "        nltk_text_obj = df.attrs.get('nltk_text_object', None)\n",
    "        \n",
    "        if nltk_text_obj is not None:\n",
    "            # Filter out stopwords and non-alphabetic tokens, and convert to lowercase\n",
    "            filtered_tokens = [\n",
    "                word.lower() for word in nltk_text_obj\n",
    "                if word.lower() not in stop_words and word.isalpha()\n",
    "            ]\n",
    "            \n",
    "            # Create a frequency distribution\n",
    "            fdist = FreqDist(filtered_tokens)\n",
    "            \n",
    "            # Get the top N most common words\n",
    "            most_common = fdist.most_common(top_n)\n",
    "            \n",
    "            print(f\"Top {top_n} most common words in {filename}:\")\n",
    "            for word, frequency in most_common:\n",
    "                print(f\"{word}: {frequency}\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(f\"No NLTK Text object available for {filename}\")\n",
    "\n",
    "# 2. Run the frequency distribution calculation on the processed transcripts\n",
    "calculate_most_common_words(processed_transcripts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.06 Display all unique words found in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Create a dictionary where each key is the transcript filename and value is a set of word tokens\n",
    "corpora_tokens = {}\n",
    "for df, filename in processed_transcripts:\n",
    "    if 'nltk_unprocessed_word_tokens' in df.columns:\n",
    "        # Flatten the word tokens and filter out stopwords and non-alphabetic words\n",
    "        tokens = set(\n",
    "            word.lower() for word in df['nltk_unprocessed_word_tokens'].explode().tolist()\n",
    "            if word.lower() not in stop_words and word.isalpha()\n",
    "        )\n",
    "        corpora_tokens[filename] = tokens\n",
    "    else:\n",
    "        print(f\"'nltk_unprocessed_word_tokens' not found in {filename}\")\n",
    "\n",
    "# 2. Function to find words unique to each transcript compared to others\n",
    "def find_unique_words(corpora_tokens):\n",
    "    unique_words = {}\n",
    "    for corpus_name, tokens in corpora_tokens.items():\n",
    "        # Collect all tokens from other transcripts\n",
    "        all_other_tokens = set()\n",
    "        for other_corpus_name, other_tokens in corpora_tokens.items():\n",
    "            if corpus_name != other_corpus_name:\n",
    "                all_other_tokens.update(other_tokens)\n",
    "        \n",
    "        # Unique words are those not present in other transcripts\n",
    "        unique_words[corpus_name] = tokens - all_other_tokens\n",
    "    return unique_words\n",
    "\n",
    "# 3. Find words unique to each transcript\n",
    "unique_words_by_transcript = find_unique_words(corpora_tokens)\n",
    "\n",
    "# 4. Print unique words for each transcript\n",
    "for transcript_name, unique_words in unique_words_by_transcript.items():\n",
    "    print(f\"\\nWords exclusive to {transcript_name}:\")\n",
    "    print(sorted(unique_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.07 Most frequently used words across all corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Aggregate all word tokens from all transcripts into a single list\n",
    "all_tokens = []\n",
    "for df, filename in processed_transcripts:  # Unpack the tuple (DataFrame, filename)\n",
    "    if 'nltk_unprocessed_word_tokens' in df.columns:\n",
    "        # Flatten the word tokens\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()\n",
    "        \n",
    "        # Filter out stopwords and non-alphabetic tokens, and convert to lowercase\n",
    "        filtered_tokens = [\n",
    "            word.lower() for word in word_tokens\n",
    "            if word.lower() not in stop_words and word.isalpha()\n",
    "        ]\n",
    "        \n",
    "        all_tokens.extend(filtered_tokens)\n",
    "    else:\n",
    "        print(f\"'nltk_unprocessed_word_tokens' not found in {filename}\")\n",
    "\n",
    "# 2. Calculate the frequency distribution of all tokens\n",
    "token_freq_dist = Counter(all_tokens)\n",
    "\n",
    "# 3. Find the most common words across all transcripts (adjust the number as needed)\n",
    "most_common_words = token_freq_dist.most_common(100)\n",
    "\n",
    "# 4. Function to print data in columns\n",
    "def print_in_columns(data, columns=3):\n",
    "    # Split the data into chunks of size 'columns'\n",
    "    for i in range(0, len(data), columns):\n",
    "        chunk = data[i:i + columns]\n",
    "        # Format and print each chunk\n",
    "        print(\"  \".join(f\"{word}: {freq}\" for word, freq in chunk))\n",
    "\n",
    "# 5. Print the most common words in columns\n",
    "print(\"Most frequently used words across all transcripts:\")\n",
    "print_in_columns(most_common_words, columns=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.00 NLTK Text Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.01 N-grams and collocations\n",
    "N-grams point out recurring word combinations found throughout the text corpus. For example, \"spring break\" is an example of a bigrams while \"New York City\" is a trigrams. Bigrams and repeated collocations of words convey a lot of information about the contents of the text corpus.\n",
    "To generate an ordered list of the most common bigrams, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Loop through each transcript to find and display top bigrams and trigrams\n",
    "for i, (df, filename) in enumerate(processed_transcripts):  # Unpack the tuple (DataFrame, filename)\n",
    "    if 'nltk_unprocessed_word_tokens' in df.columns:\n",
    "        # Retrieve word tokens from the DataFrame (unprocessed word tokens)\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "\n",
    "        # Filter out stopwords and non-alphabetic tokens, and convert to lowercase\n",
    "        filtered_tokens = [\n",
    "            word.lower() for word in word_tokens\n",
    "            if word.lower() not in stop_words and word.isalpha()\n",
    "        ]\n",
    "\n",
    "        # Display frequency of highest 50 bigrams\n",
    "        print(f\"\\nTop 50 bigrams for Transcript {i + 1} ({filename}):\")\n",
    "        bigram_finder = BigramCollocationFinder.from_words(filtered_tokens)\n",
    "\n",
    "        # Apply frequency filter to remove less frequent bigrams (optional)\n",
    "        bigram_finder.apply_freq_filter(2)  # Adjust the minimum frequency as needed\n",
    "\n",
    "        # Get the top 50 bigrams using raw frequency\n",
    "        bigrams = bigram_finder.ngram_fd.most_common(50)\n",
    "        for bigram, freq in bigrams:\n",
    "            print(f\"{bigram}: {freq}\")\n",
    "\n",
    "        # Display frequency of highest 50 trigrams\n",
    "        print(f\"\\nTop 50 trigrams for Transcript {i + 1} ({filename}):\")\n",
    "        trigram_finder = TrigramCollocationFinder.from_words(filtered_tokens)\n",
    "\n",
    "        # Apply frequency filter to remove less frequent trigrams (optional)\n",
    "        trigram_finder.apply_freq_filter(2)  # Adjust the minimum frequency as needed\n",
    "\n",
    "        # Get the top 50 trigrams using raw frequency\n",
    "        trigrams = trigram_finder.ngram_fd.most_common(50)\n",
    "        for trigram, freq in trigrams:\n",
    "            print(f\"{trigram}: {freq}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"'nltk_unprocessed_word_tokens' column not found in {filename}\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.02 Concordance\n",
    "Concordance is an NLTK Text object method that also looks for word distribution, but specifically searches for words found before and after a specific word of choice. Concordance allows us to find out how words are used contextually throughout a corpus. This can be particularly powerful when looking at trends over time or between groups. For example, in the sample below we search for the all the contextual occurrences of the word “pi” in our seven separate corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4e6b8dc7254957a27fd983e2aa6441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='equity', description='Search word:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa95d27a71643349d3fc5bf1007232c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Search Concordance', style=ButtonStyle(), tooltip='Click to search…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "\n",
    "# Function to display concordance results for the selected word\n",
    "def show_concordance(word_to_search):\n",
    "    for df, filename in processed_transcripts:\n",
    "        nltk_text_obj = df.attrs['nltk_text_object']\n",
    "        print(f\"Concordance for '{word_to_search}' in {filename}:\")\n",
    "        try:\n",
    "            nltk_text_obj.concordance(word_to_search, width=150)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {filename}: {e}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Function to update and display concordance based on user input\n",
    "def on_button_click(b):\n",
    "    word_to_search = word_input.value  # Get the user input word\n",
    "    if word_to_search:  # If the user provided a word\n",
    "        show_concordance(word_to_search)\n",
    "    else:\n",
    "        print(\"Please enter a word to search for concordance.\")\n",
    "\n",
    "# Create a text box for user input\n",
    "word_input = widgets.Text(\n",
    "    value='equity',  # Default word\n",
    "    description='Search word:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a button to trigger the concordance search\n",
    "search_button = widgets.Button(\n",
    "    description='Search Concordance',\n",
    "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to search for the word in transcripts'\n",
    ")\n",
    "\n",
    "# Attach an event listener to the button click\n",
    "search_button.on_click(on_button_click)\n",
    "\n",
    "# Display the input box and the button\n",
    "display(word_input, search_button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.00 Word usage visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.01 Context Dispersion Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "\n",
    "# Function to create the dispersion plot based on user input words\n",
    "def create_dispersion_plots(words_to_track):\n",
    "    words_to_track = [word.lower().strip() for word in words_to_track]  # Ensure lowercase and no spaces\n",
    "\n",
    "    # Loop through each transcript and create the dispersion plot\n",
    "    for df, filename in processed_transcripts:\n",
    "        # Retrieve the NLTK Text object from the DataFrame's attributes\n",
    "        nltk_text_obj = df.attrs.get('nltk_text_object', None)\n",
    "\n",
    "        if nltk_text_obj is not None:\n",
    "            print(f\"Dispersion plot for {filename}:\")\n",
    "            try:\n",
    "                # Create the dispersion plot\n",
    "                nltk_text_obj.dispersion_plot(words_to_track)\n",
    "                plt.title(f\"Dispersion Plot for {filename}\")\n",
    "                plt.xlabel('Word Offset')\n",
    "                plt.ylabel('Words')\n",
    "                plt.show()  # Show the plot for each file\n",
    "                plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"No NLTK Text object available for {filename}\")\n",
    "\n",
    "# Function to handle button click and trigger the dispersion plot creation\n",
    "def on_button_click(b):\n",
    "    words_input = words_input_box.value  # Get user input\n",
    "    if words_input:\n",
    "        words_to_track = [word.strip() for word in words_input.split(',')]  # Split input into words\n",
    "        create_dispersion_plots(words_to_track)\n",
    "    else:\n",
    "        print(\"Please enter at least one word to track.\")\n",
    "\n",
    "# Create a text input box for user input\n",
    "words_input_box = widgets.Text(\n",
    "    value='equity, students, teachers',  # Default words\n",
    "    description='Words to track:',\n",
    "    placeholder='Enter words separated by commas'\n",
    ")\n",
    "\n",
    "# Create a button to trigger the dispersion plot generation\n",
    "plot_button = widgets.Button(\n",
    "    description='Create Dispersion Plot',\n",
    "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to create the dispersion plot'\n",
    ")\n",
    "\n",
    "# Attach the button click event\n",
    "plot_button.on_click(on_button_click)\n",
    "\n",
    "# Display the input box and the button\n",
    "display(words_input_box, plot_button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.02 Bag of Words Frequency Distribution with Plots\n",
    "This will search for each word in the bag of words to find its frequency in each text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm  # For progress bars in Jupyter notebooks\n",
    "\n",
    "# Predefined keywords to track (users can add their own)\n",
    "predefined_keywords = sorted(set([\n",
    "    'students', 'science', 'engagement', 'ability', 'community', 'talk',\n",
    "    'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations',\n",
    "    'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "    'engaging', 'literacy', 'kids', 'connect', 'student', 'classroom',\n",
    "    'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking',\n",
    "    'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'questions',\n",
    "    'think', 'want', 'kind', 'time', 'grade', 'thinking', 'different', 'conversation',\n",
    "    'hard', 'saying'\n",
    "]), reverse=True)\n",
    "\n",
    "# Function to create keyword occurrence plots\n",
    "def plot_keyword_occurrences(keywords_to_track):\n",
    "    keywords_to_track = [keyword.lower().strip() for keyword in keywords_to_track]\n",
    "    keyword_mapping = {keyword: i for i, keyword in enumerate(keywords_to_track)}\n",
    "\n",
    "    # Number of transcripts (corpora)\n",
    "    num_corpora = len(processed_transcripts)\n",
    "\n",
    "    # Create subplots for each transcript\n",
    "    fig, axes = plt.subplots(num_corpora, 1, figsize=(25, num_corpora * 9), sharex=True)\n",
    "\n",
    "    # Convert axes to an array if it's not (happens when num_corpora is 1)\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    # Iterate over each transcript and plot keyword occurrences\n",
    "    for ax, (i, (df, filename)) in zip(axes, enumerate(tqdm(processed_transcripts, desc='Plotting Keywords'))):\n",
    "        corpus_name = os.path.basename(filename)\n",
    "        ax.set_title(f\"{corpus_name}\")\n",
    "        \n",
    "        # Check if 'nltk_unprocessed_sentence_tokens' column exists\n",
    "        if 'nltk_unprocessed_sentence_tokens' in df.columns:\n",
    "            # Get the sentence tokens from each transcript\n",
    "            sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().dropna().tolist()  # Flatten and remove NaN\n",
    "            \n",
    "            # Plot the occurrences of each keyword\n",
    "            for keyword in keywords_to_track:\n",
    "                occurrences = [(i + 1, j + 1) for j, sentence in enumerate(sentence_tokens) if keyword in sentence.lower()]\n",
    "                sentence_nums = [occ[1] for occ in occurrences]\n",
    "                \n",
    "                y_values = np.full(len(sentence_nums), keyword_mapping[keyword])\n",
    "                ax.scatter(sentence_nums, y_values, label=keyword, alpha=0.6, edgecolors='none')\n",
    "            \n",
    "            ax.set_yticks(list(keyword_mapping.values()))\n",
    "            ax.set_yticklabels(list(keyword_mapping.keys()))\n",
    "            ax.set_ylabel('Keywords')\n",
    "        else:\n",
    "            print(f\"'nltk_unprocessed_sentence_tokens' column not found in {filename}\")\n",
    "            continue  # Skip this transcript if the required column is missing\n",
    "\n",
    "    # Adjust layout and labels\n",
    "    plt.xlabel('Sentence Number')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure to an image file before displaying\n",
    "    plt.savefig('data/my_keyword_plots.png', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to handle the plot button click event\n",
    "def on_plot_button_click(b):\n",
    "    keywords_input = keyword_input_box.value  # Get user input\n",
    "    if keywords_input:\n",
    "        keywords_to_track = [keyword.strip() for keyword in keywords_input.split(',')]\n",
    "        plot_keyword_occurrences(keywords_to_track)\n",
    "    else:\n",
    "        print(\"Please enter at least one keyword to track.\")\n",
    "\n",
    "# Text box for user input of keywords\n",
    "keyword_input_box = widgets.Text(\n",
    "    value=', '.join(predefined_keywords[:5]),  # Default words (first 5 predefined)\n",
    "    description='Keywords:',\n",
    "    placeholder='Enter keywords separated by commas'\n",
    ")\n",
    "\n",
    "# Button to trigger the plotting\n",
    "plot_button = widgets.Button(\n",
    "    description='Create Keyword Plot',\n",
    "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to create the keyword occurrence plot'\n",
    ")\n",
    "\n",
    "# Attach the button click event\n",
    "plot_button.on_click(on_plot_button_click)\n",
    "\n",
    "# Display the input box and the button\n",
    "display(keyword_input_box, plot_button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.03 Plotting numerous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Example keywords to track (remove duplicates and sort in reverse for plotting)\n",
    "keywords = sorted(set([\n",
    "    'students', 'science', 'engagement', 'ability', 'community', 'talk',\n",
    "    'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations',\n",
    "    'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "    'engaging', 'literacy', 'kids', 'connect', 'student', 'classroom',\n",
    "    'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking',\n",
    "    'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'questions',\n",
    "    'think', 'want', 'kind', 'time', 'grade', 'thinking', 'different', 'conversation',\n",
    "    'hard', 'saying'\n",
    "]), reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Number of transcripts (corpora)\n",
    "num_corpora = len(processed_transcripts)\n",
    "\n",
    "# Create subplots for each transcript\n",
    "fig, axes = plt.subplots(num_corpora, 1, figsize=(25, num_corpora * 9), sharex=True)\n",
    "\n",
    "# Convert axes to an array if it's not (happens when num_corpora is 1)\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "# Iterate over each transcript and plot keyword occurrences\n",
    "for ax, (i, (df, filename)) in zip(axes, enumerate(processed_transcripts)):\n",
    "    corpus_name = os.path.basename(filename)\n",
    "    ax.set_title(f\"{corpus_name}\")\n",
    "    \n",
    "    # Check if 'nltk_unprocessed_sentence_tokens' column exists\n",
    "    if 'nltk_unprocessed_sentence_tokens' in df.columns:\n",
    "        # Get the sentence tokens from each transcript\n",
    "        sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().dropna().tolist()  # Flatten and remove NaN\n",
    "        \n",
    "        # Plot the occurrences of each keyword\n",
    "        for keyword in keywords:\n",
    "            occurrences = [(i + 1, j + 1) for j, sentence in enumerate(sentence_tokens) if keyword in sentence.lower()]\n",
    "            sentence_nums = [occ[1] for occ in occurrences]\n",
    "            \n",
    "            y_values = np.full(len(sentence_nums), keyword_mapping[keyword])\n",
    "            ax.scatter(sentence_nums, y_values, label=keyword, alpha=0.6, edgecolors='none')\n",
    "        \n",
    "        ax.set_yticks(list(keyword_mapping.values()))\n",
    "        ax.set_yticklabels(list(keyword_mapping.keys()))\n",
    "        ax.set_ylabel('Keywords')\n",
    "    else:\n",
    "        print(f\"'nltk_unprocessed_sentence_tokens' column not found in {filename}\")\n",
    "        continue  # Skip this transcript if the required column is missing\n",
    "\n",
    "# Adjust layout and labels\n",
    "plt.xlabel('Sentence Number')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to an image file before displaying\n",
    "plt.savefig('data/my_keyword_plots.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
