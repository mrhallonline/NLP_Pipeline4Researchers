{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription Comparison Tool\n",
    "\n",
    "Explanation\n",
    "Reading CSV Files:\n",
    "\n",
    "The read_csv_files function recursively reads all CSV files in the specified directory and subdirectories.\n",
    "Each CSV file is read into a separate DataFrame, and the source file path is stored in a new column.\n",
    "Processing Transcriptions:\n",
    "\n",
    "Each DataFrame is processed individually to clean the text, tokenize, and perform POS tagging and NER.\n",
    "This keeps the analyses of each transcription separate.\n",
    "Generating Embeddings and Calculating Similarity:\n",
    "\n",
    "Embeddings are generated for the entire cleaned transcript text of each file.\n",
    "The embeddings are stored in a list along with the corresponding file names.\n",
    "The cosine similarity matrix is computed to compare each transcription against all others.\n",
    "Topic Modeling with LDA:\n",
    "\n",
    "LDA is performed on the tokens of each transcription separately.\n",
    "The topics for each transcription are printed out, allowing you to compare the topics between different transcriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pandas numpy nltk spacy sentence-transformers scikit-learn\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevinhall/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinhall/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape for data/Interviews/20240522_INT_HS_UT_Steffan/20240522_INT_HS_UT_Steffan_2024-08-07_transcription.csv: (1324, 7)\n",
      "Cleaned transcripts for data/Interviews/20240522_INT_HS_UT_Steffan/20240522_INT_HS_UT_Steffan_2024-08-07_transcription.csv: 0                       ok so ill start recording then\n",
      "1                                            all right\n",
      "2                                      its going great\n",
      "3                                                   ok\n",
      "4    yeah so just a little bit first to kind of get...\n",
      "Name: cleaned_transcript, dtype: object\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/kevinhall/nltk_data'\n    - '/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/nltk_data'\n    - '/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/share/nltk_data'\n    - '/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_transcript\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned transcripts for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_transcript\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_transcript\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tags\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_transcript\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(spacy_analysis(x))\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m, in \u001b[0;36mtokenize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_text\u001b[39m(text):\n\u001b[0;32m---> 41\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/kevinhall/nltk_data'\n    - '/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/nltk_data'\n    - '/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/share/nltk_data'\n    - '/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "file_path = 'data/Interviews'  # Update this if needed\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Functions\n",
    "def read_csv_files(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = file  # Keep track of the source file\n",
    "        data_frames.append(df)\n",
    "    return data_frames\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pos_tags, entities\n",
    "\n",
    "def perform_lda(tokens_list, num_topics=5):\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "    return lda, corpus, dictionary\n",
    "\n",
    "# Execution\n",
    "directory = file_path\n",
    "transcript_dfs = read_csv_files(directory)\n",
    "\n",
    "embeddings_list = []\n",
    "file_names = []\n",
    "lda_models = []\n",
    "\n",
    "for df in transcript_dfs:\n",
    "    print(f\"DataFrame shape for {df['source_file'].iloc[0]}: {df.shape}\")\n",
    "    df['cleaned_transcript'] = df['text'].apply(preprocess_text)\n",
    "    print(f\"Cleaned transcripts for {df['source_file'].iloc[0]}: {df['cleaned_transcript'].head()}\")\n",
    "\n",
    "    df['tokens'] = df['cleaned_transcript'].apply(tokenize_text)\n",
    "    df[['pos_tags', 'entities']] = df['cleaned_transcript'].apply(\n",
    "        lambda x: pd.Series(spacy_analysis(x))\n",
    "    )\n",
    "\n",
    "    # Generate embeddings\n",
    "    text = ' '.join(df['cleaned_transcript'])\n",
    "    embedding = model.encode(text)\n",
    "    print(f\"Embedding for {df['source_file'].iloc[0]}: {embedding.shape}\")\n",
    "\n",
    "    if embedding is not None and embedding.size > 0:\n",
    "        embeddings_list.append(embedding)\n",
    "        file_names.append(df['source_file'].iloc[0])\n",
    "\n",
    "        # Perform LDA\n",
    "        tokens_list = df['tokens'].tolist()\n",
    "        lda_model, corpus, dictionary = perform_lda(tokens_list)\n",
    "        lda_models.append({\n",
    "            'model': lda_model,\n",
    "            'corpus': corpus,\n",
    "            'dictionary': dictionary,\n",
    "            'source_file': df['source_file'].iloc[0]\n",
    "        })\n",
    "    else:\n",
    "        print(f\"No valid embedding for {df['source_file'].iloc[0]}.\")\n",
    "\n",
    "# Compute similarity matrix\n",
    "if embeddings_list:\n",
    "    embeddings_array = np.vstack(embeddings_list)\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=file_names, columns=file_names)\n",
    "    print(similarity_df)\n",
    "\n",
    "    # Visualize similarity matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_df, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.title('Semantic Similarity Between Transcriptions')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No embeddings to compute similarity matrix.\")\n",
    "\n",
    "# Print topics from LDA models\n",
    "for lda_info in lda_models:\n",
    "    print(f\"Topics for {lda_info['source_file']}:\")\n",
    "    topics = lda_info['model'].print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to /Users/kevinhall/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinhall/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m\n\u001b[1;32m     83\u001b[0m     lda_models\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: lda_model,\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpus\u001b[39m\u001b[38;5;124m'\u001b[39m: corpus,\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdictionary\u001b[39m\u001b[38;5;124m'\u001b[39m: dictionary,\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m'\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m     })\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Compute similarity matrix\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m embeddings_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m cosine_similarity(embeddings_array)\n\u001b[1;32m     93\u001b[0m similarity_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(similarity_matrix, index\u001b[38;5;241m=\u001b[39mfile_names, columns\u001b[38;5;241m=\u001b[39mfile_names)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "file_path = 'data\\Interviews'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Functions\n",
    "def read_csv_files(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = file  # Keep track of the source file\n",
    "        data_frames.append(df)\n",
    "    return data_frames\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pos_tags, entities\n",
    "\n",
    "def perform_lda(tokens_list, num_topics=5):\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "    return lda, corpus, dictionary\n",
    "\n",
    "# Execution\n",
    "directory = file_path\n",
    "transcript_dfs = read_csv_files(directory)\n",
    "\n",
    "embeddings_list = []\n",
    "file_names = []\n",
    "lda_models = []\n",
    "\n",
    "for df in transcript_dfs:\n",
    "    df['cleaned_transcript'] = df['text'].apply(preprocess_text)\n",
    "    df['tokens'] = df['cleaned_transcript'].apply(tokenize_text)\n",
    "    df[['pos_tags', 'entities']] = df['cleaned_transcript'].apply(\n",
    "        lambda x: pd.Series(spacy_analysis(x))\n",
    "    )\n",
    "\n",
    "    # Generate embeddings\n",
    "    text = ' '.join(df['cleaned_transcript'])\n",
    "    embedding = model.encode(text)\n",
    "    embeddings_list.append(embedding)\n",
    "    file_names.append(df['source_file'].iloc[0])\n",
    "\n",
    "    # Perform LDA\n",
    "    tokens_list = df['tokens'].tolist()\n",
    "    lda_model, corpus, dictionary = perform_lda(tokens_list)\n",
    "    lda_models.append({\n",
    "        'model': lda_model,\n",
    "        'corpus': corpus,\n",
    "        'dictionary': dictionary,\n",
    "        'source_file': df['source_file'].iloc[0]\n",
    "    })\n",
    "\n",
    "# Compute similarity matrix\n",
    "embeddings_array = np.vstack(embeddings_list)\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=file_names, columns=file_names)\n",
    "print(similarity_df)\n",
    "\n",
    "# Visualize similarity matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_df, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Semantic Similarity Between Transcriptions')\n",
    "plt.show()\n",
    "\n",
    "# Print topics from LDA models\n",
    "for lda_info in lda_models:\n",
    "    print(f\"Topics for {lda_info['source_file']}:\")\n",
    "    topics = lda_info['model'].print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity Sentence Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in results_df: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'source_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 99\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 5. Visualize the results\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# A. Bar plot of occurrences\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m occurrences \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey_sentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key_sentence \u001b[38;5;129;01min\u001b[39;00m key_sentences:\n\u001b[1;32m    102\u001b[0m     key_data \u001b[38;5;241m=\u001b[39m occurrences[occurrences[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m key_sentence]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp24-env/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'source_file'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm  # To track progress\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "\n",
    "# 1. Define key sentences\n",
    "key_sentences = [\n",
    "    \"How do you support student learning in your classroom?\",\n",
    "    # Add more key sentences as needed\n",
    "]\n",
    "\n",
    "\n",
    "# 2. Generate embeddings for key sentences\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "key_embeddings = model.encode(key_sentences)\n",
    "\n",
    "# 3. Read and process transcripts\n",
    "def read_transcript_csvs(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    transcript_data = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = os.path.basename(file)  # Add 'source_file' column\n",
    "        transcript_data.append(df)\n",
    "    return transcript_data\n",
    "\n",
    "transcript_data = read_transcript_csvs('data/rawTranscriptFiles/interview_transcripts')\n",
    "\n",
    "# Preprocess function for text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_embeddings_in_batches(texts, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Use the function for processing embeddings\n",
    "for df in transcript_data:\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    df['embedding'] = compute_embeddings_in_batches(df['cleaned_text'].tolist(), model)\n",
    "\n",
    "# Preprocess and embed sentences\n",
    "# for df in transcript_data:\n",
    "#     df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "#     df['embedding'] = df['cleaned_text'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# 4. Calculate similarity scores\n",
    "similarity_threshold = 0.55  # Adjust as needed\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key_idx, key_sentence in enumerate(key_sentences):\n",
    "    key_embedding = key_embeddings[key_idx]\n",
    "    for df in transcript_data:\n",
    "        transcript_embeddings = np.vstack(df['embedding'].values)\n",
    "        similarities = cosine_similarity([key_embedding], transcript_embeddings)[0]\n",
    "        df[f'similarity_with_key_{key_idx}'] = similarities\n",
    "        similar_sentences = df[df[f'similarity_with_key_{key_idx}'] >= similarity_threshold]\n",
    "        if not similar_sentences.empty:\n",
    "            source_file = df['source_file'].values[0]  # Get the source file name\n",
    "            for idx, row in similar_sentences.iterrows():\n",
    "                results.append({\n",
    "                    'source_file': source_file,\n",
    "                    'sentence_number': row['sentence_number'],\n",
    "                    'text': row['text'],  # 'text' contains the transcript sentences\n",
    "                    'speaker': row['speaker'],  # Include speaker information if needed\n",
    "                    'similarity_score': row[f'similarity_with_key_{key_idx}'],\n",
    "                    'key_sentence': key_sentence,\n",
    "                    'key_sentence_index': key_idx\n",
    "                })\n",
    "\n",
    "# Create a DataFrame to hold results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Check if 'source_file' is present and print a sample of results\n",
    "print(\"Columns in results_df:\", results_df.columns.tolist())\n",
    "print(results_df.head())\n",
    "\n",
    "# 5. Visualize the results\n",
    "# A. Bar plot of occurrences\n",
    "occurrences = results_df.groupby(['source_file', 'key_sentence']).size().reset_index(name='counts')\n",
    "\n",
    "for key_sentence in key_sentences:\n",
    "    key_data = occurrences[occurrences['key_sentence'] == key_sentence]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='source_file', y='counts', data=key_data)\n",
    "    plt.title(f\"Occurrences of sentences similar to: '{key_sentence}'\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Number of Similar Sentences')\n",
    "    plt.xlabel('Transcript File')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# B. Scatter plot of sentence positions\n",
    "for df in transcript_data:\n",
    "    transcript_name = df['source_file'].iloc[0]\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    has_similar_sentences = False\n",
    "    for key_idx, key_sentence in enumerate(key_sentences):\n",
    "        similar_sentences = df[df[f'similarity_with_key_{key_idx}'] >= similarity_threshold]\n",
    "        if not similar_sentences.empty:\n",
    "            has_similar_sentences = True\n",
    "            plt.scatter(\n",
    "                similar_sentences['sentence_number'],\n",
    "                [key_idx]*len(similar_sentences),\n",
    "                label=f\"Key {key_idx}: {key_sentence[:30]}...\",\n",
    "                marker='|',\n",
    "                s=200\n",
    "            )\n",
    "    \n",
    "    if has_similar_sentences:\n",
    "        plt.title(f\"Positions of Similar Sentences in {transcript_name}\")\n",
    "        plt.xlabel('Sentence Number')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.yticks(range(len(key_sentences)), [f\"Key {i}\" for i in range(len(key_sentences))])\n",
    "        plt.xlim(0, df['sentence_number'].max())  # Set the x-axis range based on the max sentence number\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No similar sentences found in {transcript_name}\")\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# C. Highlight sentences in transcript text, grouped by source file\n",
    "def highlight_sentences_grouped_by_file(transcript_data, key_idx, window_size=5):\n",
    "    for df in transcript_data:\n",
    "        transcript_name = df['source_file'].iloc[0]\n",
    "        def color_sentence(row):\n",
    "            if row[f'similarity_with_key_{key_idx}'] >= similarity_threshold:\n",
    "                return f\"<mark>{row['text']}</mark>\"  # Highlight matching sentences\n",
    "            else:\n",
    "                return row['text']\n",
    "\n",
    "        # Find the rows with similar sentences\n",
    "        similar_sentences = df[df[f'similarity_with_key_{key_idx}'] >= similarity_threshold]\n",
    "\n",
    "        if similar_sentences.empty:\n",
    "            print(f\"No similar sentences found in {transcript_name} for key sentence {key_idx}\")\n",
    "            continue\n",
    "\n",
    "        # Store the indices of matching sentences\n",
    "        matching_sentence_indices = similar_sentences.index.tolist()\n",
    "\n",
    "        # Gather a window of sentences around each matching sentence\n",
    "        selected_indices = set()\n",
    "\n",
    "        for idx in matching_sentence_indices:\n",
    "            start_idx = max(0, idx - window_size)  # Ensure we don't go below index 0\n",
    "            end_idx = min(len(df) - 1, idx + window_size)  # Ensure we don't exceed the number of rows\n",
    "            selected_indices.update(range(start_idx, end_idx + 1))  # Add indices to the set\n",
    "\n",
    "        # Filter the DataFrame to include only the selected indices\n",
    "        window_df = df.loc[sorted(selected_indices)].copy()\n",
    "\n",
    "        # Apply highlighting only within the window\n",
    "        window_df['highlighted_sentence'] = window_df.apply(color_sentence, axis=1)\n",
    "\n",
    "        # Initialize transcript_html to hold the highlighted sentences\n",
    "        transcript_html = ' '.join(window_df['highlighted_sentence'].tolist())\n",
    "\n",
    "        # Display the file name and the highlighted sentences\n",
    "        display(HTML(f\"<h3>{transcript_name}</h3>\"))\n",
    "        display(HTML(f\"<p>{transcript_html}</p>\"))\n",
    "\n",
    "\n",
    "# Example usage for the first key sentence\n",
    "highlight_sentences_grouped_by_file(transcript_data, key_idx=0)\n",
    "\n",
    "\n",
    "\n",
    "# Save results as CSV\n",
    "results_df.to_csv('highlighted_sentences.csv', index=False)\n",
    "\n",
    "# Save results as HTML\n",
    "with open('highlighted_sentences.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"<h1>Transcript Highlights</h1>\")\n",
    "    for df in transcript_data:\n",
    "        transcript_name = df['source_file'].iloc[0]\n",
    "        f.write(f\"<h3>{transcript_name}</h3>\")\n",
    "        f.write(f\"<p>{transcript_html}</p>\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Response Segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrhal\\anaconda3\\envs\\NLTKpipeline-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found for question 1\n",
      "No match found for question 3\n",
      "No match found for question 3\n",
      "No match found for question 3\n",
      "                                            question  \\\n",
      "0  one goal we have is for students to take owner...   \n",
      "1  What barriers and challenges have you faced to...   \n",
      "2  What do you think are some of the barriers and...   \n",
      "3    Tell me about your experience teaching science.   \n",
      "4  one goal we have is for students to take owner...   \n",
      "\n",
      "                                            response  \\\n",
      "0  So can you describe a time that you saw this h...   \n",
      "1                                                      \n",
      "2  The year after COVID, right, same deal. But I ...   \n",
      "3  You know what I mean? And I think it's really ...   \n",
      "4  Um, so has there been a time where you've seen...   \n",
      "\n",
      "                                         source_file  \n",
      "0  20240522_INT_HS_UT_Steffan_2024-08-07_transcri...  \n",
      "1  20240522_INT_HS_UT_Steffan_2024-08-07_transcri...  \n",
      "2  20240522_INT_HS_UT_Steffan_2024-08-07_transcri...  \n",
      "3  20240522_INT_HS_UT_Steffan_2024-08-07_transcri...  \n",
      "4  20240531_INT_MS_Vera_2024-08-07_transcription.csv  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# List of interview questions (in the order they were asked)\n",
    "interview_questions = [\n",
    "    \"one goal we have is for students to take ownership of their learning. Has there been a time that you saw this happening in your classroom?\",\n",
    "    \"What barriers and challenges have you faced to enact lessons?\",\n",
    "    \"What do you think are some of the barriers and challenges that students might be facing when you are asking them to engage in these kinds of lessons?\",\n",
    "    \"Tell me about your experience teaching science.\",\n",
    "    # Add more questions here\n",
    "]\n",
    "\n",
    "# 1. Embed the questions\n",
    "question_embeddings = model.encode(interview_questions)\n",
    "\n",
    "# 2. Function to read transcripts\n",
    "def read_transcript_csvs(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    transcript_data = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = os.path.basename(file)  # Track the source file\n",
    "        transcript_data.append(df)\n",
    "    return transcript_data\n",
    "\n",
    "# 3. Load transcript data\n",
    "transcript_data = read_transcript_csvs('data/rawTranscriptFiles/interview_transcripts')\n",
    "\n",
    "# 4. Preprocess and embed the transcript sentences\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def embed_transcript(df):\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Compute embeddings for all sentences (this returns a 2D array)\n",
    "    embeddings = model.encode(df['cleaned_text'].tolist())\n",
    "    \n",
    "    # Convert each embedding into a list and store it in the 'embedding' column\n",
    "    df['embedding'] = list(embeddings)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# def embed_transcript(df):\n",
    "#     df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "#     df['embedding'] = model.encode(df['cleaned_text'].tolist())\n",
    "#     return df\n",
    "\n",
    "# Embed each transcript\n",
    "for df in transcript_data:\n",
    "    df = embed_transcript(df)\n",
    "\n",
    "# 5. Function to find the teacher's responses by locating question embeddings\n",
    "def find_teacher_responses(df, question_embeddings, similarity_threshold=0.65):\n",
    "    responses = []\n",
    "    \n",
    "    # Loop through each question\n",
    "    for q_idx, question_embedding in enumerate(question_embeddings):\n",
    "        # Compute similarities between the question embedding and the transcript embeddings\n",
    "        similarities = cosine_similarity([question_embedding], np.vstack(df['embedding'].values))[0]\n",
    "        \n",
    "        # Find the most similar sentence for each question\n",
    "        question_indices = np.where(similarities >= similarity_threshold)[0]\n",
    "        if len(question_indices) == 0:\n",
    "            print(f\"No match found for question {q_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Take the first occurrence of the question\n",
    "        question_start = question_indices[0]\n",
    "        \n",
    "        # Find the next question or the end of the transcript\n",
    "        if q_idx < len(question_embeddings) - 1:\n",
    "            next_question_embedding = question_embeddings[q_idx + 1]\n",
    "            next_similarities = cosine_similarity([next_question_embedding], np.vstack(df['embedding'].values))[0]\n",
    "            next_question_indices = np.where(next_similarities >= similarity_threshold)[0]\n",
    "            question_end = next_question_indices[0] if len(next_question_indices) > 0 else len(df)\n",
    "        else:\n",
    "            question_end = len(df)  # End of transcript\n",
    "        \n",
    "        # Extract the teacher's response between the question and the next one\n",
    "        response_text = ' '.join(df['text'][question_start + 1:question_end])\n",
    "        responses.append({\n",
    "            'question': interview_questions[q_idx],\n",
    "            'response': response_text,\n",
    "            'source_file': df['source_file'].iloc[0]\n",
    "        })\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# 6. Extract responses for each transcript\n",
    "all_responses = []\n",
    "for df in transcript_data:\n",
    "    responses = find_teacher_responses(df, question_embeddings)\n",
    "    all_responses.extend(responses)\n",
    "\n",
    "# 7. Convert the results to a DataFrame and print/save the responses\n",
    "response_df = pd.DataFrame(all_responses)\n",
    "print(response_df.head())\n",
    "\n",
    "# Save the results to CSV\n",
    "response_df.to_csv('teacher_responses.csv', index=False)\n",
    "\n",
    "# Save the results to Excel\n",
    "response_df.to_excel('teacher_responses.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLTKpipeline-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
