{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription Comparison Tool\n",
    "\n",
    "Explanation\n",
    "Reading CSV Files:\n",
    "\n",
    "The read_csv_files function recursively reads all CSV files in the specified directory and subdirectories.\n",
    "Each CSV file is read into a separate DataFrame, and the source file path is stored in a new column.\n",
    "Processing Transcriptions:\n",
    "\n",
    "Each DataFrame is processed individually to clean the text, tokenize, and perform POS tagging and NER.\n",
    "This keeps the analyses of each transcription separate.\n",
    "Generating Embeddings and Calculating Similarity:\n",
    "\n",
    "Embeddings are generated for the entire cleaned transcript text of each file.\n",
    "The embeddings are stored in a list along with the corresponding file names.\n",
    "The cosine similarity matrix is computed to compare each transcription against all others.\n",
    "Topic Modeling with LDA:\n",
    "\n",
    "LDA is performed on the tokens of each transcription separately.\n",
    "The topics for each transcription are printed out, allowing you to compare the topics between different transcriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pandas numpy nltk spacy sentence-transformers scikit-learn\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "file_path = 'data/Interviews'  # Update this if needed\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Functions\n",
    "def read_csv_files(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = file  # Keep track of the source file\n",
    "        data_frames.append(df)\n",
    "    return data_frames\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pos_tags, entities\n",
    "\n",
    "def perform_lda(tokens_list, num_topics=5):\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "    return lda, corpus, dictionary\n",
    "\n",
    "# Execution\n",
    "directory = file_path\n",
    "transcript_dfs = read_csv_files(directory)\n",
    "\n",
    "embeddings_list = []\n",
    "file_names = []\n",
    "lda_models = []\n",
    "\n",
    "for df in transcript_dfs:\n",
    "    print(f\"DataFrame shape for {df['source_file'].iloc[0]}: {df.shape}\")\n",
    "    df['cleaned_transcript'] = df['text'].apply(preprocess_text)\n",
    "    print(f\"Cleaned transcripts for {df['source_file'].iloc[0]}: {df['cleaned_transcript'].head()}\")\n",
    "\n",
    "    df['tokens'] = df['cleaned_transcript'].apply(tokenize_text)\n",
    "    df[['pos_tags', 'entities']] = df['cleaned_transcript'].apply(\n",
    "        lambda x: pd.Series(spacy_analysis(x))\n",
    "    )\n",
    "\n",
    "    # Generate embeddings\n",
    "    text = ' '.join(df['cleaned_transcript'])\n",
    "    embedding = model.encode(text)\n",
    "    print(f\"Embedding for {df['source_file'].iloc[0]}: {embedding.shape}\")\n",
    "\n",
    "    if embedding is not None and embedding.size > 0:\n",
    "        embeddings_list.append(embedding)\n",
    "        file_names.append(df['source_file'].iloc[0])\n",
    "\n",
    "        # Perform LDA\n",
    "        tokens_list = df['tokens'].tolist()\n",
    "        lda_model, corpus, dictionary = perform_lda(tokens_list)\n",
    "        lda_models.append({\n",
    "            'model': lda_model,\n",
    "            'corpus': corpus,\n",
    "            'dictionary': dictionary,\n",
    "            'source_file': df['source_file'].iloc[0]\n",
    "        })\n",
    "    else:\n",
    "        print(f\"No valid embedding for {df['source_file'].iloc[0]}.\")\n",
    "\n",
    "# Compute similarity matrix\n",
    "if embeddings_list:\n",
    "    embeddings_array = np.vstack(embeddings_list)\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=file_names, columns=file_names)\n",
    "    print(similarity_df)\n",
    "\n",
    "    # Visualize similarity matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_df, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.title('Semantic Similarity Between Transcriptions')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No embeddings to compute similarity matrix.\")\n",
    "\n",
    "# Print topics from LDA models\n",
    "for lda_info in lda_models:\n",
    "    print(f\"Topics for {lda_info['source_file']}:\")\n",
    "    topics = lda_info['model'].print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "file_path = 'data/Interviews'  # Updated to use forward slashes\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Functions\n",
    "def read_csv_files(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)\n",
    "    print(f\"Found {len(csv_files)} CSV files in '{directory}' and its subdirectories.\")\n",
    "    \n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            if 'text' not in df.columns:\n",
    "                print(f\"Warning: 'text' column not found in {file}. Skipping this file.\")\n",
    "                continue\n",
    "            df['source_file'] = file  # Keep track of the source file\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    return data_frames\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pos_tags, entities\n",
    "\n",
    "def perform_lda(tokens_list, num_topics=5):\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "    if not corpus:\n",
    "        print(\"Warning: Corpus is empty. Skipping LDA.\")\n",
    "        return None, None, None\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "    return lda, corpus, dictionary\n",
    "\n",
    "def is_valid_text(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Execution\n",
    "directory = file_path\n",
    "transcript_dfs = read_csv_files(directory)\n",
    "\n",
    "if not transcript_dfs:\n",
    "    print(\"No valid CSV files found. Exiting script.\")\n",
    "    exit()\n",
    "\n",
    "embeddings_list = []\n",
    "file_names = []\n",
    "lda_models = []\n",
    "\n",
    "for idx, df in enumerate(transcript_dfs):\n",
    "    print(f\"\\nProcessing file {idx + 1}/{len(transcript_dfs)}: {df['source_file'].iloc[0]}\")\n",
    "    \n",
    "    # Preprocess text with validity check\n",
    "    df['cleaned_transcript'] = df['text'].apply(lambda x: preprocess_text(x) if is_valid_text(x) else \"\")\n",
    "    \n",
    "    # Check if 'cleaned_transcript' is not empty\n",
    "    if df['cleaned_transcript'].isnull().all() or df['cleaned_transcript'].str.len().max() == 0:\n",
    "        print(f\"Warning: All transcripts are empty after preprocessing in {df['source_file'].iloc[0]}. Skipping this file.\")\n",
    "        continue\n",
    "    \n",
    "    # Tokenize text\n",
    "    df['tokens'] = df['cleaned_transcript'].apply(tokenize_text)\n",
    "    \n",
    "    # Perform Spacy analysis\n",
    "    df[['pos_tags', 'entities']] = df['cleaned_transcript'].apply(\n",
    "        lambda x: pd.Series(spacy_analysis(x))\n",
    "    )\n",
    "    \n",
    "    # Generate embeddings\n",
    "    text = ' '.join(df['cleaned_transcript'])\n",
    "    if not text.strip():\n",
    "        print(f\"Warning: Combined text is empty for {df['source_file'].iloc[0]}. Skipping embedding.\")\n",
    "        continue\n",
    "    try:\n",
    "        embedding = model.encode(text)\n",
    "        embeddings_list.append(embedding)\n",
    "        file_names.append(df['source_file'].iloc[0])\n",
    "        print(\"Embedding generated and appended successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for {df['source_file'].iloc[0]}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Perform LDA\n",
    "    tokens_list = df['tokens'].tolist()\n",
    "    if not tokens_list:\n",
    "        print(f\"Warning: No tokens to perform LDA for {df['source_file'].iloc[0]}. Skipping LDA.\")\n",
    "        continue\n",
    "    try:\n",
    "        lda_model, corpus, dictionary = perform_lda(tokens_list)\n",
    "        if lda_model is not None:\n",
    "            lda_models.append({\n",
    "                'model': lda_model,\n",
    "                'corpus': corpus,\n",
    "                'dictionary': dictionary,\n",
    "                'source_file': df['source_file'].iloc[0]\n",
    "            })\n",
    "            print(\"LDA model trained and appended successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing LDA for {df['source_file'].iloc[0]}: {e}\")\n",
    "\n",
    "# Compute similarity matrix\n",
    "if embeddings_list:\n",
    "    embeddings_array = np.vstack(embeddings_list)\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=file_names, columns=file_names)\n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    print(similarity_df)\n",
    "    \n",
    "    # Visualize similarity matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_df, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.title('Semantic Similarity Between Transcriptions')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No embeddings were generated. Please check the CSV files and preprocessing steps.\")\n",
    "\n",
    "# Print topics from LDA models\n",
    "for lda_info in lda_models:\n",
    "    print(f\"\\nTopics for {lda_info['source_file']}:\")\n",
    "    topics = lda_info['model'].print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity Sentence Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm  # To track progress\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "\n",
    "# 1. Define key sentences\n",
    "key_sentences = [\n",
    "    \"How do you support student learning in your classroom?\",\n",
    "    # Add more key sentences as needed\n",
    "]\n",
    "\n",
    "# 2. Generate embeddings for key sentences\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "key_embeddings = model.encode(key_sentences)\n",
    "\n",
    "# 3. Read and process transcripts\n",
    "def read_transcript_csvs(directory):\n",
    "    # Use '**/*.csv' to match CSV files in all subdirectories\n",
    "    csv_files = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)\n",
    "    print(f\"Found {len(csv_files)} CSV files in '{directory}' and its subdirectories.\")\n",
    "    \n",
    "    transcript_data = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Verify necessary columns\n",
    "            required_columns = ['text', 'sentence_number']\n",
    "            for col in required_columns:\n",
    "                if col not in df.columns:\n",
    "                    print(f\"Warning: '{col}' column missing in {file}. Skipping this file.\")\n",
    "                    raise ValueError(f\"Missing '{col}' column.\")\n",
    "            # Add 'speaker' column if missing\n",
    "            if 'speaker' not in df.columns:\n",
    "                df['speaker'] = 'Unknown'\n",
    "            # Capture relative path for 'source_file' to ensure uniqueness\n",
    "            relative_path = os.path.relpath(file, directory)\n",
    "            df['source_file'] = relative_path  # Use relative path instead of basename\n",
    "            transcript_data.append(df)\n",
    "            print(f\"Successfully processed {relative_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    return transcript_data\n",
    "\n",
    "transcript_data = read_transcript_csvs('data/interviews')\n",
    "\n",
    "# Debugging: Check 'source_file' in each DataFrame\n",
    "for idx, df in enumerate(transcript_data):\n",
    "    print(f\"Transcript {idx + 1}: {df['source_file'].iloc[0]}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\\n\")\n",
    "\n",
    "# Preprocess function for text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def compute_embeddings_in_batches(texts, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Use the function for processing embeddings\n",
    "for idx, df in enumerate(transcript_data):\n",
    "    print(f\"Processing embeddings for Transcript {idx + 1}: {df['source_file'].iloc[0]}\")\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    df['embedding'] = compute_embeddings_in_batches(df['cleaned_text'].tolist(), model)\n",
    "\n",
    "# 4. Calculate similarity scores\n",
    "similarity_threshold = 0.55  # Adjust as needed\n",
    "results = []\n",
    "\n",
    "for key_idx, key_sentence in enumerate(key_sentences):\n",
    "    key_embedding = key_embeddings[key_idx]\n",
    "    for df in transcript_data:\n",
    "        source_file = df['source_file'].values[0] if 'source_file' in df.columns else 'Unknown'\n",
    "        if source_file == 'Unknown':\n",
    "            print(f\"Warning: 'source_file' missing for one of the transcripts. Assigning as 'Unknown'.\")\n",
    "        # Ensure embeddings are present\n",
    "        if 'embedding' not in df.columns or df['embedding'].isnull().all():\n",
    "            print(f\"Warning: No embeddings found in {source_file}. Skipping similarity computation.\")\n",
    "            continue\n",
    "        try:\n",
    "            transcript_embeddings = np.vstack(df['embedding'].values)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error stacking embeddings in {source_file}: {ve}. Skipping.\")\n",
    "            continue\n",
    "        similarities = cosine_similarity([key_embedding], transcript_embeddings)[0]\n",
    "        df[f'similarity_with_key_{key_idx}'] = similarities\n",
    "        similar_sentences = df[df[f'similarity_with_key_{key_idx}'] >= similarity_threshold]\n",
    "        if not similar_sentences.empty:\n",
    "            for idx, row in similar_sentences.iterrows():\n",
    "                # Ensure 'source_file' is present and valid\n",
    "                results.append({\n",
    "                    'source_file': source_file,\n",
    "                    'sentence_number': row['sentence_number'],\n",
    "                    'text': row['text'],  # 'text' contains the transcript sentences\n",
    "                    'speaker': row['speaker'],  # Include speaker information if needed\n",
    "                    'similarity_score': row[f'similarity_with_key_{key_idx}'],\n",
    "                    'key_sentence': key_sentence,\n",
    "                    'key_sentence_index': key_idx\n",
    "                })\n",
    "\n",
    "# Create a DataFrame to hold results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Check if 'source_file' is present and print a sample of results\n",
    "print(\"\\nColumns in results_df:\", results_df.columns.tolist())\n",
    "if not results_df.empty:\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"Warning: 'results_df' is empty. No similar sentences found above the threshold.\")\n",
    "\n",
    "# 5. Visualize the results\n",
    "if not results_df.empty:\n",
    "    # A. Bar plot of occurrences\n",
    "    occurrences = results_df.groupby(['source_file', 'key_sentence']).size().reset_index(name='counts')\n",
    "    \n",
    "    for key_sentence in key_sentences:\n",
    "        key_data = occurrences[occurrences['key_sentence'] == key_sentence]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='source_file', y='counts', data=key_data)\n",
    "        plt.title(f\"Occurrences of sentences similar to: '{key_sentence}'\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Number of Similar Sentences')\n",
    "        plt.xlabel('Transcript File')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # B. Scatter plot of sentence positions\n",
    "    for df in transcript_data:\n",
    "        transcript_name = df['source_file'].iloc[0]\n",
    "        plt.figure(figsize=(12, 2))\n",
    "        has_similar_sentences = False\n",
    "        for key_idx, key_sentence in enumerate(key_sentences):\n",
    "            similar_sentences = df[df[f'similarity_with_key_{key_idx}'] >= similarity_threshold]\n",
    "            if not similar_sentences.empty:\n",
    "                has_similar_sentences = True\n",
    "                plt.scatter(\n",
    "                    similar_sentences['sentence_number'],\n",
    "                    [key_idx]*len(similar_sentences),\n",
    "                    label=f\"Key {key_idx}: {key_sentence[:30]}...\",\n",
    "                    marker='|',\n",
    "                    s=200\n",
    "                )\n",
    "        \n",
    "        if has_similar_sentences:\n",
    "            plt.title(f\"Positions of Similar Sentences in {transcript_name}\")\n",
    "            plt.xlabel('Sentence Number')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.yticks(range(len(key_sentences)), [f\"Key {i}\" for i in range(len(key_sentences))])\n",
    "            plt.xlim(0, df['sentence_number'].max())  # Set the x-axis range based on the max sentence number\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No similar sentences found in {transcript_name}\")\n",
    "    \n",
    "    # C. Highlight sentences in transcript text, grouped by source file\n",
    "    def highlight_sentences_grouped_by_file(transcript_data, key_idx, window_size=5):\n",
    "        html_content = \"<h1>Transcript Highlights</h1>\"\n",
    "        for df in transcript_data:\n",
    "            transcript_name = df['source_file'].iloc[0]\n",
    "            def color_sentence(row):\n",
    "                if row[f'similarity_with_key_{key_idx}'] >= similarity_threshold:\n",
    "                    return f\"<mark>{row['text']}</mark>\"  # Highlight matching sentences\n",
    "                else:\n",
    "                    return row['text']\n",
    "    \n",
    "            # Find the rows with similar sentences\n",
    "            similar_sentences = df[df[f'similarity_with_key_{key_idx}'] >= similarity_threshold]\n",
    "    \n",
    "            if similar_sentences.empty:\n",
    "                print(f\"No similar sentences found in {transcript_name} for key sentence {key_idx}\")\n",
    "                continue\n",
    "    \n",
    "            # Store the indices of matching sentences\n",
    "            matching_sentence_indices = similar_sentences.index.tolist()\n",
    "    \n",
    "            # Gather a window of sentences around each matching sentence\n",
    "            selected_indices = set()\n",
    "    \n",
    "            for idx in matching_sentence_indices:\n",
    "                start_idx = max(0, idx - window_size)  # Ensure we don't go below index 0\n",
    "                end_idx = min(len(df) - 1, idx + window_size)  # Ensure we don't exceed the number of rows\n",
    "                selected_indices.update(range(start_idx, end_idx + 1))  # Add indices to the set\n",
    "    \n",
    "            # Filter the DataFrame to include only the selected indices\n",
    "            window_df = df.loc[sorted(selected_indices)].copy()\n",
    "    \n",
    "            # Apply highlighting only within the window\n",
    "            window_df['highlighted_sentence'] = window_df.apply(color_sentence, axis=1)\n",
    "    \n",
    "            # Initialize transcript_html to hold the highlighted sentences\n",
    "            transcript_html = ' '.join(window_df['highlighted_sentence'].tolist())\n",
    "    \n",
    "            # Add to overall HTML content\n",
    "            html_content += f\"<h3>{transcript_name}</h3>\"\n",
    "            html_content += f\"<p>{transcript_html}</p>\"\n",
    "    \n",
    "        return html_content\n",
    "    \n",
    "    # Generate HTML content\n",
    "    html_content = highlight_sentences_grouped_by_file(transcript_data, key_idx=0)\n",
    "    \n",
    "    # Display in Jupyter Notebook\n",
    "    display(HTML(html_content))\n",
    "    \n",
    "    # Save results as CSV\n",
    "    results_df.to_csv('highlighted_sentences.csv', index=False)\n",
    "    print(\"\\nResults saved to 'highlighted_sentences.csv'.\")\n",
    "    \n",
    "    # Save results as HTML\n",
    "    with open('highlighted_sentences.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    print(\"Results saved to 'highlighted_sentences.html'.\")\n",
    "else:\n",
    "    print(\"No similar sentences to visualize.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Response Segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# List of interview questions (in the order they were asked)\n",
    "interview_questions = [\n",
    "    \"one goal we have is for students to take ownership of their learning. Has there been a time that you saw this happening in your classroom?\",\n",
    "    \"What barriers and challenges have you faced to enact lessons?\",\n",
    "    \"What do you think are some of the barriers and challenges that students might be facing when you are asking them to engage in these kinds of lessons?\",\n",
    "    \"Tell me about your experience teaching science.\",\n",
    "    # Add more questions here\n",
    "]\n",
    "\n",
    "# 1. Embed the questions\n",
    "question_embeddings = model.encode(interview_questions)\n",
    "\n",
    "# 2. Function to read transcripts\n",
    "def read_transcript_csvs(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    transcript_data = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = os.path.basename(file)  # Track the source file\n",
    "        transcript_data.append(df)\n",
    "    return transcript_data\n",
    "\n",
    "# 3. Load transcript data\n",
    "transcript_data = read_transcript_csvs('data/Interviews')\n",
    "\n",
    "# 4. Preprocess and embed the transcript sentences\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def embed_transcript(df):\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Compute embeddings for all sentences (this returns a 2D array)\n",
    "    embeddings = model.encode(df['cleaned_text'].tolist())\n",
    "    \n",
    "    # Convert each embedding into a list and store it in the 'embedding' column\n",
    "    df['embedding'] = list(embeddings)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# def embed_transcript(df):\n",
    "#     df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "#     df['embedding'] = model.encode(df['cleaned_text'].tolist())\n",
    "#     return df\n",
    "\n",
    "# Embed each transcript\n",
    "for df in transcript_data:\n",
    "    df = embed_transcript(df)\n",
    "\n",
    "# 5. Function to find the teacher's responses by locating question embeddings\n",
    "def find_teacher_responses(df, question_embeddings, similarity_threshold=0.65):\n",
    "    responses = []\n",
    "    \n",
    "    # Loop through each question\n",
    "    for q_idx, question_embedding in enumerate(question_embeddings):\n",
    "        # Compute similarities between the question embedding and the transcript embeddings\n",
    "        similarities = cosine_similarity([question_embedding], np.vstack(df['embedding'].values))[0]\n",
    "        \n",
    "        # Find the most similar sentence for each question\n",
    "        question_indices = np.where(similarities >= similarity_threshold)[0]\n",
    "        if len(question_indices) == 0:\n",
    "            print(f\"No match found for question {q_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Take the first occurrence of the question\n",
    "        question_start = question_indices[0]\n",
    "        \n",
    "        # Find the next question or the end of the transcript\n",
    "        if q_idx < len(question_embeddings) - 1:\n",
    "            next_question_embedding = question_embeddings[q_idx + 1]\n",
    "            next_similarities = cosine_similarity([next_question_embedding], np.vstack(df['embedding'].values))[0]\n",
    "            next_question_indices = np.where(next_similarities >= similarity_threshold)[0]\n",
    "            question_end = next_question_indices[0] if len(next_question_indices) > 0 else len(df)\n",
    "        else:\n",
    "            question_end = len(df)  # End of transcript\n",
    "        \n",
    "        # Extract the teacher's response between the question and the next one\n",
    "        response_text = ' '.join(df['text'][question_start + 1:question_end])\n",
    "        responses.append({\n",
    "            'question': interview_questions[q_idx],\n",
    "            'response': response_text,\n",
    "            'source_file': df['source_file'].iloc[0]\n",
    "        })\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# 6. Extract responses for each transcript\n",
    "all_responses = []\n",
    "for df in transcript_data:\n",
    "    responses = find_teacher_responses(df, question_embeddings)\n",
    "    all_responses.extend(responses)\n",
    "\n",
    "# 7. Convert the results to a DataFrame and print/save the responses\n",
    "response_df = pd.DataFrame(all_responses)\n",
    "print(response_df.head())\n",
    "\n",
    "# Save the results to CSV\n",
    "response_df.to_csv('teacher_responses.csv', index=False)\n",
    "\n",
    "# Save the results to Excel\n",
    "response_df.to_excel('teacher_responses.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLTKpipeline-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
