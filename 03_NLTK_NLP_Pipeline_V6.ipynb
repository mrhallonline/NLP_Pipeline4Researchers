{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Kevin's NLTK NLP Text Analytics Pipeline V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.00 Installing Libraries and Dependencies\n",
    "\n",
    "#needs 3.10 or 3.11 for spaCy \n",
    "\n",
    "1. conda update -n base -c conda-forge conda\n",
    "2. conda create -n nltk-env python=3.11                 \n",
    "3. conda activate nltk-env\n",
    "4. conda install ipykernel jupyter nltk pandas numpy plotly matplotlib ipywidgets openpyxl seaborn -c conda-forge\n",
    "5. pip install spacy\n",
    "6. pip install sentence-transformers scikit-learn textblob nbformat ipython\n",
    "6. python -m spacy download en_core_web_sm\n",
    "7. python -m spacy download en_core_web_trf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.01 NLTK Preprocessing and tokenization for Peeking under the Hood Text Analytics\n",
    "\n",
    "Running the code cell below will result in us pulling the following token types for each of our seven corpora.\n",
    "1. word tokens - formed from decomposing sentences into their constituent pieces.\n",
    "2. NLTK text objects - The NLTK library has a unique tokenizer that adds additional metadata to the word token which allows for unique analysis as compared to normal word tokens.\n",
    "3. original sentence tokens - based on numerous features, but normally bounded by punctuation marks. This is why we normally tokenize this first before other text processing.\n",
    "4. normalized sentence tokens - original sentence tokens that have been lowercased, and had stop words, punctuations, and special chracters removed.\n",
    "\n",
    "You can always figure out what type of data corpus you are dealing with by running these print checks. It is also extremely important to also note the importance of keeping your documents categorized, lest they get out of control. The more processing and feature extractions you do, the more you may end up with more different buckets of data to keep up with.\n",
    "* We will look at the utility of each token type in these modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV to corpus\n",
    "folder_path = 'data/outputFiles/csvOutputs/diarizedTranscripts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob  # For sentiment analysis (optional)\n",
    "\n",
    "# Load SpaCy model for preprocessing and analysis (en_core_web_sm for English)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Download NLTK stopwords and tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # For sentence and word tokenization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Function to read CSV files from a folder and its subfolders\n",
    "def read_transcripts_from_folder(base_dir):\n",
    "    csv_files = glob.glob(os.path.join(base_dir, '**/*.csv'), recursive=True)\n",
    "    transcripts = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        transcripts.append(df)\n",
    "    return transcripts\n",
    "\n",
    "# 2. Text preprocessing with SpaCy (no stopword removal yet)\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text using SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize and remove punctuation, no stopword removal yet\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 3. SpaCy POS, NER, and Dependency parsing analysis\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]  # Part-of-speech tagging\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]  # Named entity recognition\n",
    "    dependencies = [(token.text, token.dep_, token.head.text) for token in doc]  # Dependency parsing\n",
    "    return pos_tags, entities, dependencies\n",
    "\n",
    "# 4. Sentiment analysis using TextBlob\n",
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity  # Sentiment polarity and subjectivity\n",
    "\n",
    "# 5. NLTK tokenization and text object creation\n",
    "def nltk_tokenization_and_text_object(df):\n",
    "    # Sentence tokens of unprocessed and processed text\n",
    "    df['nltk_unprocessed_sentence_tokens'] = df['text'].apply(nltk.sent_tokenize)  # Unprocessed sentence tokens\n",
    "    df['nltk_processed_sentence_tokens'] = df['cleaned_text'].apply(nltk.sent_tokenize)  # Processed sentence tokens\n",
    "    \n",
    "    # Word tokens of unprocessed and processed text\n",
    "    df['nltk_unprocessed_word_tokens'] = df['text'].apply(nltk.word_tokenize)  # Unprocessed word tokens\n",
    "    df['nltk_processed_word_tokens'] = df['cleaned_text'].apply(nltk.word_tokenize)  # Processed word tokens\n",
    "    \n",
    "    # NLTK Text object for processed text\n",
    "    df['nltk_text_object'] = df['nltk_unprocessed_word_tokens'].apply(nltk.Text)  # NLTK Text object for processed text\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 6. Apply preprocessing and analysis to the 'text' column of each CSV\n",
    "def preprocess_and_analyze_transcripts(transcripts):\n",
    "    results = []\n",
    "    \n",
    "    for df in transcripts:\n",
    "        df['cleaned_text'] = df['text'].apply(preprocess_text)  # Preprocess the text\n",
    "        \n",
    "        # Apply SpaCy analysis (POS tags, NER, Dependency parsing)\n",
    "        df['pos_tags'], df['entities'], df['dependencies'] = zip(*df['text'].apply(spacy_analysis))\n",
    "        \n",
    "        # Word count and sentence length\n",
    "        df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "        df['sentence_length'] = df['text'].apply(lambda x: len(x.split()))  # Original text sentence length\n",
    "        \n",
    "        # Sentiment analysis (polarity and subjectivity)\n",
    "        df['sentiment_polarity'], df['sentiment_subjectivity'] = zip(*df['text'].apply(sentiment_analysis))\n",
    "        \n",
    "        # Apply NLTK tokenization and text object creation\n",
    "        df = nltk_tokenization_and_text_object(df)\n",
    "        \n",
    "        results.append(df)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 7. Main pipeline function to process CSV files\n",
    "def process_csv_files(base_dir):\n",
    "    # Step 1: Read all transcripts from the CSV files\n",
    "    transcripts = read_transcripts_from_folder(base_dir)\n",
    "    \n",
    "    # Step 2: Preprocess and analyze the transcripts\n",
    "    processed_transcripts = preprocess_and_analyze_transcripts(transcripts)\n",
    "    \n",
    "    return processed_transcripts\n",
    "\n",
    "# Example usage of the pipeline\n",
    "base_dir = 'data/Interviews'  # Folder containing the CSV transcript files\n",
    "output_dir = 'data/Processed'  # Folder to save the processed CSV files\n",
    "\n",
    "# Ensure the output directory exists (create it if it doesn't)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process the CSV files\n",
    "processed_transcripts = process_csv_files(base_dir)\n",
    "\n",
    "# Save the processed CSV files with reordered columns\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    # Define the desired column order\n",
    "    column_order = ['sentence_number', 'speaker', 'cleaned_text', 'entities', 'word_count', 'sentence_length', \n",
    "                    'sentiment_polarity', 'sentiment_subjectivity', 'nltk_unprocessed_sentence_tokens', \n",
    "                    'nltk_processed_sentence_tokens', 'nltk_unprocessed_word_tokens', 'nltk_processed_word_tokens', \n",
    "                    'nltk_text_object', 'pos_tags', 'dependencies']\n",
    "    \n",
    "    # Save the DataFrame with reordered columns\n",
    "    df[column_order].to_csv(os.path.join(output_dir, f'processed_transcript_{i}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 rows of the first processed transcript\n",
    "print(processed_transcripts[1].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.00 Peeking Under The Hood Text Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.01 Average Sentence Length\n",
    "Average Sentence Length uses the total number of words and total number of sentences\n",
    "in a corpus to calculate exactly what it says: the average sentence length.\n",
    "While the equation is very basic and straightforward it provides information that can\n",
    "be used to infer, for example, how complex sentences are on average throughout a\n",
    "given text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to calculate and print metrics (sentence and word count, average sentence length)\n",
    "def calculate_metrics(processed_transcripts):\n",
    "    for i, df in enumerate(processed_transcripts):\n",
    "        # Retrieve sentence and word tokens from the DataFrame\n",
    "        sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().tolist()  # Flatten the sentence tokens\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "\n",
    "        # Ensure there are sentences to avoid division by zero\n",
    "        if sentence_tokens:\n",
    "            average_sentence_length = len(word_tokens) / len(sentence_tokens)\n",
    "        else:\n",
    "            average_sentence_length = 0  # Default to zero if no sentences\n",
    "\n",
    "        # Print results for each transcript\n",
    "        print(f\"\\nTranscript {i + 1}:\")\n",
    "        print(\"Number of sentences:\", len(sentence_tokens))\n",
    "        print(\"Number of word tokens:\", len(word_tokens))\n",
    "        print(\"Average sentence length:\", average_sentence_length)\n",
    "\n",
    "# 2. Run the metric calculation on the processed transcripts\n",
    "calculate_metrics(processed_transcripts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.02 Average Word Length Distribution\n",
    "Another fairly straightforward measure that can provide insight into how long, on average, words are in a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to calculate and print average word length for each transcript\n",
    "def calculate_avg_word_length(processed_transcripts):\n",
    "    for i, df in enumerate(processed_transcripts):\n",
    "        # Retrieve word tokens from the DataFrame (unprocessed word tokens)\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "\n",
    "        # Calculate the average word length if there are word tokens\n",
    "        if word_tokens:\n",
    "            avg_word_length = sum(len(word) for word in word_tokens) / len(word_tokens)\n",
    "        else:\n",
    "            avg_word_length = 0  # Default to zero if no word tokens\n",
    "\n",
    "        # Print results for each transcript\n",
    "        print(f\"\\nTranscript {i + 1}:\")\n",
    "        print(\"Number of word tokens:\", len(word_tokens))\n",
    "        print(\"Average word length:\", avg_word_length)\n",
    "\n",
    "# 2. Run the average word length calculation on the processed transcripts\n",
    "calculate_avg_word_length(processed_transcripts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.03 Lexical Diversity\n",
    "Lexical diversity quantifies the variety of unique words found in a document. It produces a numerical measure that indicates how diverse the vocabulary is that is used in a text. Broadly speaking, scores of (0.8 - 1) are considered extremely high and difficult to maintain in typical communicative texts. Scores of 0.4-0.79 are considered moderate to high; most high-quality texts fall in this range. Scores of (0 - 0.39) are considered low lexical diversity and tend to suggest highly specialized or technical language usage (e.g., instruction manuals) or language aimed at young readers. This measure is sensitive to corpus length (longer corpora have more opportunities to repeat words), but comparing lexical diversity scores can allow for quantitative comparison that might suggest potential changes in how the usage of language may differ between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to calculate and print lexical diversity for each transcript\n",
    "def calculate_lexical_diversity(processed_transcripts):\n",
    "    for i, df in enumerate(processed_transcripts):\n",
    "        # Retrieve word tokens from the DataFrame (unprocessed word tokens)\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "\n",
    "        # Calculate the lexical diversity if there are word tokens\n",
    "        if word_tokens:\n",
    "            lexical_diversity = len(set(word_tokens)) / len(word_tokens)  # Unique words divided by total words\n",
    "        else:\n",
    "            lexical_diversity = 0  # Default to zero if no word tokens\n",
    "\n",
    "        # Print results for each transcript\n",
    "        print(f\"\\nTranscript {i + 1}:\")\n",
    "        print(\"Number of word tokens:\", len(word_tokens))\n",
    "        print(\"Lexical diversity:\", lexical_diversity)\n",
    "\n",
    "# 2. Run the lexical diversity calculation on the processed transcripts\n",
    "calculate_lexical_diversity(processed_transcripts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.04 Unique Words Over Time\n",
    "Unique words can be used to identify the frequency of words that appear only once in a given corpus. We can also print a list of these word tokens. Looking at unique words between or across text corpora can allow us to look for the appearances and disappearances of specialized educational terminology over time. To find the frequency (number) of unique words, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to calculate and print the number of unique words for each transcript\n",
    "def calculate_unique_words(processed_transcripts):\n",
    "    for i, df in enumerate(processed_transcripts):\n",
    "        # Retrieve word tokens from the DataFrame (unprocessed word tokens)\n",
    "        word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "\n",
    "        # Calculate the number of unique words\n",
    "        unique_words = set(word_tokens)\n",
    "        unique_word_count = len(unique_words)\n",
    "\n",
    "        # Print results for each transcript\n",
    "        print(f\"\\nTranscript {i + 1}:\")\n",
    "        print(\"Number of unique words:\", unique_word_count)\n",
    "\n",
    "# 2. Run the unique word count calculation on the processed transcripts\n",
    "calculate_unique_words(processed_transcripts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.05 Twenty-Five Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# 1. Function to calculate and print the top 25 most common words for each transcript\n",
    "def calculate_most_common_words(processed_transcripts, top_n=25):\n",
    "    for i, df in enumerate(processed_transcripts):\n",
    "        # Retrieve the NLTK text object from the DataFrame (which is based on unprocessed word tokens)\n",
    "        text_objects = df['nltk_text_object'].values\n",
    "\n",
    "        # Check if text_objects exist and are not empty\n",
    "        if len(text_objects) > 0:\n",
    "            # Generate a frequency distribution for the text objects (word tokens)\n",
    "            freq_dist = nltk.FreqDist([word for text_obj in text_objects for word in text_obj])\n",
    "\n",
    "            # Get the top N most common words\n",
    "            most_common_words = freq_dist.most_common(top_n)\n",
    "\n",
    "            # Print results for each transcript\n",
    "            print(f\"\\nMost common words in Transcript {i + 1}:\")\n",
    "            print(most_common_words)\n",
    "        else:\n",
    "            print(f\"No text objects available for Transcript {i + 1}\")\n",
    "\n",
    "# 2. Run the frequency distribution calculation on the processed transcripts\n",
    "calculate_most_common_words(processed_transcripts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.06 Display all unique words found in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a dictionary where each key is the transcript index and value is a set of word tokens\n",
    "corpora_tokens = {\n",
    "    f'Transcript {i + 1}': set(df['nltk_unprocessed_word_tokens'].explode().tolist())\n",
    "    for i, df in enumerate(processed_transcripts)\n",
    "}\n",
    "\n",
    "# 2. Function to find words unique to each transcript compared to others\n",
    "def find_unique_words(corpora_tokens):\n",
    "    unique_words = {}\n",
    "    for corpus_name, tokens in corpora_tokens.items():\n",
    "        # Start with the current transcript's tokens\n",
    "        all_other_tokens = set()\n",
    "        for other_corpus_name, other_tokens in corpora_tokens.items():\n",
    "            if corpus_name != other_corpus_name:\n",
    "                all_other_tokens.update(other_tokens)\n",
    "        \n",
    "        # Unique words are those not in the union of all other tokens\n",
    "        unique_words[corpus_name] = tokens - all_other_tokens\n",
    "    return unique_words\n",
    "\n",
    "# 3. Find words unique to each transcript\n",
    "unique_words_by_transcript = find_unique_words(corpora_tokens)\n",
    "\n",
    "# 4. Print unique words for each transcript\n",
    "for transcript_name, unique_words in unique_words_by_transcript.items():\n",
    "    print(f\"Words exclusive to {transcript_name}:\", sorted(unique_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.07 Most frequently used words across all corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# 1. Aggregate all word tokens from all transcripts into a single list\n",
    "all_tokens = []\n",
    "for df in processed_transcripts:\n",
    "    word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "    all_tokens.extend(word_tokens)\n",
    "\n",
    "# 2. Calculate the frequency distribution of all tokens\n",
    "token_freq_dist = Counter(all_tokens)\n",
    "\n",
    "# 3. Find the most common words across all transcripts (adjust the number as needed)\n",
    "most_common_words = token_freq_dist.most_common(100)\n",
    "\n",
    "# 4. Function to print data in columns\n",
    "def print_in_columns(data, columns=3):\n",
    "    # Split the data into chunks of size 'columns'\n",
    "    for i in range(0, len(data), columns):\n",
    "        chunk = data[i:i + columns]\n",
    "        # Format and print each chunk\n",
    "        print(\"  \".join(f\"{word}: {freq}\" for word, freq in chunk))\n",
    "\n",
    "# 5. Print the most common words in columns\n",
    "print(\"Most frequently used words across all transcripts:\")\n",
    "print_in_columns(most_common_words, columns=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.08 N-grams and collocations\n",
    "N-grams point out recurring word combinations found throughout the text corpus. For example, \"spring break\" is an example of a bigrams while \"New York City\" is a trigrams. Bigrams and repeated collocations of words convey a lot of information about the contents of the text corpus.\n",
    "To generate an ordered list of the most common bigrams, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the output directory for n-gram frequencies\n",
    "output_directory = 'data/outputFiles/ngramFrequencies'\n",
    "os.makedirs(output_directory, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# 1. Loop through each transcript and find bigrams and trigrams\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    # Retrieve word tokens from the DataFrame (unprocessed word tokens)\n",
    "    word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "    \n",
    "    # Find bigrams\n",
    "    bigram_finder = BigramCollocationFinder.from_words(word_tokens)\n",
    "    bigrams = bigram_finder.ngram_fd.items()\n",
    "    bigrams_sorted = sorted(bigrams, key=lambda item: item[1], reverse=True)\n",
    "    bigram_df = pd.DataFrame(bigrams_sorted, columns=['Bigram', 'Frequency'])\n",
    "    \n",
    "    # Save bigrams to CSV\n",
    "    bigram_filename = os.path.join(output_directory, f\"transcript_{i + 1}_bigrams.csv\")\n",
    "    bigram_df.to_csv(bigram_filename, index=False)\n",
    "    print(f\"Top 50 bigrams for Transcript {i + 1} saved to {bigram_filename}\")\n",
    "    \n",
    "    # Find trigrams\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(word_tokens)\n",
    "    trigrams = trigram_finder.ngram_fd.items()\n",
    "    trigrams_sorted = sorted(trigrams, key=lambda item: item[1], reverse=True)\n",
    "    trigram_df = pd.DataFrame(trigrams_sorted, columns=['Trigram', 'Frequency'])\n",
    "    \n",
    "    # Save trigrams to CSV\n",
    "    trigram_filename = os.path.join(output_directory, f\"transcript_{i + 1}_trigrams.csv\")\n",
    "    trigram_df.to_csv(trigram_filename, index=False)\n",
    "    print(f\"Top 50 trigrams for Transcript {i + 1} saved to {trigram_filename}\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "# 1. Loop through each transcript to find and display top bigrams and trigrams\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    # Retrieve word tokens from the DataFrame (unprocessed word tokens)\n",
    "    word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the word tokens\n",
    "    \n",
    "    # Display frequency of highest 50 bigrams\n",
    "    print(f\"Top 50 bigrams for Transcript {i + 1}:\")\n",
    "    bigram_finder = BigramCollocationFinder.from_words(word_tokens)\n",
    "    bigram_finder.ngram_fd.tabulate(50)  # Display top 50 bigrams\n",
    "    \n",
    "    # Display frequency of highest 50 trigrams\n",
    "    print(f\"Top 50 trigrams for Transcript {i + 1}:\")\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(word_tokens)\n",
    "    trigram_finder.ngram_fd.tabulate(50)  # Display top 50 trigrams\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.09 Concordance\n",
    "Concordance is an NLTK Text object method that also looks for word distribution, but specifically searches for words found before and after a specific word of choice. Concordance allows us to find out how words are used contextually throughout a corpus. This can be particularly powerful when looking at trends over time or between groups. For example, in the sample below we search for the all the contextual occurrences of the word “pi” in our seven separate corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK data files are downloaded\n",
    "nltk.download('punkt')  # For word_tokenize\n",
    "\n",
    "# Specify the folder path where CSV files are stored\n",
    "folder_path = 'data/Interviews/'  # Replace with your actual folder path\n",
    "\n",
    "# Dictionary to store NLTK Text objects for each transcript\n",
    "nltk_texts = {}\n",
    "\n",
    "# Function to read CSV files from a folder and its subfolders\n",
    "def read_transcripts_from_folder(base_dir):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Read the CSV file into a pandas DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Check if 'text' column exists\n",
    "                if 'text' in df.columns:\n",
    "                    # Combine all text data into a single string\n",
    "                    text_data = ' '.join(df['text'].dropna().astype(str))\n",
    "                    # Tokenize the text data\n",
    "                    tokens = nltk.word_tokenize(text_data)\n",
    "                    # Create an NLTK Text object\n",
    "                    text_object = nltk.Text(tokens)\n",
    "                    # Store the Text object in the dictionary with the file path as the key\n",
    "                    nltk_texts[file_path] = text_object\n",
    "                else:\n",
    "                    print(f\"'text' column not found in {file_path}\")\n",
    "\n",
    "# Process the transcripts and create NLTK Text objects\n",
    "read_transcripts_from_folder(folder_path)\n",
    "\n",
    "# Example: Use concordance method to find occurrences of a word across all texts\n",
    "word_to_search = 'equity'  # Replace with the word you want to search\n",
    "\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    print(f\"Concordance for '{word_to_search}' in {file_path}:\")\n",
    "    text_obj.concordance(word_to_search, width=150)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concordance with Multiple Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_search = ['equity', 'students', 'teachers']  # Add more words as needed\n",
    "\n",
    "for word in words_to_search:\n",
    "    print(f\"\\nConcordance for '{word}':\")\n",
    "    for file_path, text_obj in nltk_texts.items():\n",
    "        print(f\"File: {file_path}\")\n",
    "        text_obj.concordance(word, width=150)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    print(f\"Top 10 bigrams for {file_path}:\")\n",
    "    bigram_finder = BigramCollocationFinder.from_words(text_obj)\n",
    "    bigrams = bigram_finder.ngram_fd.most_common(10)\n",
    "    print(bigrams)\n",
    "    \n",
    "    print(f\"Top 10 trigrams for {file_path}:\")\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(text_obj)\n",
    "    trigrams = trigram_finder.ngram_fd.most_common(10)\n",
    "    print(trigrams)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path, text_obj in nltk_texts.items():\n",
    "    total_words = len(text_obj)\n",
    "    unique_words = len(set(text_obj))\n",
    "    lexical_diversity = unique_words / total_words if total_words > 0 else 0\n",
    "    print(f\"Lexical diversity for {file_path}: {lexical_diversity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    fdist = FreqDist(text_obj)\n",
    "    print(f\"Top 10 most common words in {file_path}:\")\n",
    "    print(fdist.most_common(10))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Concordance-like Contexts (Similar Words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path, text_obj in nltk_texts.items():\n",
    "    print(f\"Words similar to 'equity' in {file_path}:\")\n",
    "    text_obj.similar('equity')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Context Dispersion Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words_to_track = ['equity', 'students', 'teachers']  # Add the words you want to track\n",
    "\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    print(f\"Dispersion plot for {file_path}:\")\n",
    "    text_obj.dispersion_plot(words_to_track)\n",
    "    plt.show()  # Show the plot for each file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Word Concordance Similarity:\n",
    "You can compare concordances of different words to see which words appear in similar contexts. This can help in finding patterns of usage between related terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'equity'\n",
    "word2 = 'diversity'\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    print(f\"Similar contexts for '{word1}' and '{word2}' in {file_path}:\")\n",
    "    text_obj.common_contexts([word1, word2])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Named Entity Recognition (NER): Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Key Collocation with Filtering: \n",
    "You can filter out common stop words to focus on more meaningful bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    bigram_finder = BigramCollocationFinder.from_words([word for word in text_obj if word.lower() not in stop_words])\n",
    "    print(f\"Top bigrams (filtered) in {file_path}:\")\n",
    "    print(bigram_finder.ngram_fd.most_common(10))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Sentiment Analysis (Using TextBlob):\n",
    "You can integrate sentiment analysis into the pipeline to examine the sentiment polarity of different sections of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    text = ' '.join(text_obj)  # Convert NLTK Text object back to string for sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    print(f\"Sentiment polarity for {file_path}: {blob.sentiment.polarity}\")\n",
    "    print(f\"Sentiment subjectivity for {file_path}: {blob.sentiment.subjectivity}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Lists to store results for plotting\n",
    "file_paths = []\n",
    "polarity_scores = []\n",
    "subjectivity_scores = []\n",
    "\n",
    "# Sentiment analysis using TextBlob\n",
    "for file_path, text_obj in nltk_texts.items():\n",
    "    text = ' '.join(text_obj)  # Convert NLTK Text object back to string for sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Append the shortened file name (just the last part of the path) and sentiment scores for plotting\n",
    "    short_name = os.path.basename(file_path)\n",
    "    file_paths.append(short_name)\n",
    "    polarity_scores.append(blob.sentiment.polarity)\n",
    "    subjectivity_scores.append(blob.sentiment.subjectivity)\n",
    "    \n",
    "    # Print the sentiment analysis results\n",
    "    print(f\"Sentiment polarity for {file_path}: {blob.sentiment.polarity}\")\n",
    "    print(f\"Sentiment subjectivity for {file_path}: {blob.sentiment.subjectivity}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plotting sentiment polarity and subjectivity\n",
    "plt.figure(figsize=(12, 6))  # Increased figure size for better readability\n",
    "\n",
    "# Plot polarity\n",
    "plt.plot(file_paths, polarity_scores, marker='o', label='Polarity', color='blue')\n",
    "\n",
    "# Plot subjectivity\n",
    "plt.plot(file_paths, subjectivity_scores, marker='o', label='Subjectivity', color='green')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title('Sentiment Polarity and Subjectivity Across Transcripts')\n",
    "plt.xlabel('Transcript File')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.00 Word Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.01 Bag of Words Frequency distribution\n",
    "\n",
    "This will search for each word in the bag of words to find its frequency in each text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "\n",
    "# List of keywords to track\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying'])\n",
    "\n",
    "# Initialize a dictionary to hold frequency distributions for each corpus\n",
    "freq_distributions = {}\n",
    "\n",
    "# Process the transcripts that have been previously preprocessed into 'processed_transcripts'\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    # Get word tokens for each transcript\n",
    "    word_tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()  # Flatten the list of tokens\n",
    "\n",
    "    # Calculate the frequency distribution of the tokens\n",
    "    freq_dist = FreqDist(word_tokens)\n",
    "    \n",
    "    # Store the frequency of each keyword in the dictionary for this corpus\n",
    "    corpus_name = f\"Transcript {i + 1}\"\n",
    "    freq_distributions[corpus_name] = {word: freq_dist[word] for word in keywords}\n",
    "\n",
    "# Convert the frequency distributions to a DataFrame for easy visualization and analysis\n",
    "freq_df = pd.DataFrame(freq_distributions)\n",
    "\n",
    "# Print a portion of the DataFrame for inspection (first 35 rows)\n",
    "print(freq_df[:35])\n",
    "\n",
    "# Save the result to a CSV file if needed\n",
    "# freq_df.to_csv('keyword_frequencies.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.02 Bag of Words Frequency Distribution with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example keywords to track\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', \n",
    "            'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Number of transcripts (corpora)\n",
    "num_corpora = len(processed_transcripts)\n",
    "\n",
    "# Create subplots for each transcript\n",
    "fig, axes = plt.subplots(num_corpora, 1, figsize=(25, num_corpora*9), sharex=True)\n",
    "\n",
    "# Convert axes to an array if it's not (happens when num_corpora is 1)\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "# Iterate over each transcript and plot keyword occurrences\n",
    "for ax, (i, df) in zip(axes, enumerate(processed_transcripts)):\n",
    "    ax.set_title(f\"Transcript {i + 1}\")\n",
    "    \n",
    "    # Get the sentence tokens from each transcript\n",
    "    sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().tolist()  # Flatten sentence tokens\n",
    "    \n",
    "    # Plot the occurrences of each keyword\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(i + 1, j + 1) for j, sentence in enumerate(sentence_tokens) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences]\n",
    "        \n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "        ax.scatter(sentence_nums, y_values, label=keyword, alpha=0.6, edgecolors='none')\n",
    "    \n",
    "    ax.set_yticks(list(keyword_mapping.values()))\n",
    "    ax.set_yticklabels(list(keyword_mapping.keys()))\n",
    "\n",
    "# Adjust layout\n",
    "plt.xlabel('Sentence Number')\n",
    "plt.ylabel('Keywords')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to an image file before displaying\n",
    "plt.savefig('data/my_keyword_plots.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.03 Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Example keywords to track\n",
    "keywords = ['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind', 'time', 'grade', 'thinking', 'different', 'talk', 'conversation', \n",
    "            'discussion', 'hard', 'saying']\n",
    "\n",
    "# Map keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Create a figure with an increased height to better fit the number of keywords\n",
    "fig = go.Figure()\n",
    "\n",
    "# Process each transcript and plot keyword occurrences\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    transcript_name = f\"Transcript {i + 1}\"\n",
    "    \n",
    "    # Get sentence tokens from each transcript\n",
    "    sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().tolist()  # Flatten sentence tokens\n",
    "    \n",
    "    # Loop through keywords and plot occurrences in sentences\n",
    "    for keyword in keywords:\n",
    "        # Find occurrences of the keyword in the sentences\n",
    "        occurrences = [(transcript_name, j + 1) for j, sentence in enumerate(sentence_tokens) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences if occ[0] == transcript_name]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Adding traces for each keyword\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers', name=keyword,\n",
    "                                 text=[f'Sentence: {num}' for num in sentence_nums],\n",
    "                                 marker=dict(size=8, opacity=0.6)))\n",
    "\n",
    "# Update layout with an appropriate height and labels\n",
    "fig.update_layout(title='Keyword Occurrence Across Sentences in Multiple Transcripts',\n",
    "                  xaxis_title='Sentence Number',\n",
    "                  yaxis=dict(tickmode='array', tickvals=list(keyword_mapping.values()), ticktext=list(keyword_mapping.keys())),\n",
    "                  legend_title='Keywords',\n",
    "                  height=1200)  # Adjust height based on number of keywords\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('data/my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.04 Side to side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Number of transcripts (corpora) to be displayed\n",
    "num_corpora = len(processed_transcripts)\n",
    "\n",
    "# Create subplots for each transcript\n",
    "fig = make_subplots(rows=1, cols=num_corpora, subplot_titles=[f\"Transcript {i + 1}\" for i in range(num_corpora)])\n",
    "\n",
    "# Example keywords to track, sorted for consistency across the plot\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', \n",
    "            'relationship', 'engaging', 'literacy', 'relationship', 'kids', 'connect', \n",
    "            'student', 'classroom', 'teacher', 'teaching', 'school', 'class', 'curriculum', \n",
    "            'learn', 'approach', 'talking', 'discussion', 'love', 'proud', 'like', 'difficult', \n",
    "            'actually', 'know', 'questions', 'think', 'want', 'kind', 'time', 'grade', \n",
    "            'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Index for the current subplot\n",
    "col_index = 1\n",
    "\n",
    "# Loop through each transcript and plot keyword occurrences\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    transcript_name = f\"Transcript {i + 1}\"\n",
    "    \n",
    "    # Get sentence tokens from each transcript\n",
    "    sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().tolist()  # Flatten sentence tokens\n",
    "    \n",
    "    # Plot the occurrences of each keyword in the sentences\n",
    "    for keyword in keywords:\n",
    "        # Find occurrences of the keyword in the sentences\n",
    "        occurrences = [(transcript_name, j + 1) for j, sentence in enumerate(sentence_tokens) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Add traces for each keyword to the respective subplot\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers', name=keyword,\n",
    "                                 text=[f'Sentence: {num}' for num in sentence_nums],\n",
    "                                 marker=dict(size=10, opacity=0.5)),  # Adjust marker size for visibility\n",
    "                      row=1, col=col_index)\n",
    "\n",
    "    col_index += 1  # Move to the next subplot for the next transcript\n",
    "\n",
    "# Update layout to ensure all keywords are visible\n",
    "fig.update_layout(\n",
    "    title='Keyword Occurrence Across Sentences in Multiple Transcripts',\n",
    "    xaxis_title='Sentence Number',\n",
    "    yaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=list(keyword_mapping.values()),\n",
    "        ticktext=list(keyword_mapping.keys())\n",
    "    ),\n",
    "    legend_title='Keywords',\n",
    "    height=1200,  # Increase height to accommodate all keywords\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('data/my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.05 Needs fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Number of transcripts (corpora) to be displayed\n",
    "num_corpora = len(processed_transcripts)\n",
    "\n",
    "# Create subplots for each transcript\n",
    "fig = make_subplots(rows=1, cols=num_corpora, subplot_titles=[f\"Transcript {i+1}\" for i in range(num_corpora)])\n",
    "\n",
    "# Example keywords to track, sorted for consistency across the plot\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', \n",
    "            'relationship', 'engaging', 'literacy', 'relationship', 'kids', 'connect', \n",
    "            'student', 'classroom', 'teacher', 'teaching', 'school', 'class', 'curriculum', \n",
    "            'learn', 'approach', 'talking', 'discussion', 'love', 'proud', 'like', 'difficult', \n",
    "            'actually', 'know', 'questions', 'think', 'want', 'kind', 'time', 'grade', \n",
    "            'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Loop through each transcript and plot keyword occurrences\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    transcript_name = f\"Transcript {i + 1}\"\n",
    "    \n",
    "    # Get sentence tokens from each transcript\n",
    "    sentence_tokens = df['nltk_unprocessed_sentence_tokens'].explode().tolist()  # Flatten sentence tokens\n",
    "    \n",
    "    # Plot the occurrences of each keyword in the sentences\n",
    "    for keyword in keywords:\n",
    "        # Find occurrences of the keyword in the sentences\n",
    "        occurrences = [(transcript_name, j + 1) for j, sentence in enumerate(sentence_tokens) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Add traces for each keyword\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers',\n",
    "                                 text=[f'{keyword} (Sentence: {num})' for num in sentence_nums],\n",
    "                                 marker=dict(size=10, opacity=0.5),\n",
    "                                 showlegend=False),  # This prevents adding each keyword to the legend multiple times\n",
    "                      row=1, col=i+1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Keyword Occurrence Across Sentences in Multiple Transcripts',\n",
    "    xaxis_title='Sentence Number',\n",
    "    yaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=list(keyword_mapping.values()),\n",
    "        ticktext=list(keyword_mapping.keys())\n",
    "    ),\n",
    "    height=1200,  # Increase height to accommodate all keywords\n",
    ")\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('data/my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.00 Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Topic Modeling and Semantic Similarities Across Transcriptions\n",
    "Generating Sentence Embeddings for Each Transcription\n",
    "We'll generate embeddings for each transcription and store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Store embeddings and related info\n",
    "embeddings_list = []\n",
    "file_names = []\n",
    "\n",
    "# Loop through each transcript DataFrame in processed_transcripts\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    # Join the cleaned text from each transcript\n",
    "    text = ' '.join(df['cleaned_text'])  # Assuming cleaned text is stored in 'cleaned_text' column\n",
    "    \n",
    "    # Generate embeddings for the entire transcript text\n",
    "    embedding = model.encode(text)\n",
    "    \n",
    "    # Append the embedding and source file name\n",
    "    embeddings_list.append(embedding)\n",
    "    \n",
    "    # Assuming the transcript DataFrame has a 'source_file' column or use a placeholder like 'Transcript {i+1}'\n",
    "    file_names.append(f\"Transcript {i + 1}\" if 'source_file' not in df.columns else df['source_file'].iloc[0])\n",
    "\n",
    "# Convert embeddings and file names into a dictionary for further use\n",
    "embedding_data = {\n",
    "    'file_names': file_names,\n",
    "    'embeddings': embeddings_list\n",
    "}\n",
    "\n",
    "# Example: Use embeddings_data for further analysis, e.g., comparing semantic similarity\n",
    "print(\"Embeddings have been generated for each transcript.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Transcriptions Using Semantic Similarity\n",
    "We'll compute the cosine similarity between each pair of transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load SentenceTransformer and generate embeddings (from previous code)\n",
    "# Assuming embeddings_list and file_names have already been generated as shown in the previous code\n",
    "\n",
    "# Step 2: Convert list of embeddings to a numpy array\n",
    "embeddings_array = np.vstack(embeddings_list)\n",
    "\n",
    "# Step 3: Calculate cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Step 4: Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=file_names, columns=file_names)\n",
    "\n",
    "# Step 5: Print or visualize the similarity matrix\n",
    "print(similarity_df)\n",
    "\n",
    "# Optional: Save the similarity matrix to a CSV file for further analysis\n",
    "similarity_df.to_csv('data/transcript_similarity_matrix.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on Individual Transcriptions\n",
    "Using Latent Dirichlet Allocation (LDA)\n",
    "First, you need to prepare the data for LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Function to perform LDA\n",
    "def perform_lda(tokens_list, num_topics=5):\n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "\n",
    "    # Filter out extremes to limit the number of features\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "\n",
    "    # Create a bag-of-words representation of the documents\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "    \n",
    "    return lda, corpus, dictionary\n",
    "\n",
    "# Step 1: Extract word tokens from the processed transcripts\n",
    "tokens_list = []\n",
    "\n",
    "for df in processed_transcripts:\n",
    "    # Extract word tokens from each transcript and flatten them into a list\n",
    "    tokens = df['nltk_unprocessed_word_tokens'].explode().tolist()\n",
    "    \n",
    "    # Add tokenized transcript to the list\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "# Step 2: Perform LDA on the extracted tokens\n",
    "lda_model, corpus, dictionary = perform_lda(tokens_list, num_topics=5)\n",
    "\n",
    "# Step 3: Print the topics discovered by the LDA model\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {idx + 1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
