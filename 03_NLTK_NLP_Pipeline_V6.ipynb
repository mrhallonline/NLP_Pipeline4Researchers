{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Kevin's NLTK NLP Text Analytics Pipeline V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.00 Installing Libraries and Dependencies\n",
    "\n",
    "python 3.11\n",
    "libraries needed:\n",
    "nltk\n",
    "pandas\n",
    "numpy\n",
    "\n",
    "\n",
    "conda update -n base -c conda-forge conda\n",
    "conda create -n nltk-env python=3.11\n",
    "conda activate nltk-env\n",
    "conda install ipykernel nltk pandas numpy plotly matplotlib ipywidgets openpyxl -c conda-forge\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade nltk\n",
    "%pip install --upgrade pandas\n",
    "%pip install --upgrade numpy\n",
    "%pip install --upgrade plotly\n",
    "%pip install --upgrade matplotlib\n",
    "%pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.01 NLTK Preprocessing and tokenization for Peeking under the Hood Text Analytics\n",
    "\n",
    "Running the code cell below will result in us pulling the following token types for each of our seven corpora.\n",
    "1. word tokens - formed from decomposing sentences into their constituent pieces.\n",
    "2. NLTK text objects - The NLTK library has a unique tokenizer that adds additional metadata to the word token which allows for unique analysis as compared to normal word tokens.\n",
    "3. original sentence tokens - based on numerous features, but normally bounded by punctuation marks. This is why we normally tokenize this first before other text processing.\n",
    "4. normalized sentence tokens - original sentence tokens that have been lowercased, and had stop words, punctuations, and special chracters removed.\n",
    "\n",
    "You can always figure out what type of data corpus you are dealing with by running these print checks. It is also extremely important to also note the importance of keeping your documents categorized, lest they get out of control. The more processing and feature extractions you do, the more you may end up with more different buckets of data to keep up with.\n",
    "* We will look at the utility of each token type in these modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV to corpus\n",
    "folder_path = 'data/outputFiles/csvOutputs/diarizedTranscripts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. needs 3.10 or 3.11 for spacy \n",
    "2. conda install ipykernel pandas numpy nltk spacy sentence-transformers scikit-learn\n",
    "3. python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinhall/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kevinhall/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob  # For sentiment analysis (optional)\n",
    "\n",
    "# Load SpaCy model for preprocessing and analysis (en_core_web_sm for English)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Download NLTK stopwords and tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # For sentence and word tokenization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Function to read CSV files from a folder and its subfolders\n",
    "def read_transcripts_from_folder(base_dir):\n",
    "    csv_files = glob.glob(os.path.join(base_dir, '**/*.csv'), recursive=True)\n",
    "    transcripts = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        transcripts.append(df)\n",
    "    return transcripts\n",
    "\n",
    "# 2. Text preprocessing with SpaCy (no stopword removal yet)\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text using SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize and remove punctuation, no stopword removal yet\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 3. SpaCy POS, NER, and Dependency parsing analysis\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]  # Part-of-speech tagging\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]  # Named entity recognition\n",
    "    dependencies = [(token.text, token.dep_, token.head.text) for token in doc]  # Dependency parsing\n",
    "    return pos_tags, entities, dependencies\n",
    "\n",
    "# 4. Sentiment analysis using TextBlob\n",
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity  # Sentiment polarity and subjectivity\n",
    "\n",
    "# 5. NLTK tokenization and text object creation\n",
    "def nltk_tokenization_and_text_object(df):\n",
    "    # Sentence tokens of unprocessed and processed text\n",
    "    df['nltk_unprocessed_sentence_tokens'] = df['text'].apply(nltk.sent_tokenize)  # Unprocessed sentence tokens\n",
    "    df['nltk_processed_sentence_tokens'] = df['cleaned_text'].apply(nltk.sent_tokenize)  # Processed sentence tokens\n",
    "    \n",
    "    # Word tokens of unprocessed and processed text\n",
    "    df['nltk_unprocessed_word_tokens'] = df['text'].apply(nltk.word_tokenize)  # Unprocessed word tokens\n",
    "    df['nltk_processed_word_tokens'] = df['cleaned_text'].apply(nltk.word_tokenize)  # Processed word tokens\n",
    "    \n",
    "    # NLTK Text object for processed text\n",
    "    df['nltk_text_object'] = df['nltk_unprocessed_word_tokens'].apply(nltk.Text)  # NLTK Text object for processed text\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 6. Apply preprocessing and analysis to the 'text' column of each CSV\n",
    "def preprocess_and_analyze_transcripts(transcripts):\n",
    "    results = []\n",
    "    \n",
    "    for df in transcripts:\n",
    "        df['cleaned_text'] = df['text'].apply(preprocess_text)  # Preprocess the text\n",
    "        \n",
    "        # Apply SpaCy analysis (POS tags, NER, Dependency parsing)\n",
    "        df['pos_tags'], df['entities'], df['dependencies'] = zip(*df['text'].apply(spacy_analysis))\n",
    "        \n",
    "        # Word count and sentence length\n",
    "        df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "        df['sentence_length'] = df['text'].apply(lambda x: len(x.split()))  # Original text sentence length\n",
    "        \n",
    "        # Sentiment analysis (polarity and subjectivity)\n",
    "        df['sentiment_polarity'], df['sentiment_subjectivity'] = zip(*df['text'].apply(sentiment_analysis))\n",
    "        \n",
    "        # Apply NLTK tokenization and text object creation\n",
    "        df = nltk_tokenization_and_text_object(df)\n",
    "        \n",
    "        results.append(df)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 7. Main pipeline function to process CSV files\n",
    "def process_csv_files(base_dir):\n",
    "    # Step 1: Read all transcripts from the CSV files\n",
    "    transcripts = read_transcripts_from_folder(base_dir)\n",
    "    \n",
    "    # Step 2: Preprocess and analyze the transcripts\n",
    "    processed_transcripts = preprocess_and_analyze_transcripts(transcripts)\n",
    "    \n",
    "    return processed_transcripts\n",
    "\n",
    "# Example usage of the pipeline\n",
    "base_dir = 'data/Interviews'  # Folder containing the CSV transcript files\n",
    "output_dir = 'data/Processed'  # Folder to save the processed CSV files\n",
    "\n",
    "# Ensure the output directory exists (create it if it doesn't)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process the CSV files\n",
    "processed_transcripts = process_csv_files(base_dir)\n",
    "\n",
    "# Save the processed CSV files with reordered columns\n",
    "for i, df in enumerate(processed_transcripts):\n",
    "    # Define the desired column order\n",
    "    column_order = ['sentence_number', 'speaker', 'cleaned_text', 'entities', 'word_count', 'sentence_length', \n",
    "                    'sentiment_polarity', 'sentiment_subjectivity', 'nltk_unprocessed_sentence_tokens', \n",
    "                    'nltk_processed_sentence_tokens', 'nltk_unprocessed_word_tokens', 'nltk_processed_word_tokens', \n",
    "                    'nltk_text_object', 'pos_tags', 'dependencies']\n",
    "    \n",
    "    # Save the DataFrame with reordered columns\n",
    "    df[column_order].to_csv(os.path.join(output_dir, f'processed_transcript_{i}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    start     end                                               text  \\\n",
      "0   3.015   6.157   Okay, all right, so yeah, we're being recorde...   \n",
      "1   6.177  10.220  All right, so some of these early ones are bas...   \n",
      "2  10.280  13.422  And so the first one is really just a very gen...   \n",
      "3  13.943  16.745  So what do you teach here as far as subject an...   \n",
      "4  16.765  21.308  And also then attach to that, are you involved...   \n",
      "5  23.269  30.875  This upcoming year, I am teaching freshman co-...   \n",
      "6  31.670  35.533                      AP biology, and food science.   \n",
      "7  36.193  38.214            So yeah, this will be a four prep year.   \n",
      "8  38.995  44.779  And then outside of that, I coach cross countr...   \n",
      "9  47.460  47.841                                           Oh, wow.   \n",
      "\n",
      "                                               words     speaker  \\\n",
      "0  [{'word': 'Okay,', 'start': 3.015, 'end': 3.45...  SPEAKER_01   \n",
      "1  [{'word': 'All', 'start': 6.177, 'end': 6.237,...  SPEAKER_01   \n",
      "2  [{'word': 'And', 'start': 10.28, 'end': 10.36,...  SPEAKER_01   \n",
      "3  [{'word': 'So', 'start': 13.943, 'end': 14.123...  SPEAKER_01   \n",
      "4  [{'word': 'And', 'start': 16.765, 'end': 16.82...  SPEAKER_01   \n",
      "5  [{'word': 'This', 'start': 23.269, 'end': 23.3...  SPEAKER_00   \n",
      "6  [{'word': 'AP', 'start': 31.67, 'end': 32.01, ...  SPEAKER_00   \n",
      "7  [{'word': 'So', 'start': 36.193, 'end': 36.473...  SPEAKER_00   \n",
      "8  [{'word': 'And', 'start': 38.995, 'end': 39.09...  SPEAKER_00   \n",
      "9  [{'word': 'Oh,', 'start': 47.46, 'end': 47.54,...  SPEAKER_01   \n",
      "\n",
      "   sentence_number                                       cleaned_text  \\\n",
      "0                1            okay all right so yeah be be record now   \n",
      "1                2  all right so some of these early one be basica...   \n",
      "2                3  and so the first one be really just a very gen...   \n",
      "3                4  so what do you teach here as far as subject an...   \n",
      "4                5  and also then attach to that be you involved i...   \n",
      "5                6  this upcoming year I be teach freshman cotaugh...   \n",
      "6                7                        ap biology and food science   \n",
      "7                8              so yeah this will be a four prep year   \n",
      "8                9  and then outside of that I coach cross country...   \n",
      "9               10                                             oh wow   \n",
      "\n",
      "                                            pos_tags  \\\n",
      "0  [( , SPACE), (Okay, INTJ), (,, PUNCT), (all, A...   \n",
      "1  [(All, ADV), (right, ADV), (,, PUNCT), (so, AD...   \n",
      "2  [(And, CCONJ), (so, ADV), (the, DET), (first, ...   \n",
      "3  [(So, ADV), (what, PRON), (do, AUX), (you, PRO...   \n",
      "4  [(And, CCONJ), (also, ADV), (then, ADV), (atta...   \n",
      "5  [(This, DET), (upcoming, ADJ), (year, NOUN), (...   \n",
      "6  [( , SPACE), (AP, PROPN), (biology, NOUN), (,,...   \n",
      "7  [(So, ADV), (yeah, INTJ), (,, PUNCT), (this, P...   \n",
      "8  [(And, CCONJ), (then, ADV), (outside, ADV), (o...   \n",
      "9  [(Oh, INTJ), (,, PUNCT), (wow, INTJ), (., PUNCT)]   \n",
      "\n",
      "                                  entities  \\\n",
      "0                                       []   \n",
      "1                                       []   \n",
      "2                       [(first, ORDINAL)]   \n",
      "3                                       []   \n",
      "4                                       []   \n",
      "5  [(This upcoming year, DATE), (AP, ORG)]   \n",
      "6                              [(AP, ORG)]   \n",
      "7                 [(four prep year, DATE)]   \n",
      "8                                       []   \n",
      "9                                       []   \n",
      "\n",
      "                                        dependencies  word_count  \\\n",
      "0  [( , dep, Okay), (Okay, advmod, right), (,, pu...           9   \n",
      "1  [(All, advmod, right), (right, advmod, are), (...          13   \n",
      "2  [(And, cc, is), (so, advmod, is), (the, det, o...          12   \n",
      "3  [(So, advmod, teach), (what, dobj, teach), (do...          12   \n",
      "4  [(And, cc, attach), (also, advmod, attach), (t...          15   \n",
      "5  [(This, det, year), (upcoming, amod, year), (y...          13   \n",
      "6  [( , dep, AP), (AP, compound, biology), (biolo...           5   \n",
      "7  [(So, advmod, be), (yeah, intj, be), (,, punct...           9   \n",
      "8  [(And, cc, coach), (then, advmod, outside), (o...          13   \n",
      "9  [(Oh, ROOT, Oh), (,, punct, Oh), (wow, intj, O...           2   \n",
      "\n",
      "   sentence_length  sentiment_polarity  sentiment_subjectivity  \\\n",
      "0                9            0.392857                0.517857   \n",
      "1               13            0.192857                0.417857   \n",
      "2               12            0.171667                0.394444   \n",
      "3               12           -0.033333                0.666667   \n",
      "4               15            0.000000                0.050000   \n",
      "5               13            0.000000                0.000000   \n",
      "6                5            0.000000                0.000000   \n",
      "7                9            0.000000                0.000000   \n",
      "8               13            0.000000                0.025000   \n",
      "9                2            0.100000                1.000000   \n",
      "\n",
      "                    nltk_unprocessed_sentence_tokens  \\\n",
      "0  [ Okay, all right, so yeah, we're being record...   \n",
      "1  [All right, so some of these early ones are ba...   \n",
      "2  [And so the first one is really just a very ge...   \n",
      "3  [So what do you teach here as far as subject a...   \n",
      "4  [And also then attach to that, are you involve...   \n",
      "5  [This upcoming year, I am teaching freshman co...   \n",
      "6                   [ AP biology, and food science.]   \n",
      "7          [So yeah, this will be a four prep year.]   \n",
      "8  [And then outside of that, I coach cross count...   \n",
      "9                                         [Oh, wow.]   \n",
      "\n",
      "                      nltk_processed_sentence_tokens  \\\n",
      "0        [  okay all right so yeah be be record now]   \n",
      "1  [all right so some of these early one be basic...   \n",
      "2  [and so the first one be really just a very ge...   \n",
      "3  [so what do you teach here as far as subject a...   \n",
      "4  [and also then attach to that be you involved ...   \n",
      "5  [this upcoming year I be teach freshman cotaug...   \n",
      "6                    [  ap biology and food science]   \n",
      "7            [so yeah this will be a four prep year]   \n",
      "8  [and then outside of that I coach cross countr...   \n",
      "9                                           [oh wow]   \n",
      "\n",
      "                        nltk_unprocessed_word_tokens  \\\n",
      "0  [Okay, ,, all, right, ,, so, yeah, ,, we, 're,...   \n",
      "1  [All, right, ,, so, some, of, these, early, on...   \n",
      "2  [And, so, the, first, one, is, really, just, a...   \n",
      "3  [So, what, do, you, teach, here, as, far, as, ...   \n",
      "4  [And, also, then, attach, to, that, ,, are, yo...   \n",
      "5  [This, upcoming, year, ,, I, am, teaching, fre...   \n",
      "6            [AP, biology, ,, and, food, science, .]   \n",
      "7  [So, yeah, ,, this, will, be, a, four, prep, y...   \n",
      "8  [And, then, outside, of, that, ,, I, coach, cr...   \n",
      "9                                    [Oh, ,, wow, .]   \n",
      "\n",
      "                          nltk_processed_word_tokens  \\\n",
      "0  [okay, all, right, so, yeah, be, be, record, now]   \n",
      "1  [all, right, so, some, of, these, early, one, ...   \n",
      "2  [and, so, the, first, one, be, really, just, a...   \n",
      "3  [so, what, do, you, teach, here, as, far, as, ...   \n",
      "4  [and, also, then, attach, to, that, be, you, i...   \n",
      "5  [this, upcoming, year, I, be, teach, freshman,...   \n",
      "6                  [ap, biology, and, food, science]   \n",
      "7    [so, yeah, this, will, be, a, four, prep, year]   \n",
      "8  [and, then, outside, of, that, I, coach, cross...   \n",
      "9                                          [oh, wow]   \n",
      "\n",
      "                                    nltk_text_object  \n",
      "0  (Okay, ,, all, right, ,, so, yeah, ,, we, 're,...  \n",
      "1  (All, right, ,, so, some, of, these, early, on...  \n",
      "2  (And, so, the, first, one, is, really, just, a...  \n",
      "3  (So, what, do, you, teach, here, as, far, as, ...  \n",
      "4  (And, also, then, attach, to, that, ,, are, yo...  \n",
      "5  (This, upcoming, year, ,, I, am, teaching, fre...  \n",
      "6            (AP, biology, ,, and, food, science, .)  \n",
      "7  (So, yeah, ,, this, will, be, a, four, prep, y...  \n",
      "8  (And, then, outside, of, that, ,, I, coach, cr...  \n",
      "9                                    (Oh, ,, wow, .)  \n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 rows of the first processed transcript\n",
    "print(processed_transcripts[1].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Folder containing CSV files\n",
    "folder_path = 'data/rawTranscriptFiles'\n",
    "\n",
    "# Column name to extract text from\n",
    "column_name = 'text'\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# List to hold the variable names and their lengths\n",
    "variable_names_and_lengths = []\n",
    "\n",
    "# Counter to create corpus variable names like corpus1, corpus2, etc.\n",
    "counter = 1\n",
    "\n",
    "# Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create a variable name based on the counter\n",
    "        base_var_name = f'corpus{counter}'\n",
    "        \n",
    "        # Step 1: Convert CSV file into a dataframe variable\n",
    "        globals()[base_var_name] = df\n",
    "\n",
    "        # Step 2: Extract and store raw text\n",
    "        raw_text_var_name = f'{base_var_name}_rawText'\n",
    "        raw_text = ' '.join(df[column_name].astype(str).tolist())\n",
    "        globals()[raw_text_var_name] = raw_text\n",
    "        \n",
    "        # Step 3: Tokenize raw text\n",
    "        raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "        raw_word_tokens = nltk.word_tokenize(raw_text)\n",
    "        raw_text_obj = nltk.Text(raw_word_tokens)\n",
    "        \n",
    "        df['rawSentenceTokens'] = df[column_name].apply(nltk.sent_tokenize)\n",
    "        df['rawWordTokens'] = df[column_name].apply(nltk.word_tokenize)\n",
    "        df['rawTextObjects'] = df['rawWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_rawSentenceTokens'] = raw_sentence_tokens\n",
    "        globals()[f'{base_var_name}_rawWordTokens'] = raw_word_tokens\n",
    "        globals()[f'{base_var_name}_rawTextObjects'] = raw_text_obj\n",
    "        \n",
    "        # Step 4: Clean text and create processed text\n",
    "        df['processedText'] = df[column_name].apply(clean_text)\n",
    "        \n",
    "        processed_text_var_name = f'{base_var_name}_processedText'\n",
    "        processed_text = ' '.join(df['processedText'].astype(str).tolist())\n",
    "        globals()[processed_text_var_name] = processed_text\n",
    "        \n",
    "        # Step 5: Tokenize processed text\n",
    "        processed_sentence_tokens = [nltk.sent_tokenize(clean_text(sent)) for sent in raw_sentence_tokens]\n",
    "        processed_sentence_tokens_flat = [sent for sublist in processed_sentence_tokens for sent in sublist]\n",
    "        processed_word_tokens = [nltk.word_tokenize(sent) for sent in processed_sentence_tokens_flat]\n",
    "        processed_word_tokens_flat = [word for sublist in processed_word_tokens for word in sublist]\n",
    "        processed_text_obj = nltk.Text(processed_word_tokens_flat)\n",
    "        \n",
    "        df['processedSentenceTokens'] = df['rawSentenceTokens'].apply(lambda x: [nltk.sent_tokenize(clean_text(sent)) for sent in x])\n",
    "        df['processedSentenceTokens'] = df['processedSentenceTokens'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "        df['processedWordTokens'] = df['processedSentenceTokens'].apply(lambda x: [nltk.word_tokenize(sent) for sent in x])\n",
    "        df['processedWordTokens'] = df['processedWordTokens'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "        df['processedTextObjects'] = df['processedWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_processedSentenceTokens'] = processed_sentence_tokens_flat\n",
    "        globals()[f'{base_var_name}_processedWordTokens'] = processed_word_tokens_flat\n",
    "        globals()[f'{base_var_name}_processedTextObjects'] = processed_text_obj\n",
    "        \n",
    "        # Step 6: Remove stop words and create fully processed text\n",
    "        df['fullyProcessedText'] = df['processedText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "        \n",
    "        fully_processed_text_var_name = f'{base_var_name}_fullyProcessedText'\n",
    "        fully_processed_text = ' '.join(df['fullyProcessedText'].astype(str).tolist())\n",
    "        globals()[fully_processed_text_var_name] = fully_processed_text\n",
    "        \n",
    "        # Step 7: Tokenize fully processed text\n",
    "        fully_processed_sentence_tokens = [nltk.sent_tokenize(sent) for sent in processed_sentence_tokens_flat]\n",
    "        fully_processed_sentence_tokens_flat = [sent for sublist in fully_processed_sentence_tokens for sent in sublist]\n",
    "        fully_processed_word_tokens = [nltk.word_tokenize(sent) for sent in fully_processed_sentence_tokens_flat]\n",
    "        fully_processed_word_tokens_flat = [word for sublist in fully_processed_word_tokens for word in sublist]\n",
    "        fully_processed_text_obj = nltk.Text(fully_processed_word_tokens_flat)\n",
    "        \n",
    "        df['fullyProcessedSentenceTokens'] = df['fullyProcessedText'].apply(nltk.sent_tokenize)\n",
    "        df['fullyProcessedWordTokens'] = df['fullyProcessedSentenceTokens'].apply(lambda x: [nltk.word_tokenize(sent) for sent in x])\n",
    "        df['fullyProcessedWordTokens'] = df['fullyProcessedWordTokens'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "        df['fullyProcessedTextObjects'] = df['fullyProcessedWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_fullyProcessedSentenceTokens'] = fully_processed_sentence_tokens_flat\n",
    "        globals()[f'{base_var_name}_fullyProcessedWordTokens'] = fully_processed_word_tokens_flat\n",
    "        globals()[f'{base_var_name}_fullyProcessedTextObjects'] = fully_processed_text_obj\n",
    "        \n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        new_file_path = os.path.join(folder_path, f'processed_{filename}')\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "        \n",
    "        # Store the lengths of each created variable\n",
    "        variable_names_and_lengths.append((raw_text_var_name, len(raw_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_rawSentenceTokens', len(raw_sentence_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_rawWordTokens', len(raw_word_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedText', len(processed_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedSentenceTokens', len(processed_sentence_tokens_flat)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedWordTokens', len(processed_word_tokens_flat)))\n",
    "        variable_names_and_lengths.append((fully_processed_text_var_name, len(fully_processed_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_fullyProcessedSentenceTokens', len(fully_processed_sentence_tokens_flat)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_fullyProcessedWordTokens', len(fully_processed_word_tokens_flat)))\n",
    "        \n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "\n",
    "# Print the lengths of all created variables\n",
    "for var_name, var_length in variable_names_and_lengths:\n",
    "    print(f'{var_name}: {var_length}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.02 Splitting by Sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "transcript_text = \"Section 1: Introduction to AI. AI is a broad field. Section 2: Applications of AI. AI is used in many industries.\"\n",
    "\n",
    "# Split the transcript by \"Section\" followed by any digit and a colon\n",
    "docs = re.split(r\"Section \\d+: \", transcript_text)\n",
    "# Remove any empty strings that might have occurred during splitting\n",
    "docs = [doc.strip() for doc in docs if doc.strip()]\n",
    "\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.00 Peeking Under The Hood Text Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.01 Average Sentence Length\n",
    "Average Sentence Length uses the total number of words and total number of sentences\n",
    "in a corpus to calculate exactly what it says: the average sentence length.\n",
    "While the equation is very basic and straightforward it provides information that can\n",
    "be used to infer, for example, how complex sentences are on average throughout a\n",
    "given text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` is populated with multiple corpus entries\n",
    "for corpus_key, data in all_results.items():\n",
    "    # Retrieve sentence and word tokens from the dictionary\n",
    "    sentence_tokens = data.get('sentence_tokens', [])\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Ensure there are sentences to avoid division by zero\n",
    "    if sentence_tokens:\n",
    "        average_sentence_length = len(word_tokens) / len(sentence_tokens)\n",
    "    else:\n",
    "        average_sentence_length = 0  # Default to zero if no sentences\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of sentences:\", len(sentence_tokens))\n",
    "    print(\"Number of word tokens:\", len(word_tokens))\n",
    "    print(\"Average sentence length:\", average_sentence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.02 Average Word Length Distribution\n",
    "Another fairly straightforward measure that can provide insight into how long, on average, words are in a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` is populated with multiple corpus entries\n",
    "for corpus_key, data in all_results.items():\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Calculate the average word length if there are words in the corpus\n",
    "    if word_tokens:\n",
    "        avg_word_length = sum(len(word) for word in word_tokens) / len(word_tokens)\n",
    "    else:\n",
    "        avg_word_length = 0  # Default to zero if no words to avoid division by zero\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of word tokens:\", len(word_tokens))\n",
    "    print(\"Average word length:\", avg_word_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.03 Lexical Diversity\n",
    "Lexical diversity quantifies the variety of unique words found in a document. It produces a numerical measure that indicates how diverse the vocabulary is that is used in a text. Broadly speaking, scores of (0.8 - 1) are considered extremely high and difficult to maintain in typical communicative texts. Scores of 0.4-0.79 are considered moderate to high; most high-quality texts fall in this range. Scores of (0 - 0.39) are considered low lexical diversity and tend to suggest highly specialized or technical language usage (e.g., instruction manuals) or language aimed at young readers. This measure is sensitive to corpus length (longer corpora have more opportunities to repeat words), but comparing lexical diversity scores can allow for quantitative comparison that might suggest potential changes in how the usage of language may differ between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` is populated with multiple corpus entries\n",
    "for corpus_key, data in all_results.items():\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Calculate the lexical diversity if there are words in the corpus\n",
    "    if word_tokens:\n",
    "        lexical_diversity = len(set(word_tokens)) / len(word_tokens)\n",
    "    else:\n",
    "        lexical_diversity = 0  # Default to zero if no words to avoid division by zero\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of word tokens:\", len(word_tokens))\n",
    "    print(\"Lexical diversity:\", lexical_diversity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.04 Unique Words Over Time\n",
    "Unique words can be used to identify the frequency of words that appear only once in a given corpus. We can also print a list of these word tokens. Looking at unique words between or across text corpora can allow us to look for the appearances and disappearances of specialized educational terminology over time. To find the frequency (number) of unique words, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` contains data for multiple groups\n",
    "for corpus_key, data in all_results.items():\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Calculate the number of unique words\n",
    "    unique_words = set(word_tokens)\n",
    "    unique_word_count = len(unique_words)\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of unique words:\", unique_word_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.05 Twenty-Five Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Assuming `all_results` contains data for multiple groups\n",
    "for corpus_key, data in all_results.items():\n",
    "    text_objects = data.get('text_objects', None)\n",
    "\n",
    "    # Check if text_objects exist\n",
    "    if text_objects:\n",
    "        # Generate a frequency distribution for the text objects\n",
    "        freq_dist = nltk.FreqDist(text_objects)\n",
    "\n",
    "        # Get the top 25 most common words\n",
    "        most_common_words = freq_dist.most_common(25)\n",
    "\n",
    "        # Print results for each group\n",
    "        print(f\"\\nMost common words in {corpus_key}:\")\n",
    "        print(most_common_words)\n",
    "    else:\n",
    "        print(f\"No text objects available for {corpus_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.06 Display all unique words found in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using all_results dictionary which contains multiple corpora\n",
    "corpora_tokens = {key: set(data['word_tokens']) for key, data in all_results.items() if 'word_tokens' in data}\n",
    "\n",
    "# Function to find words unique to each corpus compared to others\n",
    "def find_unique_words(corpora_tokens):\n",
    "    unique_words = {}\n",
    "    for corpus_name, tokens in corpora_tokens.items():\n",
    "        # Start with the current corpus tokens\n",
    "        all_other_tokens = set()\n",
    "        for other_corpus_name, other_tokens in corpora_tokens.items():\n",
    "            if corpus_name != other_corpus_name:\n",
    "                all_other_tokens.update(other_tokens)\n",
    "        \n",
    "        # Unique words are those not in the union of all other tokens\n",
    "        unique_words[corpus_name] = tokens - all_other_tokens\n",
    "    return unique_words\n",
    "\n",
    "# Find words unique to each corpus\n",
    "unique_words_by_corpus = find_unique_words(corpora_tokens)\n",
    "\n",
    "# Print unique words for each corpus\n",
    "for corpus_name, unique_words in unique_words_by_corpus.items():\n",
    "    print(f\"Words exclusive to {corpus_name}:\", sorted(unique_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.07 Most frequently used words across all corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Assuming all_results contains multiple corpora with their respective word tokens\n",
    "corpora_tokens = {key: data['word_tokens'] for key, data in all_results.items() if 'word_tokens' in data}\n",
    "\n",
    "# Aggregate all tokens from all corpora into a single list\n",
    "all_tokens = []\n",
    "for tokens in corpora_tokens.values():\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Calculate the frequency distribution of all tokens\n",
    "token_freq_dist = Counter(all_tokens)\n",
    "\n",
    "# Find the most common words across all corpora\n",
    "most_common_words = token_freq_dist.most_common(100)  # Adjust the number as needed\n",
    "\n",
    "def print_in_columns(data, columns=3):\n",
    "    # Split the data into chunks of size 'columns'\n",
    "    for i in range(0, len(data), columns):\n",
    "        chunk = data[i:i + columns]\n",
    "        # Format and print each chunk\n",
    "        print(\"  \".join(f\"{word}: {freq}\" for word, freq in chunk))\n",
    "\n",
    "# Print the most common words in columns\n",
    "print(\"Most frequently used words across all corpora:\")\n",
    "print_in_columns(most_common_words, columns=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.08 N-grams and collocations\n",
    "N-grams point out recurring word combinations found throughout the text corpus. For example, \"spring break\" is an example of a bigrams while \"New York City\" is a trigrams. Bigrams and repeated collocations of words convey a lot of information about the contents of the text corpus.\n",
    "To generate an ordered list of the most common bigrams, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming all_results contains multiple corpora with their respective text objects\n",
    "output_directory = 'data/outputFiles/ngramFrequencies'\n",
    "os.makedirs(output_directory, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "for key, data in all_results.items():\n",
    "    if 'text_objects' in data:\n",
    "        text_object = data['text_objects']\n",
    "        \n",
    "        # Find bigrams\n",
    "        bigram_finder = BigramCollocationFinder.from_words(text_object)\n",
    "        bigrams = bigram_finder.ngram_fd.items()\n",
    "        bigrams_sorted = sorted(bigrams, key=lambda item: item[1], reverse=True)\n",
    "        bigram_df = pd.DataFrame(bigrams_sorted, columns=['Bigram', 'Frequency'])\n",
    "        \n",
    "        # Save bigrams to CSV\n",
    "        bigram_filename = os.path.join(output_directory, f\"{key}_bigrams.csv\")\n",
    "        bigram_df.to_csv(bigram_filename, index=False)\n",
    "        print(f\"Top 50 bigrams for {key} saved to {bigram_filename}\")\n",
    "        \n",
    "        # Find trigrams\n",
    "        trigram_finder = TrigramCollocationFinder.from_words(text_object)\n",
    "        trigrams = trigram_finder.ngram_fd.items()\n",
    "        trigrams_sorted = sorted(trigrams, key=lambda item: item[1], reverse=True)\n",
    "        trigram_df = pd.DataFrame(trigrams_sorted, columns=['Trigram', 'Frequency'])\n",
    "        \n",
    "        # Save trigrams to CSV\n",
    "        trigram_filename = os.path.join(output_directory, f\"{key}_trigrams.csv\")\n",
    "        trigram_df.to_csv(trigram_filename, index=False)\n",
    "        print(f\"Top 50 trigrams for {key} saved to {trigram_filename}\")\n",
    "\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "# Assuming all_results contains multiple corpora with their respective text objects\n",
    "for key, data in all_results.items():\n",
    "    if 'text_objects' in data:\n",
    "        text_object = data['text_objects']\n",
    "        \n",
    "        # Display frequency of highest 50 bigrams\n",
    "        print(f\"Top 50 bigrams for {key}:\")\n",
    "        bigram_finder = BigramCollocationFinder.from_words(text_object)\n",
    "        bigram_finder.ngram_fd.tabulate(25)\n",
    "        \n",
    "        # Display frequency of highest 50 trigrams\n",
    "        print(f\"Top 50 trigrams for {key}:\")\n",
    "        trigram_finder = TrigramCollocationFinder.from_words(text_object)\n",
    "        trigram_finder.ngram_fd.tabulate(5)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.09 Concordance\n",
    "Concordance is an NLTK Text object method that also looks for word distribution, but specifically searches for words found before and after a specific word of choice. Concordance allows us to find out how words are used contextually throughout a corpus. This can be particularly powerful when looking at trends over time or between groups. For example, in the sample below we search for the all the contextual occurrences of the word “pi” in our seven separate corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all_results is a dictionary with keys as corpus names and values containing NLTK text objects among other details\n",
    "for key, data in all_results.items():\n",
    "    if 'text_objects' in data:\n",
    "        text_object = data['text_objects']\n",
    "        print(f\"Concordance for 'students' in {key}:\")\n",
    "        text_object.concordance(\"email\", width=150)\n",
    "        print(\"\\n\")  # Adding a newline for better readability between results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.00 Word Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.01 Bag of Words Frequency distribution\n",
    "\n",
    "This will search for each word in the bag of words to find its frequncy in each text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize a dictionary to hold all word tokens for each corpus\n",
    "all_word_tokens = {}\n",
    "\n",
    "# Extract word tokens from each result in all_results and store them in all_word_tokens\n",
    "for file_key, result in all_results.items():\n",
    "    all_word_tokens[file_key] = result['word_tokens']\n",
    "\n",
    "# At this point, all_word_tokens will have file keys as keys and lists of word tokens as values\n",
    "# Assuming `all_word_tokens` is a dictionary where keys are corpus names and values are lists of word tokens\n",
    "# For example:\n",
    "# all_word_tokens = {\n",
    "#     'corpus1': ['word1', 'word2', ...],\n",
    "#     'corpus2': ['word1', 'word2', ...],\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Keywords to track across corpora\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying'])\n",
    "\n",
    "# Initialize a dictionary to hold frequency distributions\n",
    "freq_distributions = {}\n",
    "\n",
    "# Calculate frequency distribution for each corpus\n",
    "for corpus_name, tokens in all_word_tokens.items():\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    freq_distributions[corpus_name] = {word: freq_dist[word] for word in keywords}\n",
    "\n",
    "# Convert the frequency distributions to a DataFrame for easy visualization and analysis\n",
    "freq_df = pd.DataFrame(freq_distributions)\n",
    "\n",
    "print(freq_df[:35])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.02 Bag of Words Frequency Distribution with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example keywords to track\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', 'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "fifteen_minute_marks = {\n",
    "    'mathTalk_file1': [15, 23, 43, 45, 67, 85, 190],\n",
    "    'mathTalk_file2': [205, 443, 520, 723, 986, 1222, 1517],\n",
    "\n",
    "}\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "\n",
    "# Number of corpora\n",
    "num_corpora = len(all_results)\n",
    "\n",
    "# Create subplots for each corpus\n",
    "fig, axes = plt.subplots(num_corpora, 1, figsize=(25, num_corpora*9), sharex=True)\n",
    "\n",
    "# Convert axes to an array if it's not (happens when num_corpora is 1)\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "for ax, (file_key, results) in zip(axes, all_results.items()):\n",
    "    ax.set_title(f\"Corpus: {file_key}\")\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        corpus_names = [occ[0] for occ in occurrences]\n",
    "        sentence_nums = [occ[1] for occ in occurrences]\n",
    "        \n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "        ax.scatter(sentence_nums, y_values, label=keyword, alpha=0.6, edgecolors='none')\n",
    "\n",
    "\n",
    "    ax.set_yticks(list(keyword_mapping.values()))\n",
    "    ax.set_yticklabels(list(keyword_mapping.keys()))\n",
    "\n",
    "# Adjust layout\n",
    "plt.xlabel('Sentence Number')\n",
    "plt.ylabel('Keywords')\n",
    "\n",
    "# Add common legend and labels\n",
    "# fig.legend(keywords, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=len(keywords))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to an image file before displaying\n",
    "plt.savefig('my_plots.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.03 Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "\n",
    "# Example keywords to track\n",
    "keywords = ['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', 'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying']\n",
    "\n",
    "fifteen_minute_marks = {\n",
    "    'mathTalk_file1': [15, 23, 43, 45, 67, 85, 190],\n",
    "    'mathTalk_file2': [205, 443, 520, 723, 986, 1222, 1517],\n",
    "    'mathTalk_file3': [174, 430, 521, 731, 986, 1198, 1557],\n",
    "    'mathTalk_file4': [52, 273, 300, 352, 406, 486, 534],\n",
    "    'mathTalk_file5': [66, 169, 250, 355, 482, 649, 760],\n",
    "    'mathTalk_file6': [316, 654, 800, 1159, 1575, 1884, 2200],\n",
    "    'mathTalk_file7': [114, 312, 381, 723, 1027, 1255, 1519],\n",
    "}\n",
    "\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Create a figure with an increased height to better fit the number of keywords\n",
    "fig = go.Figure()\n",
    "\n",
    "# Loop through each corpus\n",
    "for file_key, results in all_results.items():\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences if occ[0] == file_key]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Adding traces for each keyword\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers', name=keyword,\n",
    "                                 text=['Sentence: {}'.format(num) for num in sentence_nums],\n",
    "                                 marker=dict(size=8, opacity=0.6)))\n",
    "\n",
    "\n",
    "# Update layout with an appropriate height\n",
    "fig.update_layout(title='Keyword Occurrence Across Sentences in Multiple Corpora',\n",
    "                  xaxis_title='Sentence Number',\n",
    "                  yaxis=dict(tickmode='array', tickvals=list(keyword_mapping.values()), ticktext=list(keyword_mapping.keys())),\n",
    "                  legend_title='Keywords',\n",
    "                  height=1200)  # Set a larger height depending on the number of keywords\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.04 Side to side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "num_corpora = len(all_results)  # Count of corpora to be displayed\n",
    "fig = make_subplots(rows=1, cols=num_corpora, subplot_titles=[f\"Corpus {i+1}\" for i in range(num_corpora)])\n",
    "\n",
    "# Example keywords to track, sorted to maintain consistency across the plot\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', \n",
    "            'think', 'want', 'kind', 'time', 'grade', 'thinking', 'different', 'talk', 'conversation', \n",
    "            'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Index for the current subplot\n",
    "col_index = 1\n",
    "\n",
    "# Loop through each corpus\n",
    "for file_key, results in all_results.items():\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences if occ[0] == file_key]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Adding traces for each keyword to the respective subplot\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers', name=keyword,\n",
    "                                 text=['Sentence: {}'.format(num) for num in sentence_nums],\n",
    "                                 marker=dict(size=10, opacity=0.5)),  # Adjusted marker size for visibility\n",
    "                      row=1, col=col_index)\n",
    "\n",
    "    col_index += 1  # Move to the next subplot for the next corpus\n",
    "\n",
    "# Update layout to make sure all keywords are visible\n",
    "fig.update_layout(\n",
    "    title='Keyword Occurrence Across Sentences in Multiple Corpora',\n",
    "    xaxis_title='Sentence Number',\n",
    "    yaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=list(keyword_mapping.values()),\n",
    "        ticktext=list(keyword_mapping.keys())\n",
    "    ),\n",
    "    legend_title='Keywords',\n",
    "    height=1200,  # Increased height to accommodate all keywords\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.05 Needs fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "num_corpora = len(all_results)  # Count of corpora to be displayed\n",
    "fig = make_subplots(rows=1, cols=num_corpora, subplot_titles=[f\"Corpus {i+1}\" for i in range(num_corpora)])\n",
    "\n",
    "# Example keywords to track, sorted to maintain consistency across the plot\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', \n",
    "            'think', 'want', 'kind', 'time', 'grade', 'thinking', 'different', 'talk', 'conversation', \n",
    "            'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Loop through each corpus\n",
    "for file_key, results in all_results.items():\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences if occ[0] == file_key]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Adding traces for each keyword\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers',\n",
    "                                 text=[f'{keyword} (Sentence: {num})' for num in sentence_nums],\n",
    "                                 marker=dict(size=10, opacity=0.5),\n",
    "                                 showlegend=False),  # This prevents adding to the legend\n",
    "                      row=1, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Keyword Occurrence Across Sentences in Multiple Corpora',\n",
    "    xaxis_title='Sentence Number',\n",
    "    yaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=list(keyword_mapping.values()),\n",
    "        ticktext=list(keyword_mapping.keys())\n",
    "    ),\n",
    "    height=1200,  # Increased height to accommodate all keywords\n",
    ")\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.00 Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.01 Importing data from csv file in Google Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# This is the full shared Drive link, the file ID starts at \"1i\" and ends at \"8S\"\n",
    "# https://docs.google.com/spreadsheets/d/1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S/edit?usp=drive_link&ouid=106477043869312333876&rtpof=true&sd=true\n",
    "# https://drive.google.com/file/d/1hLRRRvawjxrdI141_bT5QXELb0jk9Jhg/view?usp=sharing\n",
    "\n",
    "# the file ID from the shareable link is pasted below in orange.\n",
    "file_id = \"1hLRRRvawjxrdI141_bT5QXELb0jk9Jhg\"\n",
    "\n",
    "# construct the download URL, you would not need to change anything here.\n",
    "download_url = f\"https://docs.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "# send a GET request to the download URL and save the response content\n",
    "response = requests.get(download_url)\n",
    "\n",
    "# The next line names the file after download. If you change it here, you will also need to change in the subsequent fields.\n",
    "# If you click on the folder icon in Colab you should see a file now appear called \"uncertaintyText.xlsx\"\n",
    "# These names can be changed to suit you own data\n",
    "with open(\"uncertaintyText.xlsx\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "\n",
    "# Specify the path to the Excel file this where it was placed in 2.4 so that is the file and path you want to open\n",
    "excel_file_path = '/content/uncertaintyText.xlsx'\n",
    "\n",
    "# Specify the column name you want to pull the data corpus from\n",
    "column_name = 'transcript'\n",
    "\n",
    "# Read the Excel file and extract the specified column\n",
    "data = pd.read_excel(excel_file_path, engine='openpyxl')\n",
    "text_column = data[column_name]\n",
    "\n",
    "\n",
    "# Convert each item in the column to a string and then join them together to be saved as a text file containing all data in the transcript column.\n",
    "raw_uncertaintyText = ' '.join(map(str, text_column))\n",
    "\n",
    "\n",
    "# Save the string to a text file in your Google Drive\n",
    "with open('/content/raw_uncertaintyText.txt', 'w') as file:\n",
    "  file.write(raw_uncertaintyText)\n",
    "\n",
    "print(\"Text saved to raw_uncertaintyText.txt\")\n",
    "print(\"Raw text file is a: \",type(raw_uncertaintyText), \"It contains: \",len(raw_uncertaintyText), \"characters\")\n",
    "print(\"Here are the first 251 characters in the raw text file: \", raw_uncertaintyText[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Topic Modeling and Semantic Similarities Across Transcriptions\n",
    "Generating Sentence Embeddings for Each Transcription\n",
    "We'll generate embeddings for each transcription and store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Store embeddings and related info\n",
    "embeddings_list = []\n",
    "file_names = []\n",
    "\n",
    "for df in transcript_dfs:\n",
    "    text = ' '.join(df['cleaned_transcript'])\n",
    "    embedding = model.encode(text)\n",
    "    embeddings_list.append(embedding)\n",
    "    file_names.append(df['source_file'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Transcriptions Using Semantic Similarity\n",
    "We'll compute the cosine similarity between each pair of transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert list of embeddings to a numpy array\n",
    "embeddings_array = np.vstack(embeddings_list)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=file_names, columns=file_names)\n",
    "\n",
    "print(similarity_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on Individual Transcriptions\n",
    "Using Latent Dirichlet Allocation (LDA)\n",
    "First, you need to prepare the data for LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "def perform_lda(tokens_list, num_topics=5):\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "\n",
    "    # Filter out extremes to limit the number of features\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "\n",
    "    # Create a bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "    return lda, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
