{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Kevin's NLTK NLP Text Analytics Pipeline V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.00 Installing Libraries and Dependencies\n",
    "\n",
    "\n",
    "libraries needed:\n",
    "gensim\n",
    "nltk\n",
    "spacy\n",
    "pandas\n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade nltk\n",
    "%pip install --upgrade pandas\n",
    "%pip install --upgrade numpy\n",
    "%pip install --upgrade plotly\n",
    "%pip install --upgrade matplotlib\n",
    "%pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.01 NLTK Preprocessing and tokenization for Peeking under the Hood Text Analytics\n",
    "\n",
    "Running the code cell below will result in us pulling the following token types for each of our seven corpora.\n",
    "1. word tokens - formed from decomposing sentences into their constituent pieces.\n",
    "2. NLTK text objects - The NLTK library has a unique tokenizer that adds additional metadata to the word token which allows for unique analysis as compared to normal word tokens.\n",
    "3. original sentence tokens - based on numerous features, but normally bounded by punctuation marks. This is why we normally tokenize this first before other text processing.\n",
    "4. normalized sentence tokens - original sentence tokens that have been lowercased, and had stop words, punctuations, and special chracters removed.\n",
    "\n",
    "You can always figure out what type of data corpus you are dealing with by running these print checks. It is also extremely important to also note the importance of keeping your documents categorized, lest they get out of control. The more processing and feature extractions you do, the more you may end up with more different buckets of data to keep up with.\n",
    "* We will look at the utility of each token type in these modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV to corpus\n",
    "folder_path = 'data/outputFiles/csvOutputs/diarizedTranscripts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. needs 3.10 or 3.11 for spacy \n",
    "2. conda install ipykernel pandas numpy nltk spacy sentence-transformers scikit-learn\n",
    "3. python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV files from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def read_csv_files(directory):\n",
    "    csv_files = glob.glob(os.path.join(directory, '**', '*.csv'), recursive=True)\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = file  # Keep track of the source file\n",
    "        data_frames.append(df)\n",
    "    return data_frames\n",
    "\n",
    "# Example usage:\n",
    "directory = 'data/rawTranscriptFiles'\n",
    "transcript_dfs = read_csv_files(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing each transcription seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mrhal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mrhal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def spacy_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pos_tags, entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_number   start     end  \\\n",
      "0                1   1.733   2.453   \n",
      "1                2   6.116  10.678   \n",
      "2                3  12.339  17.542   \n",
      "3                4  18.823  20.023   \n",
      "4                5  22.885  24.066   \n",
      "\n",
      "                                                text  \\\n",
      "0                                           the job.   \n",
      "1  Glad to see things are going well and business...   \n",
      "2  Andrea told me about your outstanding numbers ...   \n",
      "3                             Keep up the good work.   \n",
      "4                             Now to other business.   \n",
      "\n",
      "                                               words     speaker  \\\n",
      "0  [{'word': 'the', 'start': 1.733, 'end': 1.853,...  SPEAKER_00   \n",
      "1  [{'word': 'Glad', 'start': 6.116, 'end': 6.496...  SPEAKER_00   \n",
      "2  [{'word': 'Andrea', 'start': 12.339, 'end': 12...  SPEAKER_00   \n",
      "3  [{'word': 'Keep', 'start': 18.823, 'end': 19.0...  SPEAKER_00   \n",
      "4  [{'word': 'Now', 'start': 22.885, 'end': 23.10...  SPEAKER_00   \n",
      "\n",
      "                                         source_file  \n",
      "0  data/rawTranscriptFiles\\Monologue_transcriptio...  \n",
      "1  data/rawTranscriptFiles\\Monologue_transcriptio...  \n",
      "2  data/rawTranscriptFiles\\Monologue_transcriptio...  \n",
      "3  data/rawTranscriptFiles\\Monologue_transcriptio...  \n",
      "4  data/rawTranscriptFiles\\Monologue_transcriptio...  \n"
     ]
    }
   ],
   "source": [
    "print(transcript_dfs[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in transcript_dfs:\n",
    "    df['cleaned_transcript'] = df['text'].apply(preprocess_text)\n",
    "    df['tokens'] = df['cleaned_transcript'].apply(tokenize_text)\n",
    "    df[['pos_tags', 'entities']] = df['cleaned_transcript'].apply(\n",
    "        lambda x: pd.Series(spacy_analysis(x))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cursory Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Latent Dirichlet Allocation (LDA)\n",
    "# First, you need to prepare the data for LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, apply LDA to each transcription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models = []\n",
    "\n",
    "for df in transcript_dfs:\n",
    "    tokens_list = df['tokens'].tolist()\n",
    "    lda_model, corpus, dictionary = perform_lda(tokens_list)\n",
    "    lda_models.append({\n",
    "        'model': lda_model,\n",
    "        'corpus': corpus,\n",
    "        'dictionary': dictionary,\n",
    "        'source_file': df['source_file'].iloc[0]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Folder containing CSV files\n",
    "folder_path = 'data/rawTranscriptFiles'\n",
    "\n",
    "# Column name to extract text from\n",
    "column_name = 'text'\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# List to hold the variable names and their lengths\n",
    "variable_names_and_lengths = []\n",
    "\n",
    "# Counter to create corpus variable names like corpus1, corpus2, etc.\n",
    "counter = 1\n",
    "\n",
    "# Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create a variable name based on the counter\n",
    "        base_var_name = f'corpus{counter}'\n",
    "        \n",
    "        # Step 1: Convert CSV file into a dataframe variable\n",
    "        globals()[base_var_name] = df\n",
    "\n",
    "        # Step 2: Extract and store raw text\n",
    "        raw_text_var_name = f'{base_var_name}_rawText'\n",
    "        raw_text = ' '.join(df[column_name].astype(str).tolist())\n",
    "        globals()[raw_text_var_name] = raw_text\n",
    "        \n",
    "        # Step 3: Tokenize raw text\n",
    "        raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "        raw_word_tokens = nltk.word_tokenize(raw_text)\n",
    "        raw_text_obj = nltk.Text(raw_word_tokens)\n",
    "        \n",
    "        df['rawSentenceTokens'] = df[column_name].apply(nltk.sent_tokenize)\n",
    "        df['rawWordTokens'] = df[column_name].apply(nltk.word_tokenize)\n",
    "        df['rawTextObjects'] = df['rawWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_rawSentenceTokens'] = raw_sentence_tokens\n",
    "        globals()[f'{base_var_name}_rawWordTokens'] = raw_word_tokens\n",
    "        globals()[f'{base_var_name}_rawTextObjects'] = raw_text_obj\n",
    "        \n",
    "        # Step 4: Clean text and create processed text\n",
    "        df['processedText'] = df[column_name].apply(clean_text)\n",
    "        \n",
    "        processed_text_var_name = f'{base_var_name}_processedText'\n",
    "        processed_text = ' '.join(df['processedText'].astype(str).tolist())\n",
    "        globals()[processed_text_var_name] = processed_text\n",
    "        \n",
    "        # Step 5: Tokenize processed text\n",
    "        processed_sentence_tokens = [nltk.sent_tokenize(clean_text(sent)) for sent in raw_sentence_tokens]\n",
    "        processed_sentence_tokens_flat = [sent for sublist in processed_sentence_tokens for sent in sublist]\n",
    "        processed_word_tokens = [nltk.word_tokenize(sent) for sent in processed_sentence_tokens_flat]\n",
    "        processed_word_tokens_flat = [word for sublist in processed_word_tokens for word in sublist]\n",
    "        processed_text_obj = nltk.Text(processed_word_tokens_flat)\n",
    "        \n",
    "        df['processedSentenceTokens'] = df['rawSentenceTokens'].apply(lambda x: [nltk.sent_tokenize(clean_text(sent)) for sent in x])\n",
    "        df['processedSentenceTokens'] = df['processedSentenceTokens'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "        df['processedWordTokens'] = df['processedSentenceTokens'].apply(lambda x: [nltk.word_tokenize(sent) for sent in x])\n",
    "        df['processedWordTokens'] = df['processedWordTokens'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "        df['processedTextObjects'] = df['processedWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_processedSentenceTokens'] = processed_sentence_tokens_flat\n",
    "        globals()[f'{base_var_name}_processedWordTokens'] = processed_word_tokens_flat\n",
    "        globals()[f'{base_var_name}_processedTextObjects'] = processed_text_obj\n",
    "        \n",
    "        # Step 6: Remove stop words and create fully processed text\n",
    "        df['fullyProcessedText'] = df['processedText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "        \n",
    "        fully_processed_text_var_name = f'{base_var_name}_fullyProcessedText'\n",
    "        fully_processed_text = ' '.join(df['fullyProcessedText'].astype(str).tolist())\n",
    "        globals()[fully_processed_text_var_name] = fully_processed_text\n",
    "        \n",
    "        # Step 7: Tokenize fully processed text\n",
    "        fully_processed_sentence_tokens = [nltk.sent_tokenize(sent) for sent in processed_sentence_tokens_flat]\n",
    "        fully_processed_sentence_tokens_flat = [sent for sublist in fully_processed_sentence_tokens for sent in sublist]\n",
    "        fully_processed_word_tokens = [nltk.word_tokenize(sent) for sent in fully_processed_sentence_tokens_flat]\n",
    "        fully_processed_word_tokens_flat = [word for sublist in fully_processed_word_tokens for word in sublist]\n",
    "        fully_processed_text_obj = nltk.Text(fully_processed_word_tokens_flat)\n",
    "        \n",
    "        df['fullyProcessedSentenceTokens'] = df['fullyProcessedText'].apply(nltk.sent_tokenize)\n",
    "        df['fullyProcessedWordTokens'] = df['fullyProcessedSentenceTokens'].apply(lambda x: [nltk.word_tokenize(sent) for sent in x])\n",
    "        df['fullyProcessedWordTokens'] = df['fullyProcessedWordTokens'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "        df['fullyProcessedTextObjects'] = df['fullyProcessedWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_fullyProcessedSentenceTokens'] = fully_processed_sentence_tokens_flat\n",
    "        globals()[f'{base_var_name}_fullyProcessedWordTokens'] = fully_processed_word_tokens_flat\n",
    "        globals()[f'{base_var_name}_fullyProcessedTextObjects'] = fully_processed_text_obj\n",
    "        \n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        new_file_path = os.path.join(folder_path, f'processed_{filename}')\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "        \n",
    "        # Store the lengths of each created variable\n",
    "        variable_names_and_lengths.append((raw_text_var_name, len(raw_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_rawSentenceTokens', len(raw_sentence_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_rawWordTokens', len(raw_word_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedText', len(processed_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedSentenceTokens', len(processed_sentence_tokens_flat)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedWordTokens', len(processed_word_tokens_flat)))\n",
    "        variable_names_and_lengths.append((fully_processed_text_var_name, len(fully_processed_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_fullyProcessedSentenceTokens', len(fully_processed_sentence_tokens_flat)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_fullyProcessedWordTokens', len(fully_processed_word_tokens_flat)))\n",
    "        \n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "\n",
    "# Print the lengths of all created variables\n",
    "for var_name, var_length in variable_names_and_lengths:\n",
    "    print(f'{var_name}: {var_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Folder containing CSV files\n",
    "folder_path = 'data/outputFiles/csvOutputs/diarizedTranscripts'\n",
    "\n",
    "# Column name to extract text from\n",
    "column_name = 'text'\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# List to hold the variable names and their lengths\n",
    "variable_names_and_lengths = []\n",
    "\n",
    "# Counter to create corpus variable names like corpus1, corpus2, etc.\n",
    "counter = 1\n",
    "\n",
    "# Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create a variable name based on the counter\n",
    "        base_var_name = f'corpus{counter}'\n",
    "        \n",
    "        # Step 1: Convert CSV file into a dataframe variable\n",
    "        globals()[base_var_name] = df\n",
    "\n",
    "        # Step 2: Extract and store raw text\n",
    "        raw_text_var_name = f'{base_var_name}_rawText'\n",
    "        raw_text = ' '.join(df[column_name].astype(str).tolist())\n",
    "        globals()[raw_text_var_name] = raw_text\n",
    "        \n",
    "        # Step 3: Tokenize raw text\n",
    "        raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "        raw_word_tokens = nltk.word_tokenize(raw_text)\n",
    "        raw_text_obj = nltk.Text(raw_word_tokens)\n",
    "        \n",
    "        df['rawSentenceTokens'] = df[column_name].apply(nltk.sent_tokenize)\n",
    "        df['rawWordTokens'] = df[column_name].apply(nltk.word_tokenize)\n",
    "        df['rawTextObjects'] = df['rawWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_rawSentenceTokens'] = raw_sentence_tokens\n",
    "        globals()[f'{base_var_name}_rawWordTokens'] = raw_word_tokens\n",
    "        globals()[f'{base_var_name}_rawTextObjects'] = raw_text_obj\n",
    "        \n",
    "        # Step 4: Clean text and create processed text\n",
    "        df['processedText'] = df[column_name].apply(clean_text)\n",
    "        \n",
    "        processed_text_var_name = f'{base_var_name}_processedText'\n",
    "        processed_text = ' '.join(df['processedText'].astype(str).tolist())\n",
    "        globals()[processed_text_var_name] = processed_text\n",
    "        \n",
    "        # Step 5: Tokenize processed text\n",
    "        processed_sentence_tokens = nltk.sent_tokenize(processed_text)\n",
    "        processed_word_tokens = nltk.word_tokenize(processed_text)\n",
    "        processed_text_obj = nltk.Text(processed_word_tokens)\n",
    "        \n",
    "        df['processedSentenceTokens'] = df['processedText'].apply(nltk.sent_tokenize)\n",
    "        df['processedWordTokens'] = df['processedText'].apply(nltk.word_tokenize)\n",
    "        df['processedTextObjects'] = df['processedWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_processedSentenceTokens'] = processed_sentence_tokens\n",
    "        globals()[f'{base_var_name}_processedWordTokens'] = processed_word_tokens\n",
    "        globals()[f'{base_var_name}_processedTextObjects'] = processed_text_obj\n",
    "        \n",
    "        # Step 6: Remove stop words and create fully processed text\n",
    "        df['fullyProcessedText'] = df['processedText'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "        \n",
    "        fully_processed_text_var_name = f'{base_var_name}_fullyProcessedText'\n",
    "        fully_processed_text = ' '.join(df['fullyProcessedText'].astype(str).tolist())\n",
    "        globals()[fully_processed_text_var_name] = fully_processed_text\n",
    "        \n",
    "        # Step 7: Tokenize fully processed text\n",
    "        fully_processed_sentence_tokens = nltk.sent_tokenize(fully_processed_text)\n",
    "        fully_processed_word_tokens = nltk.word_tokenize(fully_processed_text)\n",
    "        fully_processed_text_obj = nltk.Text(fully_processed_word_tokens)\n",
    "        \n",
    "        df['fullyProcessedSentenceTokens'] = df['fullyProcessedText'].apply(nltk.sent_tokenize)\n",
    "        df['fullyProcessedWordTokens'] = df['fullyProcessedText'].apply(nltk.word_tokenize)\n",
    "        df['fullyProcessedTextObjects'] = df['fullyProcessedWordTokens'].apply(nltk.Text)\n",
    "        \n",
    "        globals()[f'{base_var_name}_fullyProcessedSentenceTokens'] = fully_processed_sentence_tokens\n",
    "        globals()[f'{base_var_name}_fullyProcessedWordTokens'] = fully_processed_word_tokens\n",
    "        globals()[f'{base_var_name}_fullyProcessedTextObjects'] = fully_processed_text_obj\n",
    "        \n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        new_file_path = os.path.join(folder_path, f'processed_{filename}')\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "        \n",
    "        # Store the lengths of each created variable\n",
    "        variable_names_and_lengths.append((raw_text_var_name, len(raw_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_rawSentenceTokens', len(raw_sentence_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_rawWordTokens', len(raw_word_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedText', len(processed_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedSentenceTokens', len(processed_sentence_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_processedWordTokens', len(processed_word_tokens)))\n",
    "        variable_names_and_lengths.append((fully_processed_text_var_name, len(fully_processed_text)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_fullyProcessedSentenceTokens', len(fully_processed_sentence_tokens)))\n",
    "        variable_names_and_lengths.append((f'{base_var_name}_fullyProcessedWordTokens', len(fully_processed_word_tokens)))\n",
    "        \n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "\n",
    "# Print the lengths of all created variables\n",
    "for var_name, var_length in variable_names_and_lengths:\n",
    "    print(f'{var_name}: {var_length}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Folder containing CSV files\n",
    "folder_path = 'data/outputFiles/csvOutputs/diarizedTranscripts'\n",
    "\n",
    "# Column name to extract text from\n",
    "column_name = 'text'\n",
    "\n",
    "# List to hold the variable names\n",
    "variable_names = []\n",
    "\n",
    "# Counter to create corpus variable names like corpus1, corpus2, etc.\n",
    "counter = 1\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the column data to a single string\n",
    "        text = ' '.join(df[column_name].astype(str).tolist())\n",
    "        \n",
    "        # Create a variable name based on the counter\n",
    "        variable_name = f'corpus{counter}'\n",
    "        \n",
    "        # Store the text in the global namespace\n",
    "        globals()[variable_name] = text\n",
    "        \n",
    "        # Append the variable name to the list\n",
    "        variable_names.append(variable_name)\n",
    "        \n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "\n",
    "# Print the names of all variables\n",
    "for variable_name in variable_names:\n",
    "    print(variable_name)\n",
    "\n",
    "# Example of how to access the variables\n",
    "# print(corpus1)\n",
    "# print(corpus2)\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CSV folder instead of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "# Set the path to your text files directory\n",
    "input_directory = 'data/outputFiles/csvOutputs/diarizedTranscripts'\n",
    "output_directory = 'data/outputFiles/processedFiles/newpipline'\n",
    "\n",
    "# Automatically list all CSV files in the input directory\n",
    "files_to_process = [(os.path.join(input_directory, f), os.path.join(output_directory, f'{os.path.splitext(f)[0]}_processed.csv'))\n",
    "                    for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Function to process a file\n",
    "def process_file(input_path, output_path):\n",
    "    results = {'sentence_tokens': [], 'cleaned_sentences': [], 'word_tokens': [], 'text_objects': None, 'removed_elements': {'punctuation': [], 'non_alpha': [], 'stop_words': []}}\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "        df['processed_text'] = None  # Initialize a new column for processed text\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            raw_text = row['text']\n",
    "            sent_tokens = sent_tokenize(raw_text)\n",
    "            cleaned_sent_tokens = []\n",
    "\n",
    "            for sentence in sent_tokens:\n",
    "                words = word_tokenize(sentence.lower())\n",
    "                cleaned_words = []\n",
    "                for word in words:\n",
    "                    if word.isalpha():\n",
    "                        if word not in stop_words_set:\n",
    "                            cleaned_words.append(word)\n",
    "                        else:\n",
    "                            results['removed_elements']['stop_words'].append(word)\n",
    "                    else:\n",
    "                        results['removed_elements']['non_alpha'].append(word)\n",
    "                        if any(char in string.punctuation for char in word):\n",
    "                            results['removed_elements']['punctuation'].append(word)\n",
    "\n",
    "                cleaned_sentence = ' '.join(cleaned_words)\n",
    "                cleaned_sent_tokens.append(cleaned_sentence)\n",
    "\n",
    "            cleaned_text = ' '.join(cleaned_sent_tokens)\n",
    "            df.at[index, 'processed_text'] = cleaned_text\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "        word_tokens = word_tokenize(cleaned_text)\n",
    "        text_objects = nltk.Text(word_tokens)\n",
    "\n",
    "        results.update({\n",
    "            'sentence_tokens': sent_tokens,\n",
    "            'cleaned_sentences': cleaned_sent_tokens,\n",
    "            'word_tokens': word_tokens,\n",
    "            'text_objects': text_objects\n",
    "        })\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_path}' was not found. Check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return results\n",
    "\n",
    "# Dictionary to hold all results\n",
    "all_results = {}\n",
    "corpus_number = 1  # Initialize a counter for the corpus number\n",
    "\n",
    "# Process each file\n",
    "for input_path, output_path in files_to_process:\n",
    "    results = process_file(input_path, output_path)\n",
    "    corpus_key = f'corpus{corpus_number}'  # Generate a corpus key like \"corpus1\", \"corpus2\", etc.\n",
    "    all_results[corpus_key] = results\n",
    "    corpus_number += 1  # Increment the corpus number for the next iteration\n",
    "\n",
    "# Example of accessing the results\n",
    "for key, value in all_results.items():\n",
    "    if 'word_tokens' in value and value['word_tokens']:\n",
    "        print(f\"Results for {key}:\")\n",
    "        print(value['word_tokens'][:5])  # Display first 5 word tokens\n",
    "    else:\n",
    "        print(f\"No word tokens available for {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "# Set the path to your text files directory\n",
    "input_directory = 'data/rawTextCorpora/summerPD2023/Transcriptions/NSF_Transcripts/mergedtranscript'\n",
    "output_directory = 'data/outputFiles/processedFiles/newpipline'\n",
    "\n",
    "# Automatically list all text files in the input directory\n",
    "files_to_process = [(os.path.join(input_directory, f), os.path.join(output_directory, f'{os.path.splitext(f)[0]}_processed.txt'))\n",
    "                    for f in os.listdir(input_directory) if f.endswith('.txt')]\n",
    "\n",
    "# Function to process a file\n",
    "def process_file(input_path, output_path):\n",
    "    results = {'sentence_tokens': [], 'cleaned_sentences': [], 'word_tokens': [], 'text_objects': None, 'removed_elements': {'punctuation': [], 'non_alpha': [], 'stop_words': []}}\n",
    "    try:\n",
    "        with open(input_path, 'rt', encoding='utf-8', errors='replace') as file:\n",
    "            raw_text = file.read().replace(\"\\n\", \" \").replace('yeah', '').replace('like', '').replace('Yeah', '')\n",
    "\n",
    "        sent_tokens = sent_tokenize(raw_text)\n",
    "        cleaned_sent_tokens = []\n",
    "\n",
    "        for sentence in sent_tokens:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            cleaned_words = []\n",
    "            for word in words:\n",
    "                if word.isalpha():\n",
    "                    if word not in stop_words_set:\n",
    "                        cleaned_words.append(word)\n",
    "                    else:\n",
    "                        results['removed_elements']['stop_words'].append(word)\n",
    "                else:\n",
    "                    results['removed_elements']['non_alpha'].append(word)\n",
    "                    if any(char in string.punctuation for char in word):\n",
    "                        results['removed_elements']['punctuation'].append(word)\n",
    "\n",
    "            cleaned_sentence = ' '.join(cleaned_words)\n",
    "            cleaned_sent_tokens.append(cleaned_sentence)\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            for sentence in cleaned_sent_tokens:\n",
    "                file.write(sentence + '\\n')\n",
    "\n",
    "        cleaned_text = ' '.join(cleaned_sent_tokens)\n",
    "        word_tokens = word_tokenize(cleaned_text)\n",
    "        text_objects = nltk.Text(word_tokens)\n",
    "\n",
    "        results.update({\n",
    "            'sentence_tokens': sent_tokens,\n",
    "            'cleaned_sentences': cleaned_sent_tokens,\n",
    "            'word_tokens': word_tokens,\n",
    "            'text_objects': text_objects\n",
    "        })\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_path}' was not found. Check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return results\n",
    "\n",
    "# Dictionary to hold all results\n",
    "all_results = {}\n",
    "corpus_number = 1  # Initialize a counter for the corpus number\n",
    "\n",
    "# Process each file\n",
    "for input_path, output_path in files_to_process:\n",
    "    results = process_file(input_path, output_path)\n",
    "    corpus_key = f'corpus{corpus_number}'  # Generate a corpus key like \"corpus1\", \"corpus2\", etc.\n",
    "    all_results[corpus_key] = results\n",
    "    corpus_number += 1  # Increment the corpus number for the next iteration\n",
    "\n",
    "# Example of accessing the results\n",
    "for key, value in all_results.items():\n",
    "    if 'word_tokens' in value and value['word_tokens']:\n",
    "        print(f\"Results for {key}:\")\n",
    "        print(value['word_tokens'][:5])  # Display first 5 word tokens\n",
    "    else:\n",
    "        print(f\"No word tokens available for {key}\")\n",
    "\n",
    "\n",
    "# Assuming 'all_results' is a dictionary structured like {'corpus1': {...}, 'corpus2': {...}, ...}\n",
    "corpora_keys = list(all_results.keys())  # Get all the keys which are corpus identifiers\n",
    "\n",
    "# For each corpus key, create variables for different elements\n",
    "for corpus_key in corpora_keys:\n",
    "    index = corpora_keys.index(corpus_key) + 1  # To match 'corpus1' with index 1, 'corpus2' with index 2, etc.\n",
    "\n",
    "    # Dynamically create variable names and assign data\n",
    "    globals()[f'word_tokens_corpus{index}'] = all_results[corpus_key]['word_tokens']\n",
    "    globals()[f'text_objects_corpus{index}'] = all_results[corpus_key]['text_objects']\n",
    "    globals()[f'sentence_tokens_corpus{index}'] = all_results[corpus_key]['sentence_tokens']\n",
    "    globals()[f'normalized_sentences_corpus{index}'] = all_results[corpus_key]['cleaned_sentences']\n",
    "    globals()[f'removed_elements_corpus{index}'] = all_results[corpus_key]['removed_elements']\n",
    "\n",
    "# Example of how to access these variables dynamically\n",
    "for i in range(1, len(corpora_keys) + 1):\n",
    "    print(f\"word_tokens_corpus{i} is a:\", type(globals()[f'word_tokens_corpus{i}']), \"containing\", len(globals()[f'word_tokens_corpus{i}']), \"tokens\")\n",
    "    print(f\"sentence_tokens_corpus{i} is a:\", type(globals()[f'sentence_tokens_corpus{i}']), \"containing\", len(globals()[f'sentence_tokens_corpus{i}']), \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.02 Splitting by Sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "transcript_text = \"Section 1: Introduction to AI. AI is a broad field. Section 2: Applications of AI. AI is used in many industries.\"\n",
    "\n",
    "# Split the transcript by \"Section\" followed by any digit and a colon\n",
    "docs = re.split(r\"Section \\d+: \", transcript_text)\n",
    "# Remove any empty strings that might have occurred during splitting\n",
    "docs = [doc.strip() for doc in docs if doc.strip()]\n",
    "\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.00 Peeking Under The Hood Text Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.01 Average Sentence Length\n",
    "Average Sentence Length uses the total number of words and total number of sentences\n",
    "in a corpus to calculate exactly what it says: the average sentence length.\n",
    "While the equation is very basic and straightforward it provides information that can\n",
    "be used to infer, for example, how complex sentences are on average throughout a\n",
    "given text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` is populated with multiple corpus entries\n",
    "for corpus_key, data in all_results.items():\n",
    "    # Retrieve sentence and word tokens from the dictionary\n",
    "    sentence_tokens = data.get('sentence_tokens', [])\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Ensure there are sentences to avoid division by zero\n",
    "    if sentence_tokens:\n",
    "        average_sentence_length = len(word_tokens) / len(sentence_tokens)\n",
    "    else:\n",
    "        average_sentence_length = 0  # Default to zero if no sentences\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of sentences:\", len(sentence_tokens))\n",
    "    print(\"Number of word tokens:\", len(word_tokens))\n",
    "    print(\"Average sentence length:\", average_sentence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.02 Average Word Length Distribution\n",
    "Another fairly straightforward measure that can provide insight into how long, on average, words are in a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` is populated with multiple corpus entries\n",
    "for corpus_key, data in all_results.items():\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Calculate the average word length if there are words in the corpus\n",
    "    if word_tokens:\n",
    "        avg_word_length = sum(len(word) for word in word_tokens) / len(word_tokens)\n",
    "    else:\n",
    "        avg_word_length = 0  # Default to zero if no words to avoid division by zero\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of word tokens:\", len(word_tokens))\n",
    "    print(\"Average word length:\", avg_word_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.03 Lexical Diversity\n",
    "Lexical diversity quantifies the variety of unique words found in a document. It produces a numerical measure that indicates how diverse the vocabulary is that is used in a text. Broadly speaking, scores of (0.8 - 1) are considered extremely high and difficult to maintain in typical communicative texts. Scores of 0.4-0.79 are considered moderate to high; most high-quality texts fall in this range. Scores of (0 - 0.39) are considered low lexical diversity and tend to suggest highly specialized or technical language usage (e.g., instruction manuals) or language aimed at young readers. This measure is sensitive to corpus length (longer corpora have more opportunities to repeat words), but comparing lexical diversity scores can allow for quantitative comparison that might suggest potential changes in how the usage of language may differ between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` is populated with multiple corpus entries\n",
    "for corpus_key, data in all_results.items():\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Calculate the lexical diversity if there are words in the corpus\n",
    "    if word_tokens:\n",
    "        lexical_diversity = len(set(word_tokens)) / len(word_tokens)\n",
    "    else:\n",
    "        lexical_diversity = 0  # Default to zero if no words to avoid division by zero\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of word tokens:\", len(word_tokens))\n",
    "    print(\"Lexical diversity:\", lexical_diversity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.04 Unique Words Over Time\n",
    "Unique words can be used to identify the frequency of words that appear only once in a given corpus. We can also print a list of these word tokens. Looking at unique words between or across text corpora can allow us to look for the appearances and disappearances of specialized educational terminology over time. To find the frequency (number) of unique words, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_results` contains data for multiple groups\n",
    "for corpus_key, data in all_results.items():\n",
    "    word_tokens = data.get('word_tokens', [])\n",
    "\n",
    "    # Calculate the number of unique words\n",
    "    unique_words = set(word_tokens)\n",
    "    unique_word_count = len(unique_words)\n",
    "\n",
    "    # Print results for each group\n",
    "    print(f\"\\n{corpus_key}:\")\n",
    "    print(\"Number of unique words:\", unique_word_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.05 Twenty-Five Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Assuming `all_results` contains data for multiple groups\n",
    "for corpus_key, data in all_results.items():\n",
    "    text_objects = data.get('text_objects', None)\n",
    "\n",
    "    # Check if text_objects exist\n",
    "    if text_objects:\n",
    "        # Generate a frequency distribution for the text objects\n",
    "        freq_dist = nltk.FreqDist(text_objects)\n",
    "\n",
    "        # Get the top 25 most common words\n",
    "        most_common_words = freq_dist.most_common(25)\n",
    "\n",
    "        # Print results for each group\n",
    "        print(f\"\\nMost common words in {corpus_key}:\")\n",
    "        print(most_common_words)\n",
    "    else:\n",
    "        print(f\"No text objects available for {corpus_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.06 Display all unique words found in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using all_results dictionary which contains multiple corpora\n",
    "corpora_tokens = {key: set(data['word_tokens']) for key, data in all_results.items() if 'word_tokens' in data}\n",
    "\n",
    "# Function to find words unique to each corpus compared to others\n",
    "def find_unique_words(corpora_tokens):\n",
    "    unique_words = {}\n",
    "    for corpus_name, tokens in corpora_tokens.items():\n",
    "        # Start with the current corpus tokens\n",
    "        all_other_tokens = set()\n",
    "        for other_corpus_name, other_tokens in corpora_tokens.items():\n",
    "            if corpus_name != other_corpus_name:\n",
    "                all_other_tokens.update(other_tokens)\n",
    "        \n",
    "        # Unique words are those not in the union of all other tokens\n",
    "        unique_words[corpus_name] = tokens - all_other_tokens\n",
    "    return unique_words\n",
    "\n",
    "# Find words unique to each corpus\n",
    "unique_words_by_corpus = find_unique_words(corpora_tokens)\n",
    "\n",
    "# Print unique words for each corpus\n",
    "for corpus_name, unique_words in unique_words_by_corpus.items():\n",
    "    print(f\"Words exclusive to {corpus_name}:\", sorted(unique_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.07 Most frequently used words across all corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Assuming all_results contains multiple corpora with their respective word tokens\n",
    "corpora_tokens = {key: data['word_tokens'] for key, data in all_results.items() if 'word_tokens' in data}\n",
    "\n",
    "# Aggregate all tokens from all corpora into a single list\n",
    "all_tokens = []\n",
    "for tokens in corpora_tokens.values():\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Calculate the frequency distribution of all tokens\n",
    "token_freq_dist = Counter(all_tokens)\n",
    "\n",
    "# Find the most common words across all corpora\n",
    "most_common_words = token_freq_dist.most_common(100)  # Adjust the number as needed\n",
    "\n",
    "def print_in_columns(data, columns=3):\n",
    "    # Split the data into chunks of size 'columns'\n",
    "    for i in range(0, len(data), columns):\n",
    "        chunk = data[i:i + columns]\n",
    "        # Format and print each chunk\n",
    "        print(\"  \".join(f\"{word}: {freq}\" for word, freq in chunk))\n",
    "\n",
    "# Print the most common words in columns\n",
    "print(\"Most frequently used words across all corpora:\")\n",
    "print_in_columns(most_common_words, columns=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.08 N-grams and collocations\n",
    "N-grams point out recurring word combinations found throughout the text corpus. For example, \"spring break\" is an example of a bigrams while \"New York City\" is a trigrams. Bigrams and repeated collocations of words convey a lot of information about the contents of the text corpus.\n",
    "To generate an ordered list of the most common bigrams, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming all_results contains multiple corpora with their respective text objects\n",
    "output_directory = 'data/outputFiles/ngramFrequencies'\n",
    "os.makedirs(output_directory, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "for key, data in all_results.items():\n",
    "    if 'text_objects' in data:\n",
    "        text_object = data['text_objects']\n",
    "        \n",
    "        # Find bigrams\n",
    "        bigram_finder = BigramCollocationFinder.from_words(text_object)\n",
    "        bigrams = bigram_finder.ngram_fd.items()\n",
    "        bigrams_sorted = sorted(bigrams, key=lambda item: item[1], reverse=True)\n",
    "        bigram_df = pd.DataFrame(bigrams_sorted, columns=['Bigram', 'Frequency'])\n",
    "        \n",
    "        # Save bigrams to CSV\n",
    "        bigram_filename = os.path.join(output_directory, f\"{key}_bigrams.csv\")\n",
    "        bigram_df.to_csv(bigram_filename, index=False)\n",
    "        print(f\"Top 50 bigrams for {key} saved to {bigram_filename}\")\n",
    "        \n",
    "        # Find trigrams\n",
    "        trigram_finder = TrigramCollocationFinder.from_words(text_object)\n",
    "        trigrams = trigram_finder.ngram_fd.items()\n",
    "        trigrams_sorted = sorted(trigrams, key=lambda item: item[1], reverse=True)\n",
    "        trigram_df = pd.DataFrame(trigrams_sorted, columns=['Trigram', 'Frequency'])\n",
    "        \n",
    "        # Save trigrams to CSV\n",
    "        trigram_filename = os.path.join(output_directory, f\"{key}_trigrams.csv\")\n",
    "        trigram_df.to_csv(trigram_filename, index=False)\n",
    "        print(f\"Top 50 trigrams for {key} saved to {trigram_filename}\")\n",
    "\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "# Assuming all_results contains multiple corpora with their respective text objects\n",
    "for key, data in all_results.items():\n",
    "    if 'text_objects' in data:\n",
    "        text_object = data['text_objects']\n",
    "        \n",
    "        # Display frequency of highest 50 bigrams\n",
    "        print(f\"Top 50 bigrams for {key}:\")\n",
    "        bigram_finder = BigramCollocationFinder.from_words(text_object)\n",
    "        bigram_finder.ngram_fd.tabulate(25)\n",
    "        \n",
    "        # Display frequency of highest 50 trigrams\n",
    "        print(f\"Top 50 trigrams for {key}:\")\n",
    "        trigram_finder = TrigramCollocationFinder.from_words(text_object)\n",
    "        trigram_finder.ngram_fd.tabulate(5)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.09 Concordance\n",
    "Concordance is an NLTK Text object method that also looks for word distribution, but specifically searches for words found before and after a specific word of choice. Concordance allows us to find out how words are used contextually throughout a corpus. This can be particularly powerful when looking at trends over time or between groups. For example, in the sample below we search for the all the contextual occurrences of the word pi in our seven separate corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all_results is a dictionary with keys as corpus names and values containing NLTK text objects among other details\n",
    "for key, data in all_results.items():\n",
    "    if 'text_objects' in data:\n",
    "        text_object = data['text_objects']\n",
    "        print(f\"Concordance for 'students' in {key}:\")\n",
    "        text_object.concordance(\"email\", width=150)\n",
    "        print(\"\\n\")  # Adding a newline for better readability between results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.00 Word Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.01 Bag of Words Frequency distribution\n",
    "\n",
    "This will search for each word in the bag of words to find its frequncy in each text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize a dictionary to hold all word tokens for each corpus\n",
    "all_word_tokens = {}\n",
    "\n",
    "# Extract word tokens from each result in all_results and store them in all_word_tokens\n",
    "for file_key, result in all_results.items():\n",
    "    all_word_tokens[file_key] = result['word_tokens']\n",
    "\n",
    "# At this point, all_word_tokens will have file keys as keys and lists of word tokens as values\n",
    "# Assuming `all_word_tokens` is a dictionary where keys are corpus names and values are lists of word tokens\n",
    "# For example:\n",
    "# all_word_tokens = {\n",
    "#     'corpus1': ['word1', 'word2', ...],\n",
    "#     'corpus2': ['word1', 'word2', ...],\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Keywords to track across corpora\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying'])\n",
    "\n",
    "# Initialize a dictionary to hold frequency distributions\n",
    "freq_distributions = {}\n",
    "\n",
    "# Calculate frequency distribution for each corpus\n",
    "for corpus_name, tokens in all_word_tokens.items():\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    freq_distributions[corpus_name] = {word: freq_dist[word] for word in keywords}\n",
    "\n",
    "# Convert the frequency distributions to a DataFrame for easy visualization and analysis\n",
    "freq_df = pd.DataFrame(freq_distributions)\n",
    "\n",
    "print(freq_df[:35])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.02 Bag of Words Frequency Distribution with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example keywords to track\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', 'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "fifteen_minute_marks = {\n",
    "    'mathTalk_file1': [15, 23, 43, 45, 67, 85, 190],\n",
    "    'mathTalk_file2': [205, 443, 520, 723, 986, 1222, 1517],\n",
    "\n",
    "}\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "\n",
    "# Number of corpora\n",
    "num_corpora = len(all_results)\n",
    "\n",
    "# Create subplots for each corpus\n",
    "fig, axes = plt.subplots(num_corpora, 1, figsize=(25, num_corpora*9), sharex=True)\n",
    "\n",
    "# Convert axes to an array if it's not (happens when num_corpora is 1)\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "for ax, (file_key, results) in zip(axes, all_results.items()):\n",
    "    ax.set_title(f\"Corpus: {file_key}\")\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        corpus_names = [occ[0] for occ in occurrences]\n",
    "        sentence_nums = [occ[1] for occ in occurrences]\n",
    "        \n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "        ax.scatter(sentence_nums, y_values, label=keyword, alpha=0.6, edgecolors='none')\n",
    "\n",
    "\n",
    "    ax.set_yticks(list(keyword_mapping.values()))\n",
    "    ax.set_yticklabels(list(keyword_mapping.keys()))\n",
    "\n",
    "# Adjust layout\n",
    "plt.xlabel('Sentence Number')\n",
    "plt.ylabel('Keywords')\n",
    "\n",
    "# Add common legend and labels\n",
    "# fig.legend(keywords, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=len(keywords))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to an image file before displaying\n",
    "plt.savefig('my_plots.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.03 Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "\n",
    "# Example keywords to track\n",
    "keywords = ['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', 'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', 'know',\n",
    "            'think', 'want', 'kind','time', 'grade', 'thinking', 'different', 'talk', 'conversation', 'discussion', 'hard', 'saying']\n",
    "\n",
    "fifteen_minute_marks = {\n",
    "    'mathTalk_file1': [15, 23, 43, 45, 67, 85, 190],\n",
    "    'mathTalk_file2': [205, 443, 520, 723, 986, 1222, 1517],\n",
    "    'mathTalk_file3': [174, 430, 521, 731, 986, 1198, 1557],\n",
    "    'mathTalk_file4': [52, 273, 300, 352, 406, 486, 534],\n",
    "    'mathTalk_file5': [66, 169, 250, 355, 482, 649, 760],\n",
    "    'mathTalk_file6': [316, 654, 800, 1159, 1575, 1884, 2200],\n",
    "    'mathTalk_file7': [114, 312, 381, 723, 1027, 1255, 1519],\n",
    "}\n",
    "\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Create a figure with an increased height to better fit the number of keywords\n",
    "fig = go.Figure()\n",
    "\n",
    "# Loop through each corpus\n",
    "for file_key, results in all_results.items():\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences if occ[0] == file_key]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Adding traces for each keyword\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers', name=keyword,\n",
    "                                 text=['Sentence: {}'.format(num) for num in sentence_nums],\n",
    "                                 marker=dict(size=8, opacity=0.6)))\n",
    "\n",
    "\n",
    "# Update layout with an appropriate height\n",
    "fig.update_layout(title='Keyword Occurrence Across Sentences in Multiple Corpora',\n",
    "                  xaxis_title='Sentence Number',\n",
    "                  yaxis=dict(tickmode='array', tickvals=list(keyword_mapping.values()), ticktext=list(keyword_mapping.keys())),\n",
    "                  legend_title='Keywords',\n",
    "                  height=1200)  # Set a larger height depending on the number of keywords\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.04 Side to side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "num_corpora = len(all_results)  # Count of corpora to be displayed\n",
    "fig = make_subplots(rows=1, cols=num_corpora, subplot_titles=[f\"Corpus {i+1}\" for i in range(num_corpora)])\n",
    "\n",
    "# Example keywords to track, sorted to maintain consistency across the plot\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', \n",
    "            'think', 'want', 'kind', 'time', 'grade', 'thinking', 'different', 'talk', 'conversation', \n",
    "            'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Index for the current subplot\n",
    "col_index = 1\n",
    "\n",
    "# Loop through each corpus\n",
    "for file_key, results in all_results.items():\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences if occ[0] == file_key]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Adding traces for each keyword to the respective subplot\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers', name=keyword,\n",
    "                                 text=['Sentence: {}'.format(num) for num in sentence_nums],\n",
    "                                 marker=dict(size=10, opacity=0.5)),  # Adjusted marker size for visibility\n",
    "                      row=1, col=col_index)\n",
    "\n",
    "    col_index += 1  # Move to the next subplot for the next corpus\n",
    "\n",
    "# Update layout to make sure all keywords are visible\n",
    "fig.update_layout(\n",
    "    title='Keyword Occurrence Across Sentences in Multiple Corpora',\n",
    "    xaxis_title='Sentence Number',\n",
    "    yaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=list(keyword_mapping.values()),\n",
    "        ticktext=list(keyword_mapping.keys())\n",
    "    ),\n",
    "    legend_title='Keywords',\n",
    "    height=1200,  # Increased height to accommodate all keywords\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.05 Needs fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_results is defined and populated as per your previous code\n",
    "num_corpora = len(all_results)  # Count of corpora to be displayed\n",
    "fig = make_subplots(rows=1, cols=num_corpora, subplot_titles=[f\"Corpus {i+1}\" for i in range(num_corpora)])\n",
    "\n",
    "# Example keywords to track, sorted to maintain consistency across the plot\n",
    "keywords = sorted(['students', \"science\", 'engagement', 'ability', 'community', 'talk',\n",
    "            'culture', 'ownership', 'regular', 'successful', 'participation', 'expectations', \n",
    "            'ap', 'assessment', 'cultural', 'phenomena', 'important', 'know', 'standards', 'relationship',\n",
    "            \"engaging\", 'literacy', 'relationship', 'kids', 'connect', 'student', 'classroom', \n",
    "            'teacher', 'teaching', 'school', 'class', 'curriculum', 'learn', 'approach', 'talking', \n",
    "            'discussion', 'love', 'proud', 'like', 'difficult', 'actually', 'know', 'questions', \n",
    "            'think', 'want', 'kind', 'time', 'grade', 'thinking', 'different', 'talk', 'conversation', \n",
    "            'discussion', 'hard', 'saying'], reverse=True)\n",
    "\n",
    "# Mapping keywords to numeric values for plotting\n",
    "keyword_mapping = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "\n",
    "# Loop through each corpus\n",
    "for file_key, results in all_results.items():\n",
    "    for keyword in keywords:\n",
    "        occurrences = [(file_key, i+1) for i, sentence in enumerate(results['sentence_tokens']) if keyword in sentence.lower()]\n",
    "        sentence_nums = [occ[1] for occ in occurrences if occ[0] == file_key]\n",
    "        y_values = np.full_like(sentence_nums, keyword_mapping[keyword], dtype=float)\n",
    "\n",
    "        # Adding traces for each keyword\n",
    "        fig.add_trace(go.Scatter(x=sentence_nums, y=y_values, mode='markers',\n",
    "                                 text=[f'{keyword} (Sentence: {num})' for num in sentence_nums],\n",
    "                                 marker=dict(size=10, opacity=0.5),\n",
    "                                 showlegend=False),  # This prevents adding to the legend\n",
    "                      row=1, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Keyword Occurrence Across Sentences in Multiple Corpora',\n",
    "    xaxis_title='Sentence Number',\n",
    "    yaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=list(keyword_mapping.values()),\n",
    "        ticktext=list(keyword_mapping.keys())\n",
    "    ),\n",
    "    height=1200,  # Increased height to accommodate all keywords\n",
    ")\n",
    "\n",
    "# Save the figure to an HTML file for interactive viewing\n",
    "fig.write_html('my_interactive_plots.html')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.00 Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.01 Importing data from csv file in Google Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# This is the full shared Drive link, the file ID starts at \"1i\" and ends at \"8S\"\n",
    "# https://docs.google.com/spreadsheets/d/1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S/edit?usp=drive_link&ouid=106477043869312333876&rtpof=true&sd=true\n",
    "# https://drive.google.com/file/d/1hLRRRvawjxrdI141_bT5QXELb0jk9Jhg/view?usp=sharing\n",
    "\n",
    "# the file ID from the shareable link is pasted below in orange.\n",
    "file_id = \"1hLRRRvawjxrdI141_bT5QXELb0jk9Jhg\"\n",
    "\n",
    "# construct the download URL, you would not need to change anything here.\n",
    "download_url = f\"https://docs.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "# send a GET request to the download URL and save the response content\n",
    "response = requests.get(download_url)\n",
    "\n",
    "# The next line names the file after download. If you change it here, you will also need to change in the subsequent fields.\n",
    "# If you click on the folder icon in Colab you should see a file now appear called \"uncertaintyText.xlsx\"\n",
    "# These names can be changed to suit you own data\n",
    "with open(\"uncertaintyText.xlsx\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "\n",
    "# Specify the path to the Excel file this where it was placed in 2.4 so that is the file and path you want to open\n",
    "excel_file_path = '/content/uncertaintyText.xlsx'\n",
    "\n",
    "# Specify the column name you want to pull the data corpus from\n",
    "column_name = 'transcript'\n",
    "\n",
    "# Read the Excel file and extract the specified column\n",
    "data = pd.read_excel(excel_file_path, engine='openpyxl')\n",
    "text_column = data[column_name]\n",
    "\n",
    "\n",
    "# Convert each item in the column to a string and then join them together to be saved as a text file containing all data in the transcript column.\n",
    "raw_uncertaintyText = ' '.join(map(str, text_column))\n",
    "\n",
    "\n",
    "# Save the string to a text file in your Google Drive\n",
    "with open('/content/raw_uncertaintyText.txt', 'w') as file:\n",
    "  file.write(raw_uncertaintyText)\n",
    "\n",
    "print(\"Text saved to raw_uncertaintyText.txt\")\n",
    "print(\"Raw text file is a: \",type(raw_uncertaintyText), \"It contains: \",len(raw_uncertaintyText), \"characters\")\n",
    "print(\"Here are the first 251 characters in the raw text file: \", raw_uncertaintyText[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Topic Modeling and Semantic Similarities Across Transcriptions\n",
    "Generating Sentence Embeddings for Each Transcription\n",
    "We'll generate embeddings for each transcription and store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Store embeddings and related info\n",
    "embeddings_list = []\n",
    "file_names = []\n",
    "\n",
    "for df in transcript_dfs:\n",
    "    text = ' '.join(df['cleaned_transcript'])\n",
    "    embedding = model.encode(text)\n",
    "    embeddings_list.append(embedding)\n",
    "    file_names.append(df['source_file'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Transcriptions Using Semantic Similarity\n",
    "We'll compute the cosine similarity between each pair of transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert list of embeddings to a numpy array\n",
    "embeddings_array = np.vstack(embeddings_list)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=file_names, columns=file_names)\n",
    "\n",
    "print(similarity_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on Individual Transcriptions\n",
    "Using Latent Dirichlet Allocation (LDA)\n",
    "First, you need to prepare the data for LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "def perform_lda(tokens_list, num_topics=5):\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "\n",
    "    # Filter out extremes to limit the number of features\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "\n",
    "    # Create a bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "    return lda, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
